name,ring,quadrant,isNew,description


Retrieval-augmented generation (RAG),Adopt,Techniques,FALSE,"Retrieval-augmented generation (RAG) is the preferred pattern for our teams to improve the quality of responses generated by a large language model (LLM). We've successfully used it in several projects, including the popular Jugalbandi AI Platform. With RAG, information about relevant and trustworthy documents — in formats like HTML and PDF — are stored in databases that supports a vector data type or efficient document search, such as pgvector, Qdrant or Elasticsearch Relevance Engine. For a given prompt, the database is queried to retrieve relevant documents, which are then combined with the prompt to provide richer context to the LLM. This results in higher quality output and greatly reduced hallucinations. The context window — which determines the maximum size of the LLM input — is limited, which means that selecting the most relevant documents is crucial. We improve the relevancy of the content that is added to the prompt by reranking. Similarly, the documents are usually too large to calculate an embedding, which means they must be split into smaller chunks. This is often a difficult problem, and one approach is to have the chunks overlap to a certain extent."

Automatically generate Backstage entity descriptors,Trial,Techniques,TRUE,"Backstage from Spotify has been widely adopted across our client base as the preferred platform to host developer experience portals. Backstage, on its own, is just a shell that hosts plugins and provides an interface to manage the catalog of assets that make up a platform ecosystem. Any entity to be displayed or managed by Backstage is configured in the catalog-info file, which contains data such as status, lifecycle, dependencies and APIs among other details. By default, individual entity descriptors are written by hand and usually maintained and versioned by the team responsible for the component in question. Keeping the descriptors up to date can be tedious and create a barrier to developer adoption. Also, there is always the possibility that changes are overlooked or that some components are missed entirely. We've found it more efficient and less error-prone to automatically generate Backstage entity descriptors. Most organizations have existing sources of information that can jump-start the process of populating catalog entries. Good development practices, for example, putting appropriate tags on AWS resources or adding metadata to source files, can simplify entity discovery and descriptor generation. These automated processes can then be run on a regular basis — once a day, for example — to keep the catalog fresh and up to date."
Combining traditional NLP with LLMs,Trial,Techniques,TRUE,"Large language models (LLMs) are the Swiss Army knives of natural language processing (NLP). But they're also quite expensive and not always the best tool for the job — sometimes it's more effective to use a proper corkscrew. Indeed, there's a lot of potential in combining traditional NLP with LLMs, or in building multiple NLP approaches in conjunction with LLMs to implement use cases and leverage LLMs for the steps where you actually need their capabilities. Traditional data science and NLP approaches for document clustering, topic identification and classification and even summarization are cheaper and can be more effective for solving a part of your use case problem. We then use LLMs when we need to generate and summarize longer texts, or combine multiple large documents, to take advantage of the LLM's superior attention span and memory. For example, we've successfully used this combination of techniques to generate a comprehensive trends report for a domain from a large corpus of individual trend documents, using traditional clustering alongside the generative power of LLMs."
Continuous compliance,Trial,Techniques,TRUE,"Continuous compliance is the practice of ensuring that software development processes and technologies comply with industry regulations and security standards on an ongoing basis, by heavily leveraging automation. Manually checking for security vulnerabilities and adhering to regulations can slow down development and introduce errors. As an alternative, organizations can automate compliance checks and audits. They can integrate tools into software development pipelines, allowing teams to detect and address compliance issues early in the development process. Codifying compliance rules and best practices helps enforce policies and standards consistently across teams. It enables you to scan code changes for vulnerabilities, enforce coding standards and track infrastructure configuration changes to ensure they meet compliance requirements. Lastly, automated reporting of the above simplifies audits and provides clear evidence of compliance. We've already talked about techniques like publishing SBOMs and applying the recommendations from SLSA — they can be very good starting points. The benefits of this technique are multifold. First, automation leads to more secure software by identifying and mitigating vulnerabilities early and, second, development cycles accelerate as manual tasks are eliminated. Reduced costs and enhanced consistency are additional perks. For safety-critical industries like software-driven vehicles, automated continuous compliance can improve the efficiency and reliability of the certification process, ultimately leading to safer and more reliable vehicles on the road."
Edge functions,Trial,Techniques,TRUE,"Although not a new concept, we've noticed the growing availability and use of decentralized code execution via content delivery networks (CDNs). Services such as Cloudflare Workers or Amazon CloudFront Edge Functions provide a mechanism to execute snippets of serverless code close to the client's geographic location. Edge functions not only offer lower latency if a response can be generated at the edge, they also present an opportunity to rewrite requests and responses in a location-specific way on their way to and from the regional server. For example, you might rewrite a request URL to route to a specific server that has local data relevant to a field found in the request body. This approach is best suited to short, fast-running stateless processes since the computational power at the edge is limited."
Security champions,Trial,Techniques,TRUE,"Security champions are team members who think critically about security repercussions of both technical and nontechnical delivery decisions. They raise these questions and concerns with team leadership and have a firm understanding of basic security guidelines and requirements. They help development teams approach all activities during software delivery with a security mindset, thus reducing the overall security risks for the systems they develop. A security champion is not a separate position but a responsibility assigned to an existing member of the team who is guided by appropriate training from security practitioners. Equipped with this training, security champions improve the security awareness of the team by spreading knowledge and acting as a bridge between the development and security teams. One great example of an activity security champions can help drive within the team is threat modeling, which helps teams think about security risks from the start. Appointing and training a security champion on a team is a great first step, but relying solely on champions without proper commitment from leaders can lead to problems. Building a security mindset, in our experience, requires commitment from the entire team and managers."
Text to SQL,Trial,Techniques,TRUE,"Text to SQL is a technique that converts natural language queries into SQL queries that can be executed by a database. Although large language models (LLMs) can understand and transform natural language, creating accurate SQL for your own schema can be challenging. Enter Vanna, an open-source Python retrieval-augmented generation (RAG) framework for SQL generation. Vanna works in two steps: first you create embeddings with the data definition language statements (DDLs) and sample SQLs for your schema, and then you ask questions in natural language. Although Vanna can work with any LLMs, we encourage you to assess NSQL, a domain-specific LLM for text-to-SQL tasks."
Tracking health over debt,Trial,Techniques,FALSE,"We keep experiencing the improvements teams make to their ecosystem by treating the health rating the same as other service-level objectives (SLOs) and prioritizing enhancements accordingly, instead of solely focusing on tracking technical debt. By allocating resources efficiently to address the most impactful issues related to health, teams and organizations can reduce long-term maintenance costs and evolve products more efficiently. This approach also enhances communication between technical and nontechnical stakeholders, fostering a common understanding of the system's state. Although metrics may vary among organizations (see this blog post for examples) they ultimately contribute to long-term sustainability and ensure software remains adaptable and competitive. In a rapidly changing digital landscape, focusing on tracking health over debt of systems provides a structured and evidence-based strategy to maintain and enhance them."

AI team assistants,Assess,Techniques,TRUE,"AI coding assistance tools like GitHub Copilot are currently mostly talked about in the context of assisting and enhancing an individual's work. However, software delivery is and will remain team work, so you should be looking for ways to create AI team assistants to help create the "10x team," as opposed to a bunch of siloed AI-assisted 10x engineers. We've started using a team assistance approach that can increase knowledge amplification, upskilling and alignment through a combination of prompts and knowledge sources. Standardized prompts facilitate the use of agreed-upon best practices in the team context, such as techniques and templates for user story writing or the implementation of practices like threat modeling. In addition to prompts, knowledge sources made available through retrieval-augmented generation provide contextually relevant information from organizational guidelines or industry-specific knowledge bases. This approach gives team members access to the knowledge and resources they need just in time."
Graph analysis for LLM-backed chats,Assess,Techniques,TRUE,"Chatbots backed by large language models (LLMs) are gaining a lot of popularity right now, and we're seeing emerging techniques around productionizing and productizing them. One such productization challenge is understanding how users are conversing with a chatbot that is driven by something as generic as an LLM, where the conversation can go in many directions. Understanding the reality of conversation flows is crucial to improving the product and improving conversion rates. One technique to tackle this problem is to use graph analysis for LLM-backed chats. The agents that support a chat with a specific desired outcome — such as a shopping action or a successful resolution of a customer's problem — can usually be represented as a desired state machine. By loading all conversations into a graph, you can analyze actual patterns and look for discrepancies to the expected state machine. This helps find bugs and opportunities for product improvement."
LLM-backed ChatOps,Assess,Techniques,TRUE,"LLM-backed ChatOps is an emerging application of large language models through a chat platform (primarily Slack) that allows engineers to build, deploy and operate software via natural language. This has the potential to streamline engineering workflows by enhancing the discoverability and user-friendliness of platform services. At the time of writing, two early examples are PromptOps and Kubiya. However, considering the finesse needed for production environments, organizations should thoroughly evaluate these tools before allowing them anywhere near production."
LLM-powered autonomous agentss,Assess,Techniques,TRUE,"LLM-powered autonomous agents are evolving beyond single agents and static multi-agent systems with the emergence of frameworks like Autogen and CrewAI. These frameworks allow users to define agents with specific roles, assign tasks and enable agents to collaborate on completing those tasks through delegation or conversation. Similar to single-agent systems that emerged earlier, such as AutoGPT, individual agents can break down tasks, utilize preconfigured tools and request human input. Although still in the early stages of development, this area is developing rapidly and holds exciting potential for exploration."
Using GenAI to understand legacy codebases,Assess,Techniques,TRUE,"Generative AI (GenAI) and large language models (LLMs) can help developers both write and understand code. In practical application, this is so far mostly limited to smaller code snippets, but more products and technology developments are emerging for using GenAI to understand legacy codebases. This is particularly useful in the case of legacy codebases that aren't well-documented or where the documentation is outdated or misleading. For example, Driver AI or bloop use RAG approaches that combine language intelligence and code search with LLMs to help users find their way around a codebase. Emerging models with larger and larger context windows will also help to make these techniques more viable for sizable codebases. Another promising application of GenAI for legacy code is in the space of mainframe modernization, where bottlenecks often form around reverse engineers who need to understand the existing codebase and turn that understanding into requirements for the modernization project. Using GenAI to assist those reverse engineers can help them get their work done faster."
VISS,Assess,Techniques,TRUE,"Zoom recently open-sourced its Vulnerability Impact Scoring System, or VISS. This system is mainly focused on vulnerability scoring that prioritizes actual demonstrated security measures. VISS differs from the Common Vulnerability Scoring System (CVSS) by not focusing on worst-case scenarios and attempting to more objectively measure the impact of vulnerabilities from a defender's perspective. To this aim, VISS provides a web-based UI to calculate the vulnerability score based on several parameters — categorized into platform, infrastructure and data groups — including the impact on the platform, the number of tenants impacted, data impact and more. Although we don't have too much practical experience with this specific tool yet, we think this kind of priority-tailored assessment approach based on industry and context is worth practicing."

Broad integration tests,Hold,Techniques,TRUE,"While we applaud a focus on automated testing, we continue to see numerous organizations over-invested in what we believe to be ineffective broad integration tests. As the term "integration test" is ambiguous, we've taken the broad classification from Martin Fowler's bliki entry on the subject which indicates a test that requires live versions of all run-time dependencies. Such a test is obviously expensive, because it requires a full-featured test environment with all the necessary infrastructure, data and services. Managing the right versions of all those dependencies requires significant coordination overhead, which tends to slow down release cycles. Finally, the tests themselves are often fragile and unhelpful. For example, it takes effort to determine if a test failed because of the new code, mismatched version dependencies or the environment, and the error message rarely helps pinpoint the source of the error. Those criticisms don't mean that we take issue with automated "black box" integration testing in general, but we find a more helpful approach is one that balances the need for confidence with release frequency. This can be done in two stages by first validating the behavior of the system under test assuming a certain set of responses from run-time dependencies, and then validating those assumptions. The first stage uses service virtualization to create test doubles of run-time dependencies and validates the behavior of the system under test. This simplifies test data management concerns and allows for deterministic tests. The second stage uses contract tests to validate those environmental assumptions with real dependencies."
Overenthusiastic LLM use,Hold,Techniques,TRUE,"In the rush to leverage the latest in AI, many organizations are quickly adopting large language models (LLMs) for a variety of applications, from content generation to complex decision-making processes. The allure of LLMs is undeniable; they offer a seemingly effortless solution to complex problems, and developers can often create such a solution quickly and without needing years of deep machine learning experience. It can be tempting to roll out an LLM-based solution as soon as it's more or less working and then move on. Although these LLM-based proofs of value are useful, we advise teams to look carefully at what the technology is being used for and to consider whether an LLM is actually the right end-stage solution. Many problems that an LLM can solve — such as sentiment analysis or content classification — can be solved more cheaply and easily using traditional natural language processing (NLP). Analyzing what the LLM is doing and then analyzing other potential solutions not only mitigates the risks associated with overenthusiastic LLM use but also promotes a more nuanced understanding and application of AI technologies."
Rush to fine-tune LLMs,Hold,Techniques,TRUE,"As organizations are looking for ways to make large language models (LLMs) work in the context of their product, domain or organizational knowledge, we're seeing a rush to fine-tune LLMs. While fine-tuning an LLM can be a powerful tool to gain more task-specificity for a use case, in many cases it's not needed. One of the most common cases of a misguided rush to fine-tuning is about making an LLM-backed application aware of specific knowledge and facts or an organization's codebases. In the vast majority of these cases, using a form of retrieval-augmented generation (RAG) offers a better solution and a better cost-benefit ratio. Fine-tuning requires considerable computational resources and expertise and introduces even more challenges around sensitive and proprietary data than RAG. There is also a risk of underfitting, when you don't have enough data available for fine-tuning, or, less frequently, overfitting, when you have too much data and are therefore not hitting the right balance of task specificity that you need. Look closely at these trade-offs and consider the alternatives before you rush to fine-tune an LLM for your use case."
Web components for SSR web apps,Hold,Techniques,FALSE,"With the adoption of frameworks like Next.js and htmx, we're seeing more usage of server-side rendering (SSR). As a browser technology, it's not trivial to use web components on the server. Frameworks have sprung up to make this easier, sometimes even using a browser engine, but the complexity is still there. Our developers find themselves needing workarounds and extra effort to order front-end components and server-side components. Worse than the developer experience is the user experience: page load performance is impacted when custom web components have to be loaded and hydrated in the browser, and even with pre-rendering and careful tweaking of the component, a "flash of unstyled content" or some layout shifting is all but unavoidable. As mentioned in the previous Radar, one of our teams had to move their design system away from the web components-based Stencil because of these issues. Recently, we received reports from another team that they ended up replacing server-side–generated components with browser-side components because of the development complexity. We caution against the use of web components for SSR web apps, even if supported by frameworks."




CloudEvents,Adopt,Platforms,FALSE,"Events are common mechanisms in event-driven architecture or serverless applications. However, producers or cloud providers tend to support them in different forms, which prevents interoperability across platforms and infrastructures. CloudEvents is a specification for describing event data in common formats to provide interoperability across services, platforms and systems. It provides SDKs in multiple languages so you can embed the spec into your application or toolchain. Our teams use it not only for cross-cloud platform purposes but also for domain event specification, among other scenarios. CloudEvents is hosted by the Cloud Native Computing Foundation (CNCF) and is now a graduated project. Our teams default to using CloudEvents for building event-driven architectures and for that reason we're moving it to Adopt."

Arm in the cloud,Trial,Platforms,FALSE,"Arm compute instances in the cloud have become increasingly popular in recent years due to their cost and energy efficiency compared to traditional x86-based instances. Many cloud providers now offer Arm-based instances, including AWS, Azure and GCP. The cost benefits of running Arm in the cloud can be particularly beneficial for businesses that run large workloads or need to scale. We're seeing many teams of ours moving to Arm instances for workloads like JVM services and even databases (including RDS) without any change in the code and minimal changes in the build scripts. New cloud-based applications and systems increasingly default to Arm in the cloud. Based on our experiences, we recommend Arm compute instances for all workloads unless there are architecture-specific dependencies. The tooling to support multiple architectures, such as multi-arch Docker images, also simplifies build and deploy workflows."
Azure Container Apps,Trial,Platforms,FALSE,"Azure Container Apps is a managed Kubernetes application platform that streamlines the deployment of containerized workloads. In comparison to Azure Kubernetes Service (AKS), the operational and administrative burden of running containerized applications is reduced, but this comes at the expense of some flexibility and control, which is a trade-off teams need to consider. Another product in this area, Azure Container Instances, is usually too limited for production use. Our teams started using Azure Container Apps last year, when it was still in public preview, with good results, even when running large containers. Now that it is generally available, we're considering it for more use cases. Both Dapr and the KEDA Autoscaler are supported."
Azure OpenAI Service,Trial,Platforms,FALSE,"Azure OpenAI Service provides access to OpenAI's GPT-4, GPT-35-Turbo, Embeddings, DALL-E model and more through a REST API, a Python SDK and web-based interface. The models can be adapted to tasks such as content generation, summarization, semantic search and translating natural language to code. Fine-tuning is also available via few-shot learning and the customization of hyperparameters. In comparison to OpenAI's own API, Azure OpenAI Service benefits from Azure's enterprise-grade security and compliance features, is available for more regions (although availability is limited for each of the larger geographic regions) and supports private networking, content filtering and manual model version control. For these reasons and our positive experience with it, we recommend that enterprises already using Azure consider using Azure OpenAI Service instead of the OpenAI API."
DataHub,Trial,Platforms,FALSE,"When you build data products using data product thinking, it's essential to consider data lineage, data discoverability and data governance. Our teams have found that DataHub can provide particularly useful support here. Although earlier versions of DataHub required you to fork and manage the sync from the main product (if there was a need to update the metadata model), improvements in recent releases have introduced features that allow our teams to implement custom metadata models with a plugin-based architecture. Another useful feature of DataHub is the robust end-to-end data lineage from source to processing to consumption. DataHub supports both push-based integration as well as pull-based lineage extraction that automatically crawls the technical metadata across data sources, schedulers, orchestrators (scanning the Airflow DAG), processing pipeline tasks and dashboards, to name a few. As an open-source option for a holistic data catalog, DataHub is emerging as a default choice for our teams."
Infrastructure orchestration platforms,Trial,Platforms,TRUE,"In-house infrastructure orchestration codebases frequently become a time sink to maintain and troubleshoot. Infrastructure orchestration platforms are appearing, promising to standardize and productize various aspects of infrastructure code delivery and deployment workflows. These include build tools like Terragrunt and Terraspace, services from IaC tool vendors such as Terraform Cloud and Pulumi Cloud as well as tool-agnostic platforms and services like env0 and Spacelift. There is a rich ecosystem of Terraform-specific orchestration tools and services, often called TACOS (Terraform Automation and Collaboration Software), including Atlantis, Digger, Scalr, Terramate and Terrateam. Each of these platforms enables different workflows, including GitOps, Continuous Delivery and compliance as code. We welcome the growth of solutions in this space. We recommend infrastructure and platform engineering teams explore how to use them to reduce the amount of non-differentiating custom code they need to develop and maintain their infrastructure. Standardization of how infrastructure code is structured, shared, delivered and deployed should also create opportunities for the emergence of an ecosystem of compatible tools for testing, measuring and monitoring infrastructure."
Pulumi,Trial,Platforms,FALSE,"Tooling in the infrastructure-as-code space continues to evolve, and we're pleased to see that Pulumi is no exception to this trend. The platform recently added support for Java and YAML, for managing infrastructure at scale as well as for a multitude of cloud configurations and integrations, making the platform even more compelling. For our teams, it's still the main alternative to Terraform for developing code for multiple cloud platforms"
Rancher Desktop,Trial,Platforms,TRUE,"Changes in licensing for Docker Desktop have left us scrambling for alternatives for running a fleet of containers on a developer's local laptop. Recently we've had good success with Rancher Desktop. This free and open-source app is relatively easy to download and install for Apple, Windows or Linux machines and provides a handy local Kubernetes cluster with a GUI for configuration and monitoring. Although Colima has become our Docker Desktop alternative of choice, it's primarily a CLI tool. In contrast, Rancher Desktop will appeal to those who don't want to give up the graphical interface that Docker Desktop provides. Like Colima, Rancher Desktop allows you to choose between dockerd or containerd as the underlying container run time. The choice of direct containerd frees you from the DockerCLI, but the dockerd option provides compatibility with other tools that depend on it to communicate with the run-time daemon."
Weights & Biases,Trial,Platforms,FALSE,"Weights & Biases is a machine learning (ML) platform for building models faster through experiment tracking, data set versioning, visualizing model performance and model management. It can be integrated with existing ML code to get live metrics, terminal logs and system statistics streamed to the dashboard for further analysis. Recently, Weights & Biases has expanded into LLM observability with Traces. Traces visualizes the execution flow of prompt chains as well as intermediate inputs/outputs and provides metadata around chain execution (such as tokens used and start and end time). Our teams find it useful for debugging and getting a greater understanding of the chain architecture."

Bun,Assess,Platforms,FALSE,"Bun is a new JavaScript run time, similar to Node.js or Deno. Unlike Node.js or Deno, however, Bun is built using WebKit's JavaScriptCore instead of Chrome's V8 engine. Designed as a drop-in replacement for Node.js, Bun is a single binary (written in Zig) that acts as a bundler, transpiler and package manager for JavaScript and TypeScript applications. Since our last volume, Bun has gone from beta into a stable 1.0 release. Bun has been built from the ground up with several optimizations — including fast startup, improved server-side rendering and a much faster alternative package manager — and we encourage you to assess it for your JavaScript run-time engine."
Chronosphere,Assess,Platforms,TRUE,"When managing distributed architectures, accounting for the cost of sorting, indexing and accessing data is as critical as observability. Chronosphere takes a unique approach to cost management, tracking the use of observability data so that organizations can consider the cost-value trade-offs of various metrics. With the help of the Metrics Usage Analyzer, part of the Chronosphere Control Plane, teams can identify and exclude metrics they rarely (or never) use, thus yielding significant cost savings by reducing the amount of data organizations have to comb through. Given these advantages, as well as the ability of Chronosphere to match the functionality of other observability tools for cloud-hosted solutions, we believe it to be a compelling option for organizations to look into."
DataOS,Assess,Platforms,TRUE,"With data mesh adoption on the rise, our teams have been on the lookout for data platforms that treat data products as a first-class entity. DataOS is one such product. It provides end-to-end lifecycle management to design, build, deploy and evolve data products. It offers standardized declarative specs written in YAML that abstract the low-level complexity of infrastructure setup and allow developers to define the data products easily via CLI/API. It supports access control policies with ABAC and data policies for filtering and masking data. Another notable feature is its ability to federate data across a variety of data sources, which reduces data duplication and the movement of data to a central place. DataOS fits best for greenfield scenarios where it does the heavy lifting since it provides an out-of-the-box solution for data governance, data discoverability, infrastructure resource management and observability. For brownfield scenarios, the ability to orchestrate resources outside of DataOS (for example, data stacks like Databricks) is in its nascent stage and still evolving. If your ecosystem doesn't exert a lot of opinion on data tooling, DataOS is a good way to expedite your journey for building, deploying and consuming data products in an end-to-end fashion."
Dify,Assess,Platforms,TRUE,"Dify is a UI-driven platform for developing large language model (LLM) applications that makes prototyping them even more accessible. It supports the development of chat and text generation apps with prompt templates. Additionally, Dify supports retrieval-augmented generation (RAG) with imported data sets and can work with multiple models. We're excited about this category of applications. Based on our experience, however, Dify is not quite ready for prime time yet, because some features are buggy or don't seem fully fleshed out. At the moment, though, we're not aware of a competitor that is better."
Elasticsearch Relevance Engine,Assess,Platforms,TRUE,"Although vector databases have been gaining popularity for retrieval-augmented generation (RAG) use cases, research and experience reports suggest combining traditional full-text search with vector search (into a hybrid search) can yield superior results. Through Elasticsearch Relevance Engine (ESRE), the well-established full-text search platform Elasticsearch supports built-in and custom embedding models, vector search and hybrid search with ranking mechanisms such as Reciprocal Rank Fusion. Even though this space is still maturing, in our experience, using these ESRE features along with the traditional filtering, sorting and ranking capabilities that come with Elasticsearch has yielded promising results, suggesting that established search platforms that support semantic search are not to be passed over."
FOCUS,Assess,Platforms,TRUE,"Cloud and SaaS billing data can be complex, inconsistent among providers and difficult to understand. The FinOps Open Cost and Usage Specification (FOCUS) aims to reduce this friction with a spec containing a set of terminologies (aligned with the FinOps framework), a schema and a minimum set of requirements for billing data. The spec is intended to support use cases common to a variety of FinOps practitioners. Although still in the early stages of development and adoption, it's worth watching because, with growing industry adoption, FOCUS will make it easier for platforms and end users to get a holistic view of cloud spend across a long tail of cloud and SaaS providers."
Gemini Nano,Assess,Platforms,TRUE,"Google's Gemini is a family of foundational LLMs designed to run on a wide range of hardware, from data centers to mobile phones. Gemini Nano has been specifically optimized and scaled down to run on mobile silicon accelerators. It enables capabilities such as high-quality text summarization, contextual smart replies and advanced grammar correction. For example, the language understanding of Gemini Nano enables the Pixel 8 Pro to summarize content in the Recorder app. Running on-device removes many of the latency and privacy concerns associated with cloud-based systems and allows the features to work without network connection. Android AICore simplifies the integration of the model into Android apps, but only a few devices are supported at the time of writing."
HyperDX,Assess,Platforms,TRUE,"HyperDX is an open-source observability platform that unifies all three pillars of observability: logs, metrics and tracing. With it, you can correlate end-to-end and go from browser session replay to logs and traces in just a few clicks. The platform leverages ClickHouse as a central data store for all telemetry data, and it scales to aggregate log patterns and condense billions of events into distinctive clusters. Although you can choose from several observability platforms, we want to highlight HyperDX for its unified developer experience."
IcePanel,Assess,Platforms,TRUE,"IcePanel facilitates collaborative architectural modeling and diagramming using the C4 model, which allows technical and business stakeholders to zoom in to the level of technical detail they need. It supports modeling architecture objects whose metadata and connections can be reused across diagrams, along with the visualization of flows between those objects. Versioning and tagging allows collaborators to model different architecture states (e.g., as-is versus to-be) and track user-defined classifications of various parts of the architecture. We're keeping an eye on IcePanel for its potential to improve architecture collaboration, particularly for organizations with complex architectures. For an alternative that better supports diagrams as code, check out Structurizr."
Langfuse,Assess,Platforms,TRUE,"Langfuse is an engineering platform for observability, testing and monitoring large language model (LLM) applications. Its SDKs support Python, JavaScript and TypeScript, OpenAI, LangChain and LiteLLM among other languages and frameworks. You can self-host the open-source version or use it as a paid cloud service. Our teams have had a positive experience, particularly in debugging complex LLM chains, analyzing completions and monitoring key metrics such as cost and latency across users, sessions, geographies, features and model versions. If you're looking to build data-driven LLM applications, Langfuse is a good option to consider."
Qdrant,Assess,Platforms,TRUE,"Qdrant is an open-source vector database written in Rust. In the September 2023 edition of the Radar, we talked about pgvector, a PostgreSQL extension for vector search. However, if you have to scale the vector database horizontally across nodes, we encourage you to assess Qdrant. It has built-in single instruction, multiple data (SIMD) acceleration support for improved search performance, and it helps you associate JSON payloads with vectors."
RISC-V for embedded,Assess,Platforms,TRUE,"While the Arm architecture continues to expand its impact — we've updated our assessment of Arm in the cloud in this edition — interest in the newer and less established RISC-V architecture also grows. RISC-V doesn't bring breakthroughs in performance or efficiency — in fact, its per-watt performance is similar to Arm's, and it can't quite compete on absolute performance — but it's open source, modular and not tied to a single company. This makes it an attractive proposition for embedded systems, where the cost of licensing proprietary architectures is a significant concern. This is also why the field of RISC-V for embedded is maturing, and several companies, including SiFive and espressif, are offering development boards and SoCs for a wide range of applications. Microcontrollers and microprocessors capable of running the Linux kernel are available today, along with the corresponding software stack and toolchains. We're keeping an eye on this space and expect to see more adoption in the coming years."
Tigerbeetle,Assess,Platforms,TRUE,"Tigerbeetle is an open-source distributed database for financial accounting. Unlike other databases, it's designed to be a domain-specific state machine for safety and performance. The state from one node in the cluster is replicated in a deterministic order to other nodes via the Viewstamped Replication consensus protocol. We quite like the design decisions behind Tigerbeetle to implement double-entry bookkeeping with strict serializability guarantees."
WebTransport,Assess,Platforms,TRUE,"WebTransport is a protocol that builds on top of HTTP/3 and offers bidirectional communication between servers and apps. WebTransport offers several benefits over its predecessor, WebSockets, including faster connections, lower latency and the ability to handle both reliable and ordered data streams as well as unordered ones (such as UDP). It can handle multiple streams in the same connection without head-of-line blocking, allowing for more efficient communication in complex applications. Overall, WebTransport is suitable for a wide range of use cases, including real-time web apps, streaming media and Internet of Things (IoT) data communications. Even though WebTransport is still in the early stages — support across browsers is gradually maturing, with popular libraries such as socket.io adding support for WebTransport — our teams are currently assessing its potential for real-time IoT apps."
Zarf,Assess,Platforms,TRUE,"Zarf is a declarative package manager for offline and semi-connected Kubernetes environments. With Zarf, you can build and configure applications while connected to the internet; once created, you can package and ship to a disconnected environment for deployment. As a standalone tool, Zarf packs several useful features, including automatic Software Bill of Materials (SBOM) generation, built-in Docker registry, Gitea and K9s dashboards to manage clusters from the terminal. Air-gap software delivery for cloud-native applications has its challenges; Zarf addresses most of them."
ZITADEL,Assess,Platforms,TRUE,"ZITADEL is an open-source identity and user management tool, and an alternative to Keycloak. It's lightweight (written in Golang), has flexible deployment options and is easy to configure and manage. It's also multi-tenant, offers comprehensive features for building secure and scalable authentication systems, particularly for B2B applications, and has built-in security features like multi-factor authentication and audit trails. By using ZITADEL, developers can reduce development time, enhance application security and achieve scalability for growing user bases. If you're looking for a user-friendly, secure and open-source tool for user management, ZITADEL is a strong contender."




Conan,Adopt,Tools,TRUE,""
Kaniko,Adopt,Tools,FALSE,""
Karpenter,Adopt,Tools,FALSE,""

42Crunch API Conformance Scan,Trial,Tools,TRUE,""
actions-runner-controller,Trial,Tools,FALSE,""
Android Emulator Container,Trial,Tools,TRUE,""
AWS CUDOS,Trial,Tools,TRUE,""
aws-nuke,Trial,Tools,TRUE,""
Bruno,Trial,Tools,TRUE,""
Develocity,Trial,Tools,TRUE,""
GitHub Copilot,Trial,Tools,FALSE,""
Gradio,Trial,Tools,FALSE,""
Gradle Version Catalog,Trial,Tools,TRUE,""
Maestro,Trial,Tools,FALSE,""
Microsoft SBOM tool,Trial,Tools,TRUE,""
Open Policy Agent (OPA),Trial,Tools,FALSE,""
Philips's self-hosted GitHub runner,Trial,Tools,FALSE,""
Pop,Trial,Tools,TRUE,""
Renovate,Trial,Tools,TRUE,""
Terrascan,Trial,Tools,TRUE,""
Velero,Trial,Tools,TRUE,""

aider,Assess,Tools,TRUE,""
Akvorado,Assess,Tools,TRUE,""
Baichuan 2,Assess,Tools,TRUE,""
Cargo Lambda,Assess,Tools,TRUE,""
Codium AI,Assess,Tools,TRUE,""
Continue,Assess,Tools,TRUE,""
Fern Docs,Assess,Tools,TRUE,""
Granted,Assess,Tools,TRUE,""
LinearB,Assess,Tools,TRUE,""
LLaVA,Assess,Tools,TRUE,""
Marimo,Assess,Tools,TRUE,""
Mixtral,Assess,Tools,TRUE,""
NeMo Guardrails,Assess,Tools,TRUE,""
Ollama,Assess,Tools,TRUE,""
OpenTofu,Assess,Tools,TRUE,""
QAnything,Assess,Tools,TRUE,""
System Initiative,Assess,Tools,TRUE,""
Tetragon,Assess,Tools,TRUE,""
Winglang,Assess,Tools,TRUE,""




Astro,Trial,Languages & Frameworks,FALSE,""
DataComPy,Trial,Languages & Frameworks,TRUE,""
Pinia,Trial,Languages & Frameworks,TRUE,""
Ray,Trial,Languages & Frameworks,TRUE,""

Android Adaptability,Assess,Languages & Frameworks,TRUE,""
Concrete ML,Assess,Languages & Frameworks,TRUE,""
Crabviz,Assess,Languages & Frameworks,TRUE,""
Crux,Assess,Languages & Frameworks,TRUE,""
Databricks Asset Bundles,Assess,Languages & Frameworks,TRUE,""
Electric,Assess,Languages & Frameworks,TRUE,""
LiteLLM,Assess,Languages & Frameworks,TRUE,""
LLaMA-Factory,Assess,Languages & Frameworks,TRUE,""
MLX,Assess,Languages & Frameworks,TRUE,""
Mojo,Assess,Languages & Frameworks,TRUE,""
Otter,Assess,Languages & Frameworks,TRUE,""
Pkl,Assess,Languages & Frameworks,TRUE,""
Rust for UI,Assess,Languages & Frameworks,TRUE,""
vLLM,Assess,Languages & Frameworks,TRUE,""
Voyager,Assess,Languages & Frameworks,TRUE,""
WGPU,Assess,Languages & Frameworks,TRUE,""
Zig,Assess,Languages & Frameworks,FALSE,""

LangChain,Hold,Languages & Frameworks,FALSE,""
