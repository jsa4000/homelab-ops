{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Homelab","text":""},{"location":"#what-is-a-homelab","title":"What is a Homelab?","text":"<p>Homelab is the name given for a server-based setup that resides locally in your home that can host several applications or virtualized systems. A homelab can be used for many purposes such as learning, developing, testing, media server, backup etc..</p> <p>Server can go go from simple tower, mini-PC or Single Board Computers (SBC), such as Raspberry Pi or OrangePi, to more powerful devices or repurposed rack servers that you can acquire from companies who discard them due to their age but are still usable.</p>"},{"location":"#usages","title":"Usages","text":"<p>Depending on your needs or goals a Homelab can have different usages:</p> <ul> <li>It provides you a playground for learning and breaking things.</li> <li>It lets you work and experiment with projects that you cannot try on the job itself.</li> <li>It allow you to perform backup to you personal data or storing secrets and passwords, so you do not rely on SaaS or external providers.</li> <li>If you want host media like movies, photos, music and eBooks.</li> <li>If you want to allow home automation that puts local control and privacy first.</li> </ul>"},{"location":"#my-homelab","title":"My Homelab","text":"<p>My Homelab is bases on Single Board Computers (SBC) based on ARM Architecture family so the energy consumption is low and cheap. The SBC chosen are OrangePI 5 that are very powerful boards and similar to Raspberry Pi 5 models, but with less support from the community.</p> <p></p>"},{"location":"#costs","title":"Costs","text":"<p>Building a homelab can be cheap or expensive depending on the number of servers, components, maintenance costs or energy consumption. From the component perspective my homelab setup are broken into the following items:</p> Component Units Cost/Unit Link Orange Pi 5 8GB RAM Single Board Computer RK3588S PCIE Module External Wifi6.0+BT5.0 SSD 3 91,39\u20ac AliExpress Orange Pi 5 Acrylic Case Transparent Enclosure Clear Shell Optional Cooling Fan Copper Aluminum Heat Sinks 3 5,39\u20ac AliExpress Gigabit Network Switch Mini Desktop 5Ports Switch Ethernet 1000Mbps Hi 1 16,17\u20ac AliExpress 6pack Ethernet Network Cable RJ45 1 6,32\u20ac AliExpress Transcend MTE300S 512GB NVMe PCIe Gen3 x4 M.2 2230 Internal Solid State Drive (SSD) 3 50,47\u20ac Amazon SanDisk Extreme 64 GB MicroSDXC UHS-I Class 10 2 11,99\u20ac Amazon Poppstar Juego de Tornillos de Ordenador (para M.2 SSD/NVMe montaje para placa base de ASUS y ASROCK) 1 6,59\u20ac Amazon VOOMY Cube Regleta Enchufe, 9-en-1 Alargador Enchufe Cubo, Regleta Enchufes con 5 Tomas y 4 USB Puertos 1 27,95\u20ac Amazon Geekworm Raspberry Pi 4 Alimentador Power Supply, USB-C 5V 4A 20W Power Adapter 3 13,89\u20ac Amazon Zigbee 3.0 USB Dongle Plus,SONOFF Zigbee Gateway, Zigbee USB Hub 1 26,69\u20ac Amazon ZigBee Enchufe Inteligente Alexa 16A, 3680W Smart Plug con Monitor de Energ\u00eda 1 15,29\u20ac Amazon TOTAL 606,41\u20ac"},{"location":"#cluster","title":"Cluster","text":"FrontLeftTopBack <p>Top View</p> <p></p> <p>Back View</p> <p></p> <p>Back View</p> <p></p> <p>Back View</p>"},{"location":"#devices","title":"Devices","text":"ONT Network Router 1GEHUAWEI WiFi AX3 ProSONOFF Zigbee 3.0 USB Dongle Plus V2 <p>Top View</p> <p></p> <p>Top View</p> <p></p> <p>Top View</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"layers/messaging/keda/","title":"Keda","text":"<p>If you want the number of your deployment's replicas to be managed by <code>Horizontal Pod Autoscaler</code>, then you would not want to track replicas in <code>Git</code>.</p> <ul> <li>Leaving Room For Imperativeness</li> </ul>"},{"location":"layers/messaging/keda/#references","title":"References","text":"<ul> <li>Scaling of Deployments and StatefulSets</li> <li>Maximize Cost Savings by Putting Your Kubernetes Resources to Sleep During Off-Hours</li> </ul>"},{"location":"layers/networking/cilium/","title":"Cilium","text":"<p>Cilium is a networking, observability, and security solution with an eBPF-based dataplane. It provides a simple flat Layer 3 network (OSI Model) with the ability to span multiple clusters in either a native routing or overlay mode. It is L7-protocol aware and can enforce network policies on L3-L7 using an identity based security model that is decoupled from network addressing.</p> <p>Cilium implements distributed load balancing for traffic between pods and to external services, and is able to fully replace <code>kube-proxy</code>, using efficient hash tables in eBPF allowing for almost unlimited scale. It also supports advanced functionality like integrated ingress and egress gateway, bandwidth management and service mesh, and provides deep network and security visibility and monitoring.</p> <p>A new Linux kernel technology called <code>eBPF</code> is at the foundation of Cilium. It supports dynamic insertion of eBPF bytecode into the Linux kernel at various integration points such as: network IO, application sockets, and tracepoints to implement security, networking and visibility logic. eBPF is highly efficient and flexible. To learn more about eBPF, visit <code>eBPF.io</code>.</p> <p></p> <p>This is an overview of the main functionalities of Cilium:</p> <ul> <li>Protect and secure APIs transparently</li> <li>Secure service to service communication based on identities</li> <li>Secure access to and from external services</li> <li>Simple Networking</li> <li>Load Balancing</li> <li>Bandwidth Management</li> <li>Monitoring and Troubleshooting</li> </ul>"},{"location":"layers/networking/cilium/#components","title":"Components","text":"<p>A deployment of Cilium and Hubble consists of the following components running in a cluster:</p> <p></p>"},{"location":"layers/networking/cilium/#cilium_1","title":"Cilium","text":""},{"location":"layers/networking/cilium/#the-cilium-agent","title":"The Cilium agent","text":"<p>The Cilium agent (<code>cilium-agent</code>) runs on each node in the cluster. At a high-level, the agent accepts configuration via Kubernetes or APIs that describes networking, service load-balancing, network policies, and visibility &amp; monitoring requirements.</p> <p>The Cilium agent listens for events from orchestration systems such as Kubernetes to learn when containers or workloads are started and stopped. It manages the eBPF programs which the Linux kernel uses to control all network access in / out of those containers.</p>"},{"location":"layers/networking/cilium/#the-cilium-cli-client","title":"The Cilium CLI client","text":"<p>The Cilium CLI client (<code>cilium</code>) is a command-line tool that is installed along with the Cilium agent. It interacts with the REST API of the Cilium agent running on the same node. The CLI allows inspecting the state and status of the local agent. It also provides tooling to directly access the eBPF maps to validate their state.</p> <p>Note</p> <p>The in-agent Cilium CLI client described here should not be confused with the command line tool for quick-installing, managing and troubleshooting Cilium on Kubernetes clusters, which also has the name cilium. That tool is typically installed remote from the cluster, and uses <code>kubeconfig</code> information to access Cilium running on the cluster via the Kubernetes API.</p>"},{"location":"layers/networking/cilium/#the-cilium-operator","title":"The Cilium operator","text":"<p>The Cilium Operator is responsible for managing duties in the cluster which should logically be handled once for the entire cluster, rather than once for each node in the cluster. The Cilium operator is not in the critical path for any forwarding or network policy decision. A cluster will generally continue to function if the operator is temporarily unavailable. However, depending on the configuration, failure in availability of the operator can lead to:</p> <ul> <li>Delays in IP Address Management (IPAM) and thus delay in scheduling of new workloads if the operator is required to allocate new IP addresses</li> <li>Failure to update the kvstore heartbeat key which will lead agents to declare kvstore unhealthiness and restart.</li> </ul>"},{"location":"layers/networking/cilium/#the-cni-plugin","title":"The CNI plugin","text":"<p>The CNI plugin (<code>cilium-cni</code>) is invoked by Kubernetes when a pod is scheduled or terminated on a node. It interacts with the Cilium API of the node to trigger the necessary datapath configuration to provide networking, load-balancing and network policies for the pod.</p>"},{"location":"layers/networking/cilium/#hubble","title":"Hubble","text":""},{"location":"layers/networking/cilium/#hubble-server","title":"Hubble Server","text":"<p>The Hubble server runs on each node and retrieves the eBPF-based visibility from Cilium. It is embedded into the Cilium agent in order to achieve high performance and low-overhead. It offers a gRPC service to retrieve flows and Prometheus metrics.</p>"},{"location":"layers/networking/cilium/#hubble-relay","title":"Hubble Relay","text":"<p>Hubble Relay (<code>hubble-relay</code>) is a standalone component which is aware of all running Hubble servers and offers cluster-wide visibility by connecting to their respective gRPC APIs and providing an API that represents all servers in the cluster.</p>"},{"location":"layers/networking/cilium/#the-hubble-cli","title":"The Hubble CLI","text":"<p>The Hubble CLI (<code>hubble</code>) is a command-line tool able to connect to either the gRPC API of <code>hubble-relay</code> or the local server to retrieve flow events.</p>"},{"location":"layers/networking/cilium/#the-hubble-ui","title":"The Hubble UI","text":"<p>The Hubble UI (<code>hubble-ui</code>) utilizes relay-based visibility to provide a graphical service dependency and connectivity map.</p>"},{"location":"layers/networking/cilium/#ebpf","title":"eBPF","text":"<p>Historically, the operating system has always been an ideal place to implement observability, security, and networking functionality due to the kernel's privileged ability to oversee and control the entire system. At the same time, an operating system kernel is hard to evolve due to its central role and high requirement towards stability and security. The rate of innovation at the operating system level has thus traditionally been lower compared to functionality implemented outside of the operating system.</p> <p>BPF (Berkeley Packet Filter) is a revolutionary technology with origins in the Linux kernel that can run sandboxed programs in a privileged context such as the operating system kernel. It is used to safely and efficiently extend the capabilities of the kernel without requiring to change kernel source code or load kernel modules.</p> <p>eBPF (extended Berkeley Packet Filter) is a modern Linux kernel bytecode interpreter originally introduced to filter network packets, e.g. tcpdump and socket filters at runtime. It has been extended with additional data structures such as hashtable and arrays as well as additional actions to support packet mangling, forwarding, encapsulation, etc. An in-kernel verifier ensures that eBPF programs are safe to run and a <code>Just-In-Time</code> (JIT) compiler converts the bytecode to CPU architecture specific instructions for native execution efficiency. eBPF programs can be run at various hooking points (events) in the kernel such as for incoming and outgoing packets, execute binaries, etc..</p> <p></p> <p>Today, eBPF is used extensively to drive a wide variety of use cases: Providing high-performance networking and load-balancing in modern data centers and cloud native environments, extracting fine-grained security observability data at low overhead, helping application developers trace applications and observability, providing insights for performance troubleshooting, preventive application and container runtime security enforcement, and much more. The possibilities are endless, and the innovation that eBPF is unlocking has only just begun.</p> <p>Although eBPF was initially designed for the kernel, its tremendous potential in user space, coupled with the kernel's GPL LICENSE restrictions. These runtimes allowed developers to execute eBPF bytecode outside the kernel, breaking free from GPL license restrictions and offering a more intuitive and convenient debugging environment.</p> <p></p> <p>Cilium is capable of probing the Linux kernel for available features and will automatically make use of more recent features as they are detected.</p> <p></p>"},{"location":"layers/networking/cilium/#data-store","title":"Data Store","text":"<p>Cilium requires a data store to propagate state between agents. It supports the following data stores:</p>"},{"location":"layers/networking/cilium/#kubernetes-crds-default","title":"Kubernetes CRDs (Default)","text":"<p>The default choice to store any data and propagate state is to use Kubernetes custom resource definitions (CRDs). CRDs are offered by Kubernetes for cluster components to represent configurations and state via Kubernetes resources.</p>"},{"location":"layers/networking/cilium/#key-value-store","title":"Key-Value Store","text":"<p>All requirements for state storage and propagation can be met with Kubernetes CRDs as configured in the default configuration of Cilium. A key-value store can optionally be used as an optimization to improve the scalability of a cluster as change notifications and storage requirements are more efficient with direct key-value store usage.</p> <p>The currently supported key-value stores are base on <code>etcd</code>.</p>"},{"location":"layers/networking/cilium/#feature","title":"Feature","text":""},{"location":"layers/networking/cilium/#security","title":"Security","text":"<ul> <li>Network Traffic</li> <li>File Activity</li> <li>Running Executables</li> <li>Changing Priviledges</li> </ul>"},{"location":"layers/networking/cilium/#features","title":"Features","text":""},{"location":"layers/networking/cilium/#routing","title":"Routing","text":"<p>Routing</p>"},{"location":"layers/networking/cilium/#kube-proxy-replacement","title":"Kube Proxy Replacement","text":"<p>The kube-proxy replacement offered by Cilium's CNI is a very powerful feature which can increase performance for large Kubernetes clusters. This feature uses an eBPF data plane to replace the kube-proxy implementations offered by Kubernetes distributions typically implemented with either iptables or ipvs. When using other networking infrastructure, the otherwise hidden Cilium eBPF implementation used to replace kube-proxy can bleed through and provide unintended behaviors. We see this when trying to use Istio service mesh with Cilium's kube-proxy replacement: kube-proxy replacement, by default, breaks Istio.</p>"},{"location":"layers/networking/cilium/#loadbalancer-ip-address-management-lb-ipam","title":"LoadBalancer IP Address Management (LB-IPAM)","text":"<p>In Cilium 1.13, it was added support for LoadBalancer IP Address Management (LB-IPAM) and the ability to allocate IP addresses to Kubernetes Services of the type <code>LoadBalancer</code>.</p> <p></p> <p>For users who do not want to use BGP or that just want to make these IP addresses accessible over the local network, in Cilium 1.14 a new feature called L2 Announcements was added. When you deploy a L2 Announcement Policy, Cilium will start responding to Address Resolution Protocol (<code>ARP</code>) requests from local clients for ExternalIPs and/or LoadBalancer IPs. Only one node at a time are attached to the IP Address and when a nodes dies cilium will perform another leader election to stablish the new node with the IP.</p> <p>note !!!</p> <pre><code>L2 is **not** doing real load balancing over your requests to the different nodes, since it uses `ARP` to get the current node attached to that IP; only one node can be attached to that IP Address. In order to use load balancing you may use different modes such as `BGP` or real load balancers.\n</code></pre> <p></p> <p>Note</p> <p>Typically, this would have required a tool like <code>MetalLB</code> but Cilium now natively supports this functionality. Cilium doesn't use <code>MetalLB</code> anymore, now it uses its own <code>BGP</code> speaker made with <code>gobgp</code>. They used it initially until they reached feature parity with gobgp.</p> <p><code>LB-IPAM</code> works seamlessly with Cilium <code>BGP</code>. The IP addresses allocated by Cilium can be advertised to <code>BGP</code> peers to integrate your cluster with the rest of your network.</p> <p>Cloud providers natively provide this feature for managed Kubernetes Services and therefore this feature is more one for self-managed Kubernetes deployments or home labs.</p>"},{"location":"layers/networking/cilium/#bgp-border-gateway-protocol","title":"BGP (Border Gateway Protocol)","text":"<p>BGP is a routing protocol.</p> <p>DNS (domain name system) servers provide the IP address, but BGP provides the most efficient way to reach that IP address. Roughly speaking, if DNS is the Internet's address book, then BGP is the Internet's road map (Tomtom).</p>"},{"location":"layers/networking/cilium/#threat-model","title":"Threat Model","text":"<p>Threat Model</p>"},{"location":"layers/networking/cilium/#observability","title":"Observability","text":"<p>Hubble is a fully distributed networking and security observability platform. It is built on top of Cilium and eBPF to enable deep visibility into the communication and behavior of services as well as the networking infrastructure in a completely transparent manner.</p> <p>By building on top of Cilium, Hubble can leverage eBPF for visibility. By relying on eBPF, all visibility is programmable and allows for a dynamic approach that minimizes overhead while providing deep and detailed visibility as required by users. Hubble has been created and specifically designed to make best use of these new eBPF powers.</p> <p></p> <p>Hubble can answer questions such as:</p>"},{"location":"layers/networking/cilium/#service-dependencies-communication-map","title":"Service dependencies &amp; communication map","text":"<ul> <li>What services are communicating with each other? How frequently? What does the service dependency graph look like?</li> <li>What HTTP calls are being made? What Kafka topics does a service consume from or produce to?</li> </ul>"},{"location":"layers/networking/cilium/#network-monitoring-alerting","title":"Network monitoring &amp; alerting","text":"<ul> <li>Is any network communication failing? Why is communication failing? Is it DNS? Is it an application or network problem? Is the communication broken on layer 4 (TCP) or layer 7 (HTTP)?</li> <li>Which services have experienced a DNS resolution problem in the last 5 minutes? Which services have experienced an interrupted TCP connection recently or have seen connections timing out? What is the rate of unanswered TCP SYN requests?</li> </ul>"},{"location":"layers/networking/cilium/#application-monitoring","title":"Application monitoring","text":"<ul> <li>What is the rate of 5xx or 4xx HTTP response codes for a particular service or across all clusters?</li> <li>What is the 95th and 99th percentile latency between HTTP requests and responses in my cluster? Which services are performing the worst? What is the latency between two services?</li> </ul>"},{"location":"layers/networking/cilium/#security-observability","title":"Security observability","text":"<ul> <li>Which services had connections blocked due to network policy? What services have been accessed from outside the cluster? Which services have resolved a particular DNS name?</li> </ul>"},{"location":"layers/networking/cilium/#service-mesh","title":"Service Mesh","text":"<p>Service Mesh</p>"},{"location":"layers/networking/cilium/#installation","title":"Installation","text":"<p>These are specific instructions on how to install Cilium depending on the Kubernetes cluster. However, the cilium installer will attempt to automatically pick the best configuration options for you. Check other installation options for distribution/platform specific to choose the ideal default configuration for each.</p>"},{"location":"layers/networking/cilium/#cli","title":"CLI","text":"<p>The Cilium CLI can be used to install Cilium, inspect the state of a Cilium installation, and enable/disable various features (e.g. clustermesh, Hubble).</p> <p>Cilium cli can be installed in different ways using following instructions.</p> <pre><code># Use homebrew to install cilium cli\nbrew install cilium-cli\n\n# Test the version\ncilium version --client\n\ncilium-cli: v0.16.4 compiled with go1.22.1 on darwin/arm64\ncilium image (default): v1.15.3\ncilium image (stable): v1.15.3\n</code></pre>"},{"location":"layers/networking/cilium/#cni","title":"CNI","text":"<p>There are several ways to install Cilium by using cilium cli, helm, kustomize, etc... In addition, cilium is recommended to be installed in a kubernetes distribution with no CNI pre-installed. There are several ways to prevent CNI to be installed in some distributions such as <code>K3s</code>, <code>KinD</code>, etc..</p> <p>Note</p> <p>Depending on the kubernetes distribution GKE, AKS, EKS, Openshift, K3s, etc.. cilium need to be installed using custom configuration.</p> <pre><code># For K3s cluster it's necessary to disable current CNI (flannel) and Network Policies.\ncurl -sfL https://get.k3s.io | INSTALL_K3S_EXEC='--flannel-backend=none --disable-network-policy' sh -\n</code></pre> <pre><code># Using the cilium cli, this will use helm behind the scenes.\ncilium install --version 1.15.3\n\n# Or Using directly helm\nhelm repo add cilium https://helm.cilium.io/\nhelm install cilium cilium/cilium --version 1.15.3 \\\n   --namespace kube-system \\\n   --set operator.replicas=1\n</code></pre> <p>Restart</p> <p>If you did not create a cluster with the nodes tainted with the taint <code>node.cilium.io/agent-not-ready</code>, then unmanaged pods need to be restarted manually. Restart all already running pods which are not running in host-networking mode to ensure that Cilium starts managing them. This is required to ensure that all pods which have been running before Cilium was deployed have network connectivity provided by Cilium and NetworkPolicy applies to them.</p> <pre><code>kubectl get pods --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,HOSTNETWORK:.spec.hostNetwork --no-headers=true | grep '&lt;none&gt;' | awk '{print \"-n \"$1\" \"$2}' | xargs -L 1 -r kubectl delete pod\n</code></pre> <p>If the containers have a <code>CrashLoopBackOff</code> status after the installation, that means there probable there is other <code>CNI</code> already installed. Check the K3s configuration or installation scripts.</p> <pre><code>level=info msg=\"Stop hook executed\" duration=\"549.792\u00b5s\" function=\"gops.registerGopsHooks.func2 (pkg/gops/cell.go:50)\" subsys=hive\nlevel=fatal msg=\"failed to start: daemon creation failed: error while initializing daemon: failed while reinitializing datapath: failed to setup vxlan tunnel device: setting up vxlan device: creating vxlan device: setting up device cilium_vxlan: address already in use\" subsys=daemon\n\u00b4\u00b4\u00b4\n\nValidate Cilium has been properly installed into kubernetes cluster running following command.\n\n```bash\n# Check cilium status by using the following command\ncilium status --wait -n networking\n\n    /\u00af\u00af\\\n /\u00af\u00af\\__/\u00af\u00af\\    Cilium:             OK\n \\__/\u00af\u00af\\__/    Operator:           OK\n /\u00af\u00af\\__/\u00af\u00af\\    Envoy DaemonSet:    disabled (using embedded mode)\n \\__/\u00af\u00af\\__/    Hubble Relay:       OK\n    \\__/       ClusterMesh:        disabled\n\nDeployment             hubble-ui          Desired: 1, Ready: 1/1, Available: 1/1\nDeployment             cilium-operator    Desired: 1, Ready: 1/1, Available: 1/1\nDeployment             hubble-relay       Desired: 1, Ready: 1/1, Available: 1/1\nDaemonSet              cilium             Desired: 3, Ready: 3/3, Available: 3/3\nContainers:            hubble-relay       Running: 1\n                       cilium             Running: 3\n                       hubble-ui          Running: 1\n                       cilium-operator    Running: 1\nCluster Pods:          5/5 managed by Cilium\nHelm chart version:\nImage versions         hubble-relay       quay.io/cilium/hubble-relay:v1.15.3@sha256:b9c6431aa4f22242a5d0d750c621d9d04bdc25549e4fb1116bfec98dd87958a2: 1\n                       cilium             quay.io/cilium/cilium:v1.15.3@sha256:da74ab61d1bc665c1c088dff41d5be388d252ca5800f30c7d88844e6b5e440b0: 3\n                       hubble-ui          quay.io/cilium/hubble-ui:v0.13.0@sha256:7d663dc16538dd6e29061abd1047013a645e6e69c115e008bee9ea9fef9a6666: 1\n                       hubble-ui          quay.io/cilium/hubble-ui-backend:v0.13.0@sha256:1e7657d997c5a48253bb8dc91ecee75b63018d16ff5e5797e5af367336bc8803: 1\n                       cilium-operator    quay.io/cilium/operator-generic:v1.15.3@sha256:c97f23161906b82f5c81a2d825b0646a5aa1dfb4adf1d49cbb87815079e69d61: 1\n\n# Check cilium configuration options\ncilium config view -n networking\n\n#\u00a0Filter configuration by keyword\ncilium config view -n networking  | grep l2\n\nenable-l2-announcements                           true\nenable-l2-neigh-discovery                         true\n</code></pre> <p>Run the following command to validate that your cluster has proper network connectivity.</p> <pre><code># Check the connectivity test\ncilium connectivity test -n networking\n\n# Check for any errors occurs during the tests.\n\n\u2705 All 47 tests (500 actions) successful, 28 tests skipped, 0 scenarios skipped.\n\n# Check the connectivity performance test\ncilium connectivity perf -n networking\n\n\n\ud83d\udd25 Network Performance Test Summary:\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\ud83d\udccb Scenario        | Node       | Test            | Duration        | Min             | Mean            | Max             | P50             | P90             | P99             | Transaction rate OP/s\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\ud83d\udccb pod-to-pod      | same-node  | TCP_RR          | 10s             | 21\u00b5s            | 57.46\u00b5s         | 5.027ms         | 36\u00b5s            | 131\u00b5s           | 300\u00b5s           | 17309.24\n\ud83d\udccb pod-to-pod      | same-node  | UDP_RR          | 10s             | 23\u00b5s            | 36.45\u00b5s         | 6.791ms         | 31\u00b5s            | 48\u00b5s            | 114\u00b5s           | 27297.16\n\ud83d\udccb pod-to-pod      | same-node  | TCP_CRR         | 10s             | 109\u00b5s           | 152.48\u00b5s        | 7.509ms         | 129\u00b5s           | 189\u00b5s           | 480\u00b5s           | 6543.34\n\ud83d\udccb pod-to-pod      | other-node | TCP_RR          | 10s             | 534\u00b5s           | 1.53132ms       | 14.486ms        | 1.5ms           | 1.72ms          | 3.375ms         | 651.44\n\ud83d\udccb pod-to-pod      | other-node | UDP_RR          | 10s             | 620\u00b5s           | 1.73095ms       | 16.413ms        | 1.654ms         | 1.934ms         | 5.8ms           | 576.52\n\ud83d\udccb pod-to-pod      | other-node | TCP_CRR         | 10s             | 1.748ms         | 4.96712ms       | 19.845ms        | 4.85ms          | 6.576ms         | 10.2ms          | 201.16\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n-------------------------------------------------------------------------------------\n\ud83d\udccb Scenario        | Node       | Test            | Duration        | Throughput Mb/s\n-------------------------------------------------------------------------------------\n\ud83d\udccb pod-to-pod      | same-node  | TCP_STREAM      | 10s             | 1093.77\n\ud83d\udccb pod-to-pod      | same-node  | UDP_STREAM      | 10s             | 1245.94\n\ud83d\udccb pod-to-pod      | other-node | TCP_STREAM      | 10s             | 837.18\n\ud83d\udccb pod-to-pod      | other-node | UDP_STREAM      | 10s             | 323.12\n-------------------------------------------------------------------------------------\n</code></pre>"},{"location":"layers/networking/cilium/#hubble_1","title":"Hubble","text":"<p>Observability is provided by Hubble which enables deep visibility into the communication and behavior of services as well as the networking infrastructure in a completely transparent manner.</p> <p>By default, Hubble API operates within the scope of the individual node on which the Cilium agent runs. This confines the network insights to the traffic observed by the local Cilium agent. Hubble CLI (<code>hubble</code>) can be used to query the Hubble API provided via a local Unix Domain Socket. The Hubble CLI binary is installed by default on Cilium agent pods.</p> <p>Upon deploying Hubble Relay, network visibility is provided for the entire cluster or even multiple clusters in a ClusterMesh scenario. In this mode, Hubble data can be accessed by directing Hubble CLI (<code>hubble</code>) to the Hubble Relay service or via Hubble UI. Hubble UI is a web interface which enables automatic discovery of the services dependency graph at the L3/L4 and even L7 layer, allowing user-friendly visualization and filtering of data flows as a service map.</p> <p>First you will need to install <code>hubble relay</code> and <code>hubble ui</code> into Kubernetes.</p> <p>Note</p> <p>Cilium installer allows to configure <code>hubble</code> service with TLS, metrics, port and other features.</p> <pre><code># Cilium must be installed previously in order to be install new features from cli\n\n#\u00a0Using cilium cli (without ui)\ncilium hubble enable\n\n# Enable cilium with ui (hubble must be disabled)\ncilium hubble enable --ui\n\n#Updated helm by enabling specific hubble features.\nhelm upgrade cilium cilium/cilium --version 1.15.3 \\\n   --namespace kube-system \\\n   --reuse-values \\\n   --set hubble.relay.enabled=true \\\n   --set hubble.ui.enabled=true\n</code></pre> <p>Install <code>hubble</code> cli to interact with Hubble API for observability and troubleshooting.</p> <pre><code># Install hubble using homebrew\nbrew install hubble\n</code></pre> <p>Validate the hubble installation</p> <pre><code>#\u00a0Expose the Hubble API by using Port forward on port :4245\ncilium hubble port-forward -n networking\n\n# In another terminal use following command to test the hubble status\nhubble status\n\n# List all nodes\nhubble list nodes\n\nNAME       STATUS      AGE       FLOWS/S   CURRENT/MAX-FLOWS\nserver-1   Connected   5h7m50s   14.09     4095/4095 (100.00%)\nserver-2   Connected   5h7m56s   19.43     4095/4095 (100.00%)\nserver-3   Connected   5h7m49s   14.04     4095/4095 (100.00%)\n\n# You can also query the flow API and look for flows\nhubble observe\n</code></pre> <p>Open the Hubble UI using cilium cli</p> <pre><code># Use following command for port forwarding (port 12000)\ncilium hubble ui -n networking\n\n# Run the cilium test to get some traffic (different terminal)\nwhile true; do cilium connectivity test -n networking; done\n\n#\u00a0http://localhost:12000/\n</code></pre>"},{"location":"layers/networking/cilium/#demo","title":"Demo","text":"<p>In the Star Wars-inspired example from Cilium, there are three microservices applications: <code>deathstar</code>, <code>tiefighter</code>, and <code>xwing</code>.</p> <ul> <li>The <code>deathstar</code> runs an HTTP webservice on port 80, which is exposed as a Kubernetes <code>Service</code> to load-balance requests to <code>deathstar</code> across two pod replicas. The deathstar service provides landing services to the empire's spaceships so that they can request a landing port.</li> <li>The <code>tiefighter</code> pod represents a landing-request client service on a typical <code>empire</code> ship.</li> <li>The <code>xwing</code> represents a similar service on an <code>alliance</code> ship.</li> </ul> <p></p> <p>They exist so that we can test different security policies for access control to deathstar landing services.</p>"},{"location":"layers/networking/cilium/#deployment","title":"Deployment","text":"<p>The file <code>http-sw-app.yaml</code> contains a Kubernetes Deployment for each of the three services. Each deployment is identified using the Kubernetes labels (<code>org=empire, class=deathstar</code>), (<code>org=empire, class=tiefighter</code>), and (<code>org=alliance, class=xwing</code>). It also includes a <code>deathstar-service</code>, which load-balances traffic to all pods with label (<code>org=empire, class=deathstar</code>).</p> <p>This is an example of the <code>deathstar</code> deployment with the labels <code>org=empire, class=deathstar</code>.</p> http-sw-app.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deathstar\n  labels:\n    app.kubernetes.io/name: deathstar\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      org: empire\n      class: deathstar\n  template:\n    metadata:\n      labels:\n        org: empire\n        class: deathstar\n        app.kubernetes.io/name: deathstar\n    spec:\n      containers:\n        - name: deathstar\n          image: quay.io/cilium/starwars:v2.1@sha256:833d915ec68fca3ce83668fc5dae97c455b2134d8f23ef96586f55b894cfb1e8\n</code></pre> <pre><code># Deploy the base demo resources into default namespace\nkubectl create -f https://raw.githubusercontent.com/jsa4000/homelab-ops/main/docs/_source/layers/networking/cilium/http-sw-app.yaml\n\nservice/deathstar created\ndeployment.apps/deathstar created\npod/tiefighter created\npod/xwing created\n\n# Get resources deployeed\nkubectl get,services pods\n\n# Check the pod statuses\nkubectl get pods -w\n</code></pre> <p>Each <code>pod</code> will be represented in Cilium as an <code>Endpoint</code> in the local cilium agent. We can invoke the cilium tool inside the Cilium pod to list them (in a single-node installation <code>kubectl -n networking exec ds/cilium -- cilium-dbg endpoint list</code> lists them all, but in a multi-node installation, only the ones running on the same node will be listed)</p> <p>Get the ingress and egress enforcements policies with the identities and labels assigned by cilium currently applied to these pods/endpoints.</p> <pre><code># List all endpoints (pods) available in the current node (each agent manage it's own pods)\nkubectl -n networking exec ds/cilium -- cilium-dbg endpoint list\n\nDefaulted container \"cilium-agent\" out of: cilium-agent, config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init)\nENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                              IPv6   IPv4          STATUS\n           ENFORCEMENT        ENFORCEMENT\n\n1453       Disabled           Disabled          45221      k8s:app.kubernetes.io/name=deathstar                                            10.52.1.208   ready\n                                                           k8s:class=deathstar\n                                                           k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=default\n                                                           k8s:io.cilium.k8s.policy.cluster=default\n                                                           k8s:io.cilium.k8s.policy.serviceaccount=default\n                                                           k8s:io.kubernetes.pod.namespace=default\n                                                           k8s:org=empire\n\n# This information can be also gotten from CRD (ciliumnetworkpolicy or cnp)\nkubectl get cep\nkubectl get cep -o wide\n\nNAME                        SECURITY IDENTITY   INGRESS ENFORCEMENT   EGRESS ENFORCEMENT   VISIBILITY POLICY   ENDPOINT STATE   IPV4          IPV6\ndeathstar-b4b8ccfb5-w84sn   6822                &lt;status disabled&gt;     &lt;status disabled&gt;    &lt;status disabled&gt;   ready            10.52.1.239\ndeathstar-b4b8ccfb5-f75xw   6822                &lt;status disabled&gt;     &lt;status disabled&gt;    &lt;status disabled&gt;   ready            10.52.2.36\ntiefighter                  25698               &lt;status disabled&gt;     &lt;status disabled&gt;    &lt;status disabled&gt;   ready            10.52.2.26\nxwing                       63406               &lt;status disabled&gt;     &lt;status disabled&gt;    &lt;status disabled&gt;   ready            10.52.1.220\n</code></pre>"},{"location":"layers/networking/cilium/#check-current-access","title":"Check Current Access","text":"<p>From the perspective of the <code>deathstar</code> service, only the ships with label <code>org=empire</code> are allowed to connect and request landing. Since we have no rules enforced, both <code>xwing</code> and <code>tiefighter</code> will be able to request landing. To test this, use the commands below.</p> <pre><code># Check connectivity between xwing and deathstar\nkubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\nShip landed\n\n# Check connectivity between tiefighter and deathstar\nkubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\nShip landed\n</code></pre>"},{"location":"layers/networking/cilium/#apply-an-l3l4-policy","title":"Apply an L3/L4 Policy","text":"<p>When using Cilium, endpoint IP addresses are irrelevant when defining security policies. Instead, you can use the labels assigned to the pods to define security policies. The policies will be applied to the right <code>pods</code> based on the labels irrespective of where or when it is running within the cluster.</p> <p>We'll start with the basic policy restricting deathstar landing requests to only the ships that have label (<code>org=empire</code>). This will not allow any ships that don't have the <code>org=empire</code> label to even connect with the deathstar service. This is a simple policy that filters only on IP protocol (<code>network layer 3</code>) and TCP protocol (<code>network layer 4</code>), so it is often referred to as an L3/L4 network security policy.</p> <p>Note</p> <p>Once a <code>NetworkPolicy</code> has been applied it denies automatically the connections that do not satisfied this policy. It's recommended to set a Zero Trust Security policy at a cluster level to deny every connection and start adding Network Policies to enable the trusted connections.</p> <p>The <code>CiliumNetworkPolicy</code> is an upgrade from the standard kubernetes <code>NetworkPolicy</code>. There are two flavours of <code>NetworkPolicies</code> in Cilium, <code>CiliumNetworkPolicy</code> and <code>CiliumClusterwideNetworkPolicy</code>. There are a couple of things you can't do with the <code>NetworkPolicy</code> resource:</p> <ul> <li>You can't use L7 rules - for example, you can't use HTTP methods, headers, or paths in your policies.</li> <li>Anything TLS related</li> <li>You can't write node-specific policies</li> <li>You can't write deny policies and more</li> </ul> <p></p> <p>This Endpoint Based Policy will restrict any <code>ingress</code> network connections to <code>endpoints</code> (<code>pods</code>) with labels <code>class: deathstar</code> and <code>org: empire</code>, so it only allows incoming traffic from <code>endpoints</code> with labels <code>org=empire</code> on port <code>80</code>.</p> <p>Note</p> <p>The layer 3 policy establishes the base connectivity rules regarding which endpoints can talk to each other. Layer 3 policies can be specified using the following methods: Endpoints Based, Services based, Entities Based, IP/CIDR based and DNS based. Check this link for more documentation.</p> sw_l3_l4_policy_endpoint.yaml<pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: \"rule1\"\nspec:\n  description: \"L3-L4 policy to restrict deathstar access to empire ships only\"\n  endpointSelector:\n    matchLabels:\n      org: empire\n      class: deathstar\n  ingress:\n    - fromEndpoints:\n        - matchLabels:\n            org: empire\n      toPorts:\n        - ports:\n            - port: \"80\"\n              protocol: TCP\n</code></pre> <pre><code>#\u00a0Create CiliumNetworkPolicy resource\nkubectl create -f https://raw.githubusercontent.com/jsa4000/homelab-ops/main/docs/_source/layers/networking/cilium/sw_l3_l4_policy.yaml\n\n# This information can be also gotten from CRD (ciliumnetworkpolicy or cnp)\nkubectl get cnp\n\n# Delete the policy to return to previous state\nkubectl delete cnp rule1\n</code></pre> <p>Instead a Service Based policiy can also be applied to enforce network traffic that goes from an endpoint to a specific services or endpoints (<code>egress</code>).</p> sw_l3_l4_policy_service.yaml<pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: \"rule1\"\nspec:\n  description: \"L3-L4 policy to restrict tiefighter ship access to deathstar only\"\n  endpointSelector:\n    matchLabels:\n      org: empire\n      class: tiefighter\n  ingress:\n    - {}\n  egress:\n    - toServices:\n        - k8sService:\n            namespace: default\n            serviceName: deathstar\n      toPorts:\n        - ports:\n            - port: \"80\"\n              protocol: TCP\n---\napiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: \"rule2\"\nspec:\n  description: \"L3-L4 policy to restrict xwing ship access to deathstar only\"\n  endpointSelector:\n    matchLabels:\n      org: alliance\n      class: xwing\n  ingress:\n    - {}\n  egressDeny:\n    - toServices:\n        - k8sService:\n            namespace: default\n            serviceName: deathstar\n      toPorts:\n        - ports:\n            - port: \"80\"\n              protocol: TCP\n</code></pre> <pre><code>#\u00a0Create CiliumNetworkPolicy resource\nkubectl create -f https://raw.githubusercontent.com/jsa4000/homelab-ops/main/docs/_source/layers/networking/cilium/sw_l3_l4_policy_service.yaml\n\n# This information can be also gotten from CRD (ciliumnetworkpolicy or cnp)\nkubectl get cnp\n\n# Delete the policy to return to previous state\nkubectl delete cnp rule1\nkubectl delete cnp rule2\n</code></pre> <p>Now if we run the landing requests again, only the tiefighter pods with the label <code>org=empire</code> will succeed. The xwing pods will be blocked!</p> <pre><code># tiefighter pods with the label org=empire will succeed\nkubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\nShip landed\n\n# xwing pods with the label org!=empire will not succeed\nkubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\n\n# Connection Hangs or Timeout\n</code></pre>"},{"location":"layers/networking/cilium/#inspecting-the-policy","title":"Inspecting the Policy","text":"<p>If we run cilium-dbg endpoint list again we will see that the pods with the label <code>org=empire</code> and <code>class=deathstar</code> now have ingress policy enforcement <code>enabled</code> as per the policy above.</p> <pre><code># Check policy enabled on deathstar endpoint\nkubectl -n networking exec ds/cilium -- cilium-dbg endpoint list\n\nDefaulted container \"cilium-agent\" out of: cilium-agent, config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init)\nENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                              IPv6   IPv4          STATUS\n           ENFORCEMENT        ENFORCEMENT\n\n1453       Enabled            Disabled          45221      k8s:app.kubernetes.io/name=deathstar                                            10.52.1.208   ready\n                                                           k8s:class=deathstar\n                                                           k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=default\n                                                           k8s:io.cilium.k8s.policy.cluster=default\n                                                           k8s:io.cilium.k8s.policy.serviceaccount=default\n                                                           k8s:io.kubernetes.pod.namespace=default\n                                                           k8s:org=empire\n\n# This information can be also gotten from CRD (ciliumnetworkpolicy or cnp)\nkubectl get cep\nkubectl get cep -o wide\n</code></pre>"},{"location":"layers/networking/cilium/#apply-and-test-http-aware-l7-policy","title":"Apply and Test HTTP-aware L7 Policy","text":"<p>In the simple scenario above, it was sufficient to either give <code>tiefighter</code> / <code>xwing</code> full access to deathstar's API or no access at all. But to provide the strongest security (i.e., enforce least-privilege isolation) between microservices, each service that calls deathstar's API should be limited to making only the set of HTTP requests it requires for legitimate operation.</p> <p>For example, consider that the deathstar service exposes some maintenance APIs which should not be called by random empire ships.</p> <p></p> <pre><code># force an error calling to /v1/exhaust-port\nkubectl exec tiefighter -- curl -s -XPUT deathstar.default.svc.cluster.local/v1/exhaust-port\n\nPanic: deathstar exploded\n\ngoroutine 1 [running]:\nmain.HandleGarbage(0x2080c3f50, 0x2, 0x4, 0x425c0, 0x5, 0xa)\n        /code/src/github.com/empire/deathstar/\n        temp/main.go:9 +0x64\nmain.main()\n        /code/src/github.com/empire/deathstar/\n        temp/main.go:5 +0x85\n</code></pre> <p>Cilium is capable of enforcing HTTP-layer (i.e., L7) policies to limit what URLs the tiefighter is allowed to reach. Here is an example policy file that extends our original policy by limiting tiefighter to making only a POST /v1/request-landing API call, but disallowing all other calls (including PUT /v1/exhaust-port).</p> sw_l3_l4_policy_endpoint.yaml<pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: \"rule1\"\nspec:\n  description: \"L7 policy to restrict access to specific HTTP call\"\n  endpointSelector:\n    matchLabels:\n      org: empire\n      class: deathstar\n  ingress:\n    - fromEndpoints:\n        - matchLabels:\n            org: empire\n      toPorts:\n        - ports:\n            - port: \"80\"\n              protocol: TCP\n          rules:\n            http:\n              - method: \"POST\"\n                path: \"/v1/request-landing\"\n</code></pre> <p>Update the existing rule to apply L7-aware policy to protect deathstar using.</p> <pre><code>#\u00a0Create CiliumNetworkPolicy resource\nkubectl apply -f https://raw.githubusercontent.com/jsa4000/homelab-ops/main/docs/_source/layers/networking/cilium/sw_l3_l4_l7_policy.yaml\n</code></pre> <p>We can now re-run the same test as above, but we will see a different outcome.</p> <pre><code>#\u00a0Connection successful\nkubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\nShip landed\n\n# Access denied because L7 Policy\nkubectl exec tiefighter -- curl -s -XPUT deathstar.default.svc.cluster.local/v1/exhaust-port\nAccess denied\n\n# Connection Failed because L3-L4 Policy\nkubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\n</code></pre> <p>Note</p> <p>As you can see, with Cilium L7 security policies, we are able to permit <code>tiefighter</code> to access only the required API resources on <code>deathstar</code>, thereby implementing a least privilege security approach for communication between microservices. Note that path matches the exact url, if for example you want to allow anything under <code>/v1/</code>, you need to use a regular expression.</p> <pre><code>path: \"/v1/.*\"\n</code></pre>"},{"location":"layers/networking/cilium/#locking-down-external-access-with-dns-based-policies","title":"Locking Down External Access with DNS-Based Policies","text":"<p>DNS-based policies are very useful for controlling access to services running outside the Kubernetes cluster. DNS acts as a persistent service identifier for both external services provided by AWS, Google, Twilio, Stripe, etc., and internal services such as database clusters running in private subnets outside Kubernetes. CIDR or IP-based policies are cumbersome and hard to maintain as the IPs associated with external services can change frequently. The Cilium DNS-based policies provide an easy mechanism to specify access control while Cilium manages the harder aspects of tracking DNS to IP mapping.</p> <p>In this example we will take a look to a simple scenario where the Empire's <code>mediabot</code> pods need access to <code>GitHub</code> for managing the Empire's git repositories. The pods shouldn't have access to any other external service.</p> <p></p> <p>Deploy <code>mediabot</code> pod that will attempt to connect to github.</p> <pre><code>#\u00a0Create CiliumNetworkPolicy resource\nkubectl create -f https://raw.githubusercontent.com/jsa4000/homelab-ops/main/docs/_source/layers/networking/cilium/dns-sw-app.yaml\n</code></pre>"},{"location":"layers/networking/cilium/#apply-dns-egress-policy","title":"Apply DNS Egress Policy","text":"<p>Note</p> <p>Since pods needs to resolve DNS or FQDN into IP Address it's necessary to allow access to kube-dns <code>endpoint/pod</code> (allow DNS lookups). Once the policy is apply it denies the rest of the incoming and outcoming traffic that do not match with it.</p> <p>OpenShift users will need to modify the policies to match the namespace<code>openshift-dns</code> (instead of <code>kube-system</code>), remove the match on the <code>k8s:k8s-app=kube-dns</code> label, and change the port to <code>5353</code>.</p> dns-matchname.yaml<pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: \"fqdn\"\nspec:\n  endpointSelector:\n    matchLabels:\n      org: empire\n      class: mediabot\n  egress:\n    - toFQDNs: # (1)!\n        - matchName: \"api.github.com\"\n    - toEndpoints:  # (2)!\n        - matchLabels:\n            \"k8s:io.kubernetes.pod.namespace\": kube-system\n            \"k8s:k8s-app\": kube-dns\n      toPorts:\n        - ports:\n            - port: \"53\"\n              protocol: ANY\n          rules:\n            dns:\n              - matchPattern: \"*\"\n</code></pre> <ol> <li>The first egress section uses <code>toFQDNs:</code> matchName specification to allow egress to <code>api.github.com</code>. The destination DNS should match exactly the name specified in the rule. The endpointSelector allows only pods with labels <code>class: mediabot</code>, <code>org:empire</code> to have the egress access.</li> <li>The second egress section (<code>toEndpoints</code>) allows <code>mediabot</code> pods to access <code>kube-dns</code> service. Note that <code>rules: dns</code> instructs Cilium to inspect and allow DNS lookups matching specified patterns. In this case, inspect and allow all DNS queries.</li> </ol> <p>Apply DNS policy using DNS strict match</p> <pre><code>#\u00a0Apply the DNS Polidy\nkubectl apply -f https://raw.githubusercontent.com/jsa4000/homelab-ops/main/docs/_source/layers/networking/cilium/dns-matchname.yaml\n</code></pre> <p>Testing the policy, we see that <code>mediabot</code> has access to <code>api.github.com</code> but doesn't have access to any other external service, e.g., <code>support.github.com</code>.</p> <pre><code>#\u00a0Verify the DNS match with the policy applied\nkubectl exec mediabot -- curl -I -s https://api.github.com | head -1\nHTTP/2 200\n\n#\u00a0Verify the DNS does not match with the policy applied\nkubectl exec mediabot -- curl -I -s --max-time 5 https://support.github.com | head -1\n\n# The connections it's blocked by cilium\n</code></pre>"},{"location":"layers/networking/cilium/#dns-policies-using-patterns","title":"DNS Policies Using Patterns","text":"<p>The above policy controlled DNS access based on exact match of the DNS domain name. Often, it is required to allow access to a subset of domains. Let's say, in the above example, mediabot pods need access to any GitHub sub-domain, e.g., the pattern <code>*.github.com</code>. We can achieve this easily by changing the <code>toFQDN</code> rule to use <code>matchPattern</code> instead of <code>matchName</code>.</p> dns-pattern.yaml<pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: \"fqdn\"\nspec:\n  endpointSelector:\n    matchLabels:\n      org: empire\n      class: mediabot\n  egress:\n    - toFQDNs:\n        - matchPattern: \"*.github.com\"\n    - toEndpoints:\n        - matchLabels:\n            \"k8s:io.kubernetes.pod.namespace\": kube-system\n            \"k8s:k8s-app\": kube-dns\n      toPorts:\n        - ports:\n            - port: \"53\"\n              protocol: ANY\n          rules:\n            dns:\n              - matchPattern: \"*\"\n</code></pre> <p>Apply DNS policy using a pattern instead</p> <pre><code>#\u00a0Apply the DNS Policy with wildcards\nkubectl apply -f https://raw.githubusercontent.com/jsa4000/homelab-ops/main/docs/_source/layers/networking/cilium/dns-pattern.yaml\n</code></pre> <p>Test that <code>mediabot</code> has access to multiple GitHub services for which the DNS matches the pattern <code>*.github.com</code>.</p> <p>Note</p> <p>It is important to note and test that this doesn't allow access to <code>github.com</code> because the <code>*.</code> in the pattern requires one subdomain to be present in the DNS name. You can simply add more <code>matchName</code> and <code>matchPattern</code> clauses to extend the access. (See DNS based policies to learn more about specifying DNS rules using patterns and names.)</p> <pre><code>#\u00a0Verify the DNS match with the policy applied\nkubectl exec mediabot -- curl -I -s https://api.github.com | head -1\nHTTP/2 200\n\n#\u00a0Verify the DNS match with the policy applied\nkubectl exec mediabot -- curl -I -s --max-time 5 https://support.github.com | head -1\nHTTP/2 302\n\n#\u00a0Verify the DNS does not match with the policy applied\nkubectl exec mediabot -- curl -I -s --max-time 5 https://github.com | head -1\n\n# The connections it's blocked by cilium\n</code></pre>"},{"location":"layers/networking/cilium/#combining-dns-port-and-l7-rules","title":"Combining DNS, Port and L7 Rules","text":"<p>The DNS-based policies can be combined with port (L4) and API (L7) rules to further restrict the access. In our example, we will restrict <code>mediabot</code> pods to access GitHub services only on ports <code>443</code>. The <code>toPorts</code> section in the policy below achieves the port-based restrictions along with the DNS-based policies.</p> dns-port.yaml<pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: \"fqdn\"\nspec:\n  endpointSelector:\n    matchLabels:\n      org: empire\n      class: mediabot\n  egress:\n    - toFQDNs:\n        - matchPattern: \"*.github.com\"\n        - matchName: \"github.com\"\n      toPorts:\n        - ports:\n            - port: \"443\"\n              protocol: TCP\n    - toEndpoints:\n        - matchLabels:\n            \"k8s:io.kubernetes.pod.namespace\": kube-system\n            \"k8s:k8s-app\": kube-dns\n      toPorts:\n        - ports:\n            - port: \"53\"\n              protocol: ANY\n          rules:\n            dns:\n              - matchPattern: \"*\"\n</code></pre> <p>Apply DNS policy using a pattern instead</p> <pre><code>#\u00a0Apply the DNS Policy with ports\nkubectl apply -f https://raw.githubusercontent.com/jsa4000/homelab-ops/main/docs/_source/layers/networking/cilium/dns-port.yaml\n</code></pre> <p>Test that <code>mediabot</code> has access to multiple GitHub services for which the DNS matches both <code>github.com</code> and pattern <code>*.github.com</code>. The access on port <code>443</code> will succeed but the access on port <code>80</code> will be denied.</p> <pre><code>#\u00a0Verify the DNS match with the policy applied on port 443\nkubectl exec mediabot -- curl -I -s https://api.github.com | head -1\nHTTP/2 200\n\n#\u00a0Verify the DNS match with the policy applied on port 443\nkubectl exec mediabot -- curl -I -s --max-time 5 https://support.github.com | head -1\nHTTP/2 302\n\n#\u00a0Verify the DNS match with the policy applied on port 443\nkubectl exec mediabot -- curl -I -s --max-time 5 https://github.com | head -1\n\n#\u00a0Verify the DNS match with the policy applied on port 80\nkubectl exec mediabot -- curl -I -s --max-time 5 http://support.github.com | head -1\n\n# The connections it's blocked by cilium\n</code></pre>"},{"location":"layers/networking/cilium/#inspecting-the-clusters-network-traffic-with-hubble-relay","title":"Inspecting the cluster's network traffic with Hubble Relay","text":"<p>Let's now inspect this traffic using the CLI. The command below filters all traffic on the application layer (L7, HTTP) to the <code>deathstar</code> pod.</p> <pre><code># Observe HTTP traffic from deathstar pod\nhubble observe --pod deathstar --protocol http\n\nApr  5 21:03:13.283: default/tiefighter:42746 (ID:25698) -&gt; default/deathstar-b4b8ccfb5-w84sn:80 (ID:6822) http-request FORWARDED (HTTP/1.1 POST http://deathstar.default.svc.cluster.local/v1/request-landing)\nApr  5 21:03:13.283: default/tiefighter:42746 (ID:25698) &lt;- default/deathstar-b4b8ccfb5-w84sn:80 (ID:6822) http-response FORWARDED (HTTP/1.1 200 1ms (POST http://deathstar.default.svc.cluster.local/v1/request-landing))\nApr  5 21:03:15.919: default/tiefighter:44396 (ID:25698) -&gt; default/deathstar-b4b8ccfb5-f75xw:80 (ID:6822) http-request DROPPED (HTTP/1.1 PUT http://deathstar.default.svc.cluster.local/v1/exhaust-port)\nApr  5 21:03:15.919: default/tiefighter:44396 (ID:25698) &lt;- default/deathstar-b4b8ccfb5-f75xw:80 (ID:6822) http-response FORWARDED (HTTP/1.1 403 0ms (PUT http://deathstar.default.svc.cluster.local/v1/exhaust-port))\n\n# Observe al DROPPED traffic from deathstar pod\nhubble observe --pod deathstar --verdict DROPPED\n\nApr  5 21:03:15.919: default/tiefighter:44396 (ID:25698) -&gt; default/deathstar-b4b8ccfb5-f75xw:80 (ID:6822) http-request DROPPED (HTTP/1.1 PUT http://deathstar.default.svc.cluster.local/v1/exhaust-port)\nApr  5 21:03:18.803: default/xwing:45706 (ID:63406) &lt;&gt; default/deathstar-b4b8ccfb5-f75xw:80 (ID:6822) policy-verdict:none INGRESS DENIED (TCP Flags: SYN)\nApr  5 21:03:18.803: default/xwing:45706 (ID:63406) &lt;&gt; default/deathstar-b4b8ccfb5-f75xw:80 (ID:6822) Policy denied DROPPED (TCP Flags: SYN)\nApr  5 21:03:19.820: default/xwing:45706 (ID:63406) &lt;&gt; default/deathstar-b4b8ccfb5-f75xw:80 (ID:6822) policy-verdict:none INGRESS DENIED (TCP Flags: SYN)\nApr  5 21:03:19.820: default/xwing:45706 (ID:63406) &lt;&gt; default/deathstar-b4b8ccfb5-f75xw:80 (ID:6822) Policy denied DROPPED (TCP Flags: SYN)\nApr  5 21:03:21.836: default/xwing:45706 (ID:63406) &lt;&gt; default/deathstar-b4b8ccfb5-f75xw:80 (ID:6822) policy-verdict:none INGRESS DENIED (TCP Flags: SYN)\nApr  5 21:03:21.836: default/xwing:45706 (ID:63406) &lt;&gt; default/deathstar-b4b8ccfb5-f75xw:80 (ID:6822) Policy denied DROPPED (TCP Flags: SYN)\nApr  5 21:03:25.965: default/xwing:45706 (ID:63406) &lt;&gt; default/deathstar-b4b8ccfb5-f75xw:80 (ID:6822) policy-verdict:none INGRESS DENIED (TCP Flags: SYN)\nApr  5 21:03:25.965: default/xwing:45706 (ID:63406) &lt;&gt; default/deathstar-b4b8ccfb5-f75xw:80 (ID:6822) Policy denied DROPPED (TCP Flags: SYN)\nApr  5 21:03:34.156: default/xwing:45706 (ID:63406) &lt;&gt; default/deathstar-b4b8ccfb5-f75xw:80 (ID:6822) policy-verdict:none INGRESS DENIED (TCP Flags: SYN)\nApr  5 21:03:34.156: default/xwing:45706 (ID:63406) &lt;&gt; default/deathstar-b4b8ccfb5-f75xw:80 (ID:6822) Policy denied DROPPED (TCP Flags: SYN)\nApr  5 21:03:50.285: default/xwing:45706 (ID:63406) &lt;&gt; default/deathstar-b4b8ccfb5-f75xw:80 (ID:6822) policy-verdict:none INGRESS DENIED (TCP Flags: SYN)\nApr  5 21:03:50.285: default/xwing:45706 (ID:63406) &lt;&gt; default/deathstar-b4b8ccfb5-f75xw:80 (ID:6822) Policy denied DROPPED (TCP Flags: SYN)\n</code></pre>"},{"location":"layers/networking/cilium/#labs","title":"Labs","text":"<ul> <li>IPAM and L2 Service Announcement Lab</li> </ul>"},{"location":"layers/networking/cilium/#references","title":"References","text":"<ul> <li>Kubernetes LoadBalance service using Cilium BGP control plane</li> <li>Migrating from MetaLB to Cilium</li> <li>Getting Started with the Star Wars Demo</li> </ul>"},{"location":"layers/networking/ddns/","title":"DDNS (Dynamic DNS)","text":"<p>As a solution to the frequently changing IP address of your residential connection, we use Dynamic DNS (DDNS). DDNS is a service that periodically (typically every few minutes) updates the domain registration information with the currently accessible IP address by detecting changes in the actual IP address (public IP address) provided by your ISP. The operation of DDNS is based on the method defined in the ACME (Automatic Certificate Management Environment) RFC8555 standard, many domain registrars (or DNS service providers) may use slightly different implementation methods, but essentially they follow the same RFC standard.</p> <p></p> <p>Note</p> <p>Internet Service Providers (ISPs) assign dynamic IP addresses to residential internet connections, which means that the assigned IP address to you can change at any time. Consequently, relying on accessing your services via the IP address from external sources does not guarantee a stable service. Additionally, to apply SSL HTTPS security (using SSL certificates), you need to utilize an internet domain that you own or have administrative control over. From this point forward, we assume that you have your own domain.</p> <p>First, check if your domain registrar or DNS service provider supports Dynamic DNS. Many domain registrars offer support for DDNS. GoDaddy, Cloudflare, etc.</p> <p>To ensure secure usage of your cluster, it is now considered standard practice to use encrypted HTTPS based on TLS certificates. If you currently do not have a domain to use, I'd recommended to have your own top-level domain for around $15 per year as one of your digital identity.</p>"},{"location":"layers/networking/ddns/#dns-management","title":"DNS Management","text":""},{"location":"layers/networking/ddns/#cloudflare","title":"Cloudflare","text":"<p>Cloudflare is a well-known company in the industry, primarily recognized for its CDN and other network-related services. However, as a domain registrar, it also offers convenient and useful features to end users (even in their free tier), such as basic DDoS protection, proxy services, and DDNS functionality.</p> <p>Cloudflare provides DNS capabilities such as renew registration, update settings, and edit WHOIS contacts for domains registered with Cloudflare, or transfer to another registrar. In order to use the API for DNS management is needed to create an API Token (or legacy API Keys).</p> <p>Following the step to use Cloudflare DDNS on the existing K8s cluster and set it up according to your domain.</p> <p></p>"},{"location":"layers/networking/ddns/#create-api-token","title":"Create API Token","text":"<p>Create API Token for DNS Management.</p> <p>API Tokens are better than traditional API Keys since they don't need to inform the ZoneId and add more security since tokens can be revoked.</p> <p>Go to Cloudflare -&gt; My Profile -&gt; API Tokens -&gt; <code>Create Token</code> in API Tokens.</p> <p></p>"},{"location":"layers/networking/ddns/#using-generated-ssl-by-cert-manager","title":"Using Generated SSL by Cert Manager","text":"<p>Enable Cloudflare to use vanilla DNS management from API.</p> <ol> <li>Set the DNS setup on the Cloudflare website to <code>off</code>. Go to Cloudflare -&gt; Select DNS domain (<code>mydomain.com</code>) -&gt; <code>SSL/TLS</code> -&gt; Select <code>Off (not secure</code> </li> <li>Disable Universal SSL. Go to Cloudflare -&gt; Select DNS domain (<code>mydomain.com</code>) -&gt; <code>SSL/TLS</code> -&gt; <code>Edge Certificates</code> -&gt; <code>Disable Universal SSL</code> </li> </ol>"},{"location":"layers/networking/ddns/#using-generated-ssl-by-cloudflare","title":"Using Generated SSL by CloudFlare","text":"<p>Allow <code>proxied</code> dns resolver so Cloudflare can manage automatically the connections allowing using universal certificates and preventing ddos and other  attacks.</p> <p>Cloudflare does not allow to create certificates with more than two-level subdomain or upload custom certificates using the free tier.</p> <ol> <li>Set the DNS setup on the Cloudflare website to <code>Full</code>. Go to Cloudflare -&gt; Select DNS domain (<code>mydomain.com</code>) -&gt; <code>SSL/TLS</code> -&gt; Select <code>Full (Encrypts end-to-end, using a self signed certificate on the server)</code> </li> <li>Select <code>Always Use HTTPS</code> and configure it to use a minimum TLS 1.2 or higher. Go to Cloudflare -&gt; Select DNS domain (<code>mydomain.com</code>) -&gt; <code>SSL/TLS</code> -&gt; <code>Edge Certificates</code> -&gt; Enable <code>Always Use HTTPS</code> and select <code>TLS 1.2</code> or higher     </li> <li>Enable Universal SSL. Go to Cloudflare -&gt; Select DNS domain (<code>mydomain.com</code>) -&gt; <code>SSL/TLS</code> -&gt; <code>Edge Certificates</code> -&gt; <code>Enable Universal SSL</code> </li> </ol>"},{"location":"layers/networking/ddns/#references","title":"References","text":"<ul> <li>Favonia Cloudflare DDNS</li> </ul>"},{"location":"layers/networking/metallb/","title":"Metallb","text":""},{"location":"layers/networking/metallb/#address-assignment","title":"Address Assignment","text":"<p>Metallb is not a load balancer but it has functionality for IP Address Management to the cluster, in order to be able to reach to nodes and services by using a single IP Address.</p>"},{"location":"layers/networking/metallb/#l2-advertisement","title":"L2 Advertisement","text":"<p>For this address assignment mode the client and the cluster must ber in the same local network.</p>"},{"location":"layers/networking/metallb/#bgp-advertisement","title":"BGP Advertisement","text":"<p>This mode is more advanced and provides a real load balancing capabilities. It requires interacting with a BGP enabled router.</p>"},{"location":"layers/networking/metallb/#references","title":"References","text":"<ul> <li>MetalLB, Kubernetes Bare Metal Load Balancing Youtube</li> </ul>"},{"location":"layers/networking/nat/","title":"NAT (Network address translation)","text":"<p>These are the devices needed.</p>  ONT Network Router 1GEHUAWEI WiFi AX3 Pro <p></p> <p>Top View</p> <p></p> <p>Top View</p>"},{"location":"layers/networking/nat/#firewall","title":"Firewall","text":"NAT ServicesPort forwarding <p>Top View</p> <p></p> <p>Top View</p>"},{"location":"layers/networking/traefik/","title":"Traefik","text":""},{"location":"layers/networking/zigbee/","title":"Zigbee","text":""},{"location":"layers/networking/zigbee/#commands","title":"Commands","text":"<p>Useful commands to verify the devices connected and kernel features.</p> <pre><code># Use dmesg print or control the kernel ring buffer\ndmesg\n\n[  440.815631] usb 3-1: USB disconnect, device number 2\n[  535.665640] usb 3-1: new full-speed USB device number 3 using ohci-platform\n[  535.898869] usb 3-1: New USB device found, idVendor=1a86, idProduct=55d4, bcdDevice= 4.42\n[  535.898895] usb 3-1: New USB device strings: Mfr=1, Product=2, SerialNumber=3\n[  535.898915] usb 3-1: Product: SONOFF Zigbee 3.0 USB Dongle Plus V2\n[  535.898931] usb 3-1: Manufacturer: ITEAD\n[  535.898948] usb 3-1: SerialNumber: 20240123152135\n[  535.901325] cdc_acm 3-1:1.0: ttyACM0: USB ACM device\n\ndmesg | grep tty\n\n[  535.901325] cdc_acm 3-1:1.0: ttyACM0: USB ACM device\n\n# List all USB devices connected\nlsusb -t -v\n\n# Take a look to uncommented lines.\n\n#    /:  Bus 06.Port 1: Dev 1, Class=root_hub, Driver=xhci-hcd/1p, 5000M\n#    ID 1d6b:0003 Linux Foundation 3.0 root hub\n#/:  Bus 05.Port 1: Dev 1, Class=root_hub, Driver=xhci-hcd/1p, 480M\n#    ID 1d6b:0002 Linux Foundation 2.0 root hub\n#/:  Bus 04.Port 1: Dev 1, Class=root_hub, Driver=ohci-platform/1p, 12M\n#    ID 1d6b:0001 Linux Foundation 1.1 root hub\n/:  Bus 03.Port 1: Dev 1, Class=root_hub, Driver=ohci-platform/1p, 12M\n    ID 1d6b:0001 Linux Foundation 1.1 root hub\n    |__ Port 1: Dev 2, If 0, Class=Communications, Driver=cdc_acm, 12M\n        ID 1a86:55d4 QinHeng Electronics\n    |__ Port 1: Dev 2, If 1, Class=CDC Data, Driver=cdc_acm, 12M\n        ID 1a86:55d4 QinHeng Electronics\n#/:  Bus 02.Port 1: Dev 1, Class=root_hub, Driver=ehci-platform/1p, 480M\n#    ID 1d6b:0002 Linux Foundation 2.0 root hub\n#/:  Bus 01.Port 1: Dev 1, Class=root_hub, Driver=ehci-platform/1p, 480M\n#    ID 1d6b:0002 Linux Foundation 2.0 root hub\n</code></pre> <p>In order to get the port configuration for <code>zigbee2mqtt</code> or <code>ZHA</code> (Zigbee for Home Assistant) execute following commands.</p> <pre><code># List all serials by identifier\nls -l /dev/serial/by-id\n\ntotal 0\nlrwxrwxrwx 1 root root 13 jun  3 18:11 usb-ITEAD_SONOFF_Zigbee_3.0_USB_Dongle_Plus_V2_20240123152135-if00 -&gt; ../../ttyACM0\n</code></pre> <p>In this example the correct port would be <code>/dev/ttyACM0</code></p>"},{"location":"layers/security/cert-manager/","title":"Cert Manager","text":"deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3 # (1)!\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:1.14.2\n          ports:\n            - containerPort: 80\n</code></pre> <ol> <li>Number of replicas</li> </ol> containers<pre><code>      containers:\n        - name: nginx\n          image: nginx:1.14.2\n          ports:\n            - containerPort: 80\n</code></pre>"},{"location":"layers/security/external-secrets/","title":"External Secrets","text":""},{"location":"sbc/armbian/","title":"Installation","text":""},{"location":"sbc/dietpi/","title":"DietPi OS","text":""},{"location":"sbc/dietpi/#prepare-sd-card","title":"Prepare SD Card","text":"<ol> <li>Go to official Orange Pi Web Site.</li> <li>Go to section Service &amp; Download and select <code>Download</code>.</li> <li>Search for the specific Orange Pi model (i.e <code>Orange Pi 5</code>).</li> <li>Download the Ubuntu Image (i.e <code>Orangepi5_1.1.8_ubuntu_jammy_server_linux5.10.160.7z</code>). This tutorial is mean for that specific Linux distribution.</li> <li>Flash the image into a SD Card using balenaEtcher.</li> </ol>"},{"location":"sbc/dietpi/#initialize","title":"Initialize","text":"<p>Put the SD Card into the board and turn on the device.</p> <p>NOTE: Go to your Router Gateway in order to check the IP Address of the device (i.e <code>192.168.3.49</code>). This is useful since headless installations an external monitor is not needed.</p> <pre><code># [Optional] Remove previous known host for previous connections\nrm ~/.ssh/known_hosts\n\n# SSH into the machine (orangepi/orangepi)\nssh orangepi@192.168.3.49\n\n# Update the dependencies\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# Install compression utils\nsudo apt install p7zip-full gettext-base -y\n\n# Install yq\nwget https://github.com/mikefarah/yq/releases/download/v4.40.5/yq_linux_arm64.tar.gz -O - | tar xz &amp;&amp; mv yq_linux_arm64 /usr/bin/yq\n\n# [Optional] Use the orange pi utility do some basic configuration: update firmware, system configuration, software, etc..\nsudo orangepi-config\n</code></pre>"},{"location":"sbc/dietpi/#spi-flash","title":"SPI Flash","text":"<p>In order to boot up from the SDD drive is necessary to flash the SPI with the proper <code>loader</code>.</p> <pre><code># You can use orangepi-config and select the desired mode, however you can do manually using the process bellow\nsudo nand-sata-install\n</code></pre>"},{"location":"sbc/dietpi/#nvme-spi-flash","title":"NVME SPI Flash","text":"<p>By default Orange PI can detect NVME drives, so once the device is plugin into the board it should be detected.</p> <pre><code># List Block devices (you should see a device called 'nvme0n1')\nsudo lsblk\n\n# [Optional] Erase the content of the SPI Flash\nsudo dd if=/dev/zero of=/dev/mtdblock0 bs=1M count=1 &amp;&amp; sudo sync\n\n# Use following command to Flash SPI with the loader so it boot from the NVME SSD (sudo find / -name rkspi_loader.img)\nsudo dd if=/usr/lib/linux-u-boot-legacy-orangepi5_1.1.8_arm64/rkspi_loader.img of=/dev/mtdblock0 conv=notrunc &amp;&amp; sudo sync\n\n# Reboot the system, however it's not necessary\nsudo reboot\n</code></pre>"},{"location":"sbc/dietpi/#sata-spi-flash","title":"SATA SPI Flash","text":"<p>By default Orange PI DO NOT detect SATA drives, so once the device is plugin into the board it's not recognized.</p> <pre><code># List Block devices (you should not see a device called 'sda')\nsudo lsblk\n\n# [Optional] Erase the content of the SPI Flash\nsudo dd if=/dev/zero of=/dev/mtdblock0 bs=1M count=1 &amp;&amp; sudo sync\n\n# Use following command to Flash SPI with the loader so it boot from the SATA SSD (sudo find / -name rkspi_loader_sata.img)\ndd if=/usr/share/orangepi5/rkspi_loader_sata.img of=/dev/mtdblock0 &amp;&amp; sudo sync\n\n# Enable SATA devices:\n# 1. Activate the SATA overlay in 'orangepiEnv.txt' (sudo find / -name orangepiEnv.txt):\n# 2. List all the files at /boot/dtb/rockchip/overlay (sudo ls /boot/dtb/rockchip/overlay | grep sata)\necho \"overlays=ssd-sata0 ssd-sata2\" | sudo tee -a /boot/orangepiEnv.txt\n\n# Reboot the system\nsudo reboot\n</code></pre>"},{"location":"sbc/dietpi/#ssd-installation","title":"SSD Installation","text":"<p>Download the image that is going to be installed into the SSD drive and copy it into the SD.</p> <ol> <li>Go to official Diet Pi Web Site.</li> <li>Go to section Download.</li> <li>Search for the specific Orange Pi model (i.e <code>Orange Pi 5</code>).</li> <li>Download the image (i.e <code>DietPi_OrangePi5-ARMv8-Bookworm.img.xz</code>).</li> </ol> <p>Copy the image into the Orange Pi using SSH or SFTP.</p> <pre><code># From remote computer copy the image to the SD via SCP.\nscp \"/Users/jsantosa/Downloads/tools/orangepi/DietPi_OrangePi5-ARMv8-Bookworm.img.xz\" orangepi@192.168.3.49:~/\n\n# From the orange pi Unzip the content if necessary\n7za x DietPi_OrangePi5-ARMv8-Bookworm.img.xz\n</code></pre>"},{"location":"sbc/dietpi/#generate-ssh-keys","title":"Generate SSH keys","text":"<pre><code># [Optional] If ssh folder does not exist\nmkdir -p ~/.ssh\nchmod 700 ~/.ssh\n\n# Do not fill anything in next command just enter\nssh-keygen -t rsa -b 4096 -C \"jsa4000@gmail.com\" -f ~/.ssh/server_key\n\n# [Optional] Copy keys to each node or use AUTO_SETUP_SSH_PUBKEY in 'dietpi.txt' config instead\nssh-copy-id -i ~/.ssh/server_key.pub root@192.168.3.100\n\n# Replace the content of AUTO_SETUP_SSH_PUBKEY for this key and set SOFTWARE_DISABLE_SSH_PASSWORD_LOGINS to 0\ncat ~/.ssh/server_key.pub\n</code></pre>"},{"location":"sbc/dietpi/#nvme-ssd-installation","title":"NVME SSD Installation","text":"<pre><code># Delete SSD content by setting zeros to the first data blocks\nsudo dd bs=1M if=/dev/zero of=/dev/nvme0n1 count=2000 status=progress &amp;&amp; sudo sync\n\n# Clone the SD into SSD (very slow)\n#sudo cat /dev/mmcblk0 &gt; /dev/nvme0n1\n\n# Copy the image content to the SSD\nsudo dd bs=1M if=~/DietPi_OrangePi5-ARMv8-Bookworm.img of=/dev/nvme0n1 status=progress &amp;&amp; sudo sync\n\n# Check the data blocks. Notice the new partitions are not extended yet.\nsudo lsblk\n\n# Fix the SD and SSD since it uses the same identifier\nsudo fix_mmc_ssd.sh # or sudo tune2fs -U random /dev/mmcblk1p2\n\n# Copy the auto configuration\nscp -rp \"./docs/dietpi\" orangepi@192.168.3.49:~/\n\nsudo mount /dev/nvme0n1p2 /mnt/\ncat ~/dietpi/templates/dietpi.txt | sudo tee -a /mnt/dietpi.txt\nsudo umount /mnt/ &amp;&amp; sudo sync\n\n# Power off and remove the SD card\nsudo poweroff\n</code></pre>"},{"location":"sbc/dietpi/#sata-ssd-installation","title":"SATA SSD Installation","text":"<p>Check DietPi documentation to check the process to add SATA overlays to the OS (<code>dietpiEnv.txt</code>)</p>"},{"location":"sbc/dietpi/#test","title":"Test","text":"<pre><code># [Optional] Remove previous known host for previous connections\nrm ~/.ssh/known_hosts\n\n# SSH into the machine (root/dietpi)\nssh root@192.168.3.100\n\n# SSH into the machine (private key)\nssh -i ~/.ssh/server_key root@192.168.3.100\nssh -i ~/.ssh/server_key dietpi@192.168.3.100\n</code></pre> <p>Using the SSH config file so it's not necessary to enter the private key anymore.</p> <p><code>~/.ssh/config</code></p> <pre><code>Host sbc-server-1\n    HostName 192.168.3.100\n    User     dietpi\n    IdentityFile ~/.ssh/server_key\n\nHost sbc-server-2\n    HostName 192.168.3.101\n    User     dietpi\n    IdentityFile ~/.ssh/server_key\n\nHost sbc-server-3\n    HostName 192.168.3.102\n    User     dietpi\n    IdentityFile ~/.ssh/server_key\n</code></pre> <pre><code># [Optional] Remove previous known host for previous connections\nrm ~/.ssh/known_hosts\n\n# SSH using the user@host specified in the SSH config\nssh dietpi@sbc-server-1\nssh dietpi@sbc-server-2\nssh dietpi@sbc-server-3\n\n# Power off\nssh dietpi@sbc-server-1 'sudo poweroff'\nssh dietpi@sbc-server-2 'sudo poweroff'\nssh dietpi@sbc-server-3 'sudo poweroff'\n\n# Reboot\nssh dietpi@sbc-server-1 'sudo reboot'\nssh dietpi@sbc-server-2 'sudo reboot'\nssh dietpi@sbc-server-3 'sudo reboot'\n</code></pre>"},{"location":"sbc/dietpi/#first-boot","title":"First Boot","text":"<p>DietPi offers the option for an automatic first boot installation. See section How to do an automatic base installation at first boot for details.</p> <p>Following are the most important changes to take a loot in <code>dietpi.txt</code>.</p> <pre><code>AUTO_SETUP_LOCALE=es_ES.UTF-8\nAUTO_SETUP_KEYBOARD_LAYOUT=es\nAUTO_SETUP_TIMEZONE=Europe/Madrid\nAUTO_SETUP_NET_WIFI_COUNTRY_CODE=ES\n\nAUTO_SETUP_NET_USESTATIC=1\nAUTO_SETUP_NET_STATIC_IP=192.168.3.101\nAUTO_SETUP_NET_STATIC_MASK=255.255.255.0\nAUTO_SETUP_NET_STATIC_GATEWAY=192.168.3.1\nAUTO_SETUP_NET_STATIC_DNS=192.168.3.1\n\nAUTO_SETUP_NET_HOSTNAME=Server01\n\nAUTO_SETUP_HEADLESS=1\nAUTO_SETUP_SSH_SERVER_INDEX=-2\n#AUTO_SETUP_SSH_PUBKEY=ssh-ed25519 AAAAAAAA111111111111BBBBBBBBBBBB222222222222cccccccccccc333333333333 mySSHkey\n\nAUTO_SETUP_RAMLOG_MAXSIZE=200\nAUTO_SETUP_WEB_SERVER_INDEX=0\nAUTO_SETUP_DESKTOP_INDEX=0\nAUTO_SETUP_BROWSER_INDEX=0\n\nAUTO_SETUP_AUTOSTART_TARGET_INDEX=7\nAUTO_SETUP_AUTOSTART_LOGIN_USER=root\nAUTO_SETUP_GLOBAL_PASSWORD=dietpi\nAUTO_SETUP_AUTOMATED=1\nSURVEY_OPTED_IN=0\n\n# OpenSSH Client\nAUTO_SETUP_INSTALL_SOFTWARE_ID=0\n# OpenSSH Server\nAUTO_SETUP_INSTALL_SOFTWARE_ID=105\n# Python 3 pip\nAUTO_SETUP_INSTALL_SOFTWARE_ID=130\n</code></pre>"},{"location":"sbc/dietpi/#diet-software","title":"Diet Software","text":"<p>You can install as many software as you want during the setup. Since DietPi pretend to be minimalistic, it does not provide a large catalogue of software by default. You can take a look to the software list.</p> <pre><code># OpenSSH Client\nAUTO_SETUP_INSTALL_SOFTWARE_ID=0\n# OpenSSH Server\nAUTO_SETUP_INSTALL_SOFTWARE_ID=105\n# Git\nAUTO_SETUP_INSTALL_SOFTWARE_ID=17\n# Python 3 pip\nAUTO_SETUP_INSTALL_SOFTWARE_ID=130\n</code></pre> <p>Check packages currently installed in debian or ubuntu (<code>apt</code>)</p> <pre><code># Check packages installed\ndpkg -l\n</code></pre>"},{"location":"sbc/dietpi/#random-mac-address","title":"Random MAC Address","text":"<p>Orange Pi 5 - MAC address changes on reboot rk_vendor_read eth mac address failed</p> <pre><code># Run following command\nsudo journalctl | grep rk_gmac-dwmac\n\n01 16:26:49 DietPi kernel: rk_gmac-dwmac fe1c0000.ethernet: Enable RX Mitigation via HW Watchdog Timer\nMay 01 16:26:49 DietPi kernel: rk_gmac-dwmac fe1c0000.ethernet: rk_get_eth_addr: rk_vendor_read eth mac address failed (-1)\nMay 01 16:26:49 DietPi kernel: rk_gmac-dwmac fe1c0000.ethernet: rk_get_eth_addr: generate random eth mac address: b2:23:c3:61:ba:23\nMay 01 16:26:49 DietPi kernel: rk_gmac-dwmac fe1c0000.ethernet: rk_get_eth_addr: rk_vendor_write eth mac address failed (-1)\nMay 01 16:26:49 DietPi kernel: rk_gmac-dwmac fe1c0000.ethernet: rk_get_eth_addr: id: 1 rk_vendor_read eth mac address failed (-1)\nMay 01 16:26:49 DietPi kernel: rk_gmac-dwmac fe1c0000.ethernet: rk_get_eth_addr: mac address: b2:23:c3:61:ba:23\nMay 01 16:26:49 DietPi kernel: rk_gmac-dwmac fe1c0000.ethernet: device MAC address b2:23:c3:61:ba:23\n</code></pre> <p>Creating <code>/etc/network/interfaces.d/eth0</code> with content</p> <pre><code># Create file with the fixed MAC Address\nsudo tee -a /etc/network/interfaces.d/eth0 &gt; /dev/null &lt;&lt;EOT\niface eth0 inet dhcp\nhwaddress ether 8a:33:ce:b3:b0:b9\nEOT\n\n# Restart Networking\nsystemctl restart networking\n\n# check the MAC Address has been assigned\nip address | grep 8a:33:ce:b3:b0:b9\n</code></pre> <p>Using <code>dietpi-config</code> is necessary to add it into the existing interface (<code>eth0</code>) already created at <code>/etc/network/interfaces</code></p> <pre><code># Add new line with the MAC Address after 'iface eth0 inet static'\nIFACE_FILE=/etc/network/interfaces\nMAC_ADDRESS=8a:33:ce:b3:b0:b9\nHWADDRESS=\"hwaddress ether $MAC_ADDRESS\"\nIFACE_STR=\"iface eth0 inet static\"\nsed -i \"s|$IFACE_STR|$IFACE_STR\\n$HWADDRESS|g\" $IFACE_FILE # For macos use 'sed -i \"\" \"s|xx|yy|g file\n\n# Restart Networking\nsystemctl restart networking\n</code></pre>"},{"location":"sbc/orangepi/","title":"Orange Pi OS","text":""},{"location":"sbc/orangepi/#prepare-sd-card","title":"Prepare SD Card","text":"<ol> <li>Go to official Orange Pi Web Site.</li> <li>Go to section Service &amp; Download and select <code>Download</code>.</li> <li>Search for the specific Orange Pi model (i.e <code>Orange Pi 5</code>).</li> <li>Download the Ubuntu Image (i.e <code>Orangepi5_1.1.8_ubuntu_jammy_server_linux5.10.160.7z</code>). This tutorial is mean for that specific Linux distribution.</li> <li>Flash the image into a SD Card using balenaEtcher.</li> </ol>"},{"location":"sbc/orangepi/#initialize","title":"Initialize","text":"<p>Put the SD Card into the board and turn on the device.</p> <p>NOTE: Go to your Router Gateway in order to check the IP Address of the device (i.e <code>192.168.3.49</code>). This is useful since headless installations an external monitor is not needed.</p> <pre><code># [Optional] Remove previous known host for previous connections\nrm ~/.ssh/known_hosts\n\n# SSH into the machine (orangepi/orangepi)\nssh orangepi@192.168.3.49\n\n# Update the dependencies\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# Install compression utils\nsudo apt install p7zip-full -y\n\n# [Optional] Use the orange pi utility do some basic configuration: update firmware, system configuration, software, etc..\nsudo orangepi-config\n</code></pre>"},{"location":"sbc/orangepi/#spi-flash","title":"SPI Flash","text":"<p>In order to boot up from the SDD drive is necessary to flash the SPI with the proper <code>loader</code>.</p> <pre><code># You can use orangepi-config and select the desired mode, however you can do manually using the process bellow\nsudo nand-sata-install\n</code></pre>"},{"location":"sbc/orangepi/#nvme-spi-flash","title":"NVME SPI Flash","text":"<p>By default Orange PI can detect NVME drives, so once the device is plugin into the board it should be detected.</p> <pre><code># List Block devices (you should see a device called 'nvme0n1')\nsudo lsblk\n\n# [Optional] Erase the content of the SPI Flash\nsudo dd if=/dev/zero of=/dev/mtdblock0 bs=1M count=1 &amp;&amp; sudo sync\n\n# Use following command to Flash SPI with the loader so it boot from the NVME SSD (sudo find / -name rkspi_loader.img)\nsudo dd if=/usr/lib/linux-u-boot-legacy-orangepi5_1.1.8_arm64/rkspi_loader.img of=/dev/mtdblock0 conv=notrunc &amp;&amp; sudo sync\n\n# Reboot the system, however it's not necessary\nsudo reboot\n</code></pre>"},{"location":"sbc/orangepi/#sata-spi-flash","title":"SATA SPI Flash","text":"<p>By default Orange PI DO NOT detect SATA drives, so once the device is plugin into the board it's not recognized.</p> <pre><code># List Block devices (you should not see a device called 'sda')\nsudo lsblk\n\n# [Optional] Erase the content of the SPI Flash\nsudo dd if=/dev/zero of=/dev/mtdblock0 bs=1M count=1 &amp;&amp; sudo sync\n\n# Use following command to Flash SPI with the loader so it boot from the SATA SSD (sudo find / -name rkspi_loader_sata.img)\ndd if=/usr/share/orangepi5/rkspi_loader_sata.img of=/dev/mtdblock0 &amp;&amp; sudo sync\n\n# Enable SATA devices:\n# 1. Activate the SATA overlay in 'orangepiEnv.txt' (sudo find / -name orangepiEnv.txt):\n# 2. List all the files at /boot/dtb/rockchip/overlay (sudo ls /boot/dtb/rockchip/overlay | grep sata)\necho \"overlays=ssd-sata0 ssd-sata2\" | sudo tee -a /boot/orangepiEnv.txt\n\n# Reboot the system\nsudo reboot\n</code></pre>"},{"location":"sbc/orangepi/#ssd-installation","title":"SSD Installation","text":"<p>Download the image that is going to be installed into the SSD drive and copy it into the SD.</p> <pre><code># From remote computer copy the image to the SD via SCP.\nscp \"/Users/jsantosa/Downloads/tools/orangepi/Orangepi5_1.1.8_ubuntu_jammy_server_linux5.10.160.7z\" orangepi@192.168.3.49:~/\n\n# From the orange pi Unzip the content if necessary\n7za x Orangepi5_1.1.8_ubuntu_jammy_server_linux5.10.160.7z\n</code></pre>"},{"location":"sbc/orangepi/#nvme-ssd-installation","title":"NVME SSD Installation","text":"<pre><code># Delete SSD content by setting zeros to the first data blocks\nsudo dd bs=1M if=/dev/zero of=/dev/nvme0n1 count=2000 status=progress &amp;&amp; sudo sync\n\n# Clone the SD into SSD (very slow)\n#sudo cat /dev/mmcblk0 &gt; /dev/nvme0n1\n\n# Copy the image content to the SSD\nsudo dd bs=1M if=Orangepi5_1.1.8_ubuntu_jammy_server_linux5.10.160.img of=/dev/nvme0n1 status=progress &amp;&amp; sudo sync\n\n# Fix the SD and SSD since it uses the same identifier\nsudo fix_mmc_ssd.sh # or sudo tune2fs -U random /dev/mmcblk1p2\n\n# Power off and remove the SD card\nsudo poweroff\n</code></pre>"},{"location":"sbc/orangepi/#sata-ssd-installation","title":"SATA SSD Installation","text":"<pre><code># Delete SSD content by setting zeros to the first data blocks\nsudo dd bs=1M if=/dev/zero of=/dev/sda count=2000 status=progress &amp;&amp; sudo sync\n\n# Clone the SD into SSD (very slow)\n#sudo cat /dev/mmcblk0 &gt; /dev/nvme0n1\n\n# Copy the image content to the SSD\nsudo dd bs=1M if=Orangepi5_1.1.8_ubuntu_jammy_server_linux5.10.160.img of=/dev/sda status=progress &amp;&amp; sudo sync\n\n# Fix the SD and SSD since it uses the same identifier\nsudo fix_mmc_ssd.sh # or sudo tune2fs -U random /dev/mmcblk1p2\n\n# Add the STAT overlay to the SSD boot in order to be recognized\nsudo mount /dev/sda1 /mnt/\necho \"overlays=ssd-sata0 ssd-sata2\" | sudo tee -a /mnt/orangepiEnv.txt\nsudo umount /mnt/ &amp;&amp; sudo sync\n\n# Power off and remove the SD card\nsudo poweroff\n</code></pre>"},{"location":"tools/mkdocs/","title":"MkDocs","text":"Tab 1Tab 2 <p>Markdown content.</p> <p>Multiple paragraphs.</p> <p>More Markdown content.</p> <ul> <li>list item a</li> <li>list item b</li> </ul> Tab ATab B <p>Different tab set.</p> <pre><code>More content.\n</code></pre>"},{"location":"tools/ansible/","title":"Ansible","text":"<p>Ansible is a popular open-source automation tool that can help you automate your IT infrastructure. Is is an agentless automation that automates deployment, configuration management (maintain infrastructure consistency) and orchestration (execution of multiple applications in order). Ansible gains it's popularity due to it's simplicity for being agentless, efficient, requires no additional software installed on target machine, use the simple YAML and complete with reporting</p> <p>A common best practice is using ephemeral, idempotent and immutable infrastructure as possible, this means prevent infrastructure configuration drifts (idempotent ) and use of resources wherein containerized components are replaced rather than changed (immutable).</p> <p>Take following consideration using Change Management tools:</p> <ul> <li>Do not update or make change to the OS since it will drift from the original state (idempotent). If so use Ansible, do not make any change manually.</li> <li>Use immutable images, so every time an update is needed because maintenance reasons: vulnerabilities, fixes, etc.. the server must be replaced entirely by the new version of the image.</li> </ul>"},{"location":"tools/ansible/#installation","title":"Installation","text":"<p>Since it is agentless it does not require to install anything in the servers. The only requirements are:</p> <ul> <li>Python3 installed on remote machines</li> <li>Added SSH public keys</li> </ul> <p>For the server side, in order to install using MacOs the simpler way is by using homebrew.</p> <pre><code>#\u00a0Install Ansible using brew\nbrew install ansible\n\n# Check the installation\nansible --version\n</code></pre>"},{"location":"tools/ansible/#structure","title":"Structure","text":""},{"location":"tools/ansible/#config","title":"Config","text":"<p>This config file is used for ansible to simpligy the execution of playbooks using default inventory, roles folder, etc..</p> <p><code>ansible.cfg</code></p> <pre><code>[defaults]\nnocows = True\nroles_path = ./roles\ninventory  = ./inventory.yaml\n\nremote_tmp = $HOME/.ansible/tmp\nlocal_tmp  = $HOME/.ansible/tmp\npipelining = True\nbecome = True\nhost_key_checking = False\ndeprecation_warnings = False\ncallback_whitelist = profile_tasks\n</code></pre>"},{"location":"tools/ansible/#inventory","title":"Inventory","text":"<p>You can define the inventory using <code>ini</code> or <code>yaml</code> files in Ansible. In this file you must define the servers that Ansible are going to manage organized into groups. Also you can specify variables to be used later on during the tasks execution.</p> <p><code>inventory.yaml</code></p> <pre><code>---\nk3s_cluster:\n  children:\n    server:\n      hosts:\n        sbc-server-1:\n    agent:\n      hosts:\n        sbc-server-2:\n        sbc-server-3:\n  # This section is the same as using `group_vars/k3s_cluster/vars.yaml` file\n  vars:\n    custom_command: lsblk\n</code></pre> <pre><code># Check all servers in the inventory ('-i' flag is not necessary because it will use the default in ansible.cfg)\nansible all -m ping\nansible all -m ping -i inventory.yaml\n\n# Check the k3s_cluster group and children\nansible k3s_cluster -m ping\n\n# Check the server group\nansible server -m ping\n\n# Check the agent group\nansible agent -m ping\n\n# List all hosts\nansible k3s_cluster --list-hosts\n</code></pre>"},{"location":"tools/ansible/#tasks","title":"Tasks","text":"<p>Standalone tasks are stored in <code>tasks</code> folder, however it is not a good practice.</p> <p>Create a file <code>./tasks/test.yaml</code></p> <pre><code>---\n- name: Get CPU Info\n  register: cpuinfo\n  command: \"cat /proc/cpuinfo\"\n\n- name: Execute the uname command\n  register: unameout\n  command: \"uname -a\"\n\n- name: Execute custom command\n  register: commandout\n  command: \"{{ custom_command }}\"\n\n- debug:\n    var: unameout.stdout_lines, cpuinfo.stdout_lines, commandout.stdout_lines\n</code></pre> <p>Create a playbook file <code>./playbook/test.yaml</code></p> <pre><code>---\n\n- name: Return Server Info\n  hosts: all\n  gather_facts: true\n  become: true # Ansible sudo\n  tasks:\n    import_tasks: tasks/test.yaml\n</code></pre>"},{"location":"tools/ansible/#playbooks","title":"Playbooks","text":"<p>Ansible Playbooks offer a repeatable, reusable, simple configuration management and multi-machine deployment system, one that is well suited to deploying complex applications. If you need to execute a task with Ansible more than once, write a playbook and put it under source control. Then you can use the playbook to push out new configuration or confirm the configuration of remote systems. The playbooks in the ansible-examples repository illustrate many useful techniques. You may want to look at these in another tab as you read the documentation.</p> <p>Playbooks can:</p> <ul> <li>Declare configurations</li> <li>orchestrate steps of any manual ordered process, on multiple sets of machines, in a defined order</li> <li>launch tasks synchronously or asynchronously</li> </ul> <p>Playbooks are stored in the <code>playbook</code> folder, so create a file <code>./playbook/test.yaml</code></p> <pre><code>---\n\n- name: Return Server Info\n  hosts: all\n  gather_facts: true\n  become: true # Ansible sudo\n  tasks:\n    - name: Get CPU Info\n      register: cpuinfo\n      command: \"cat /proc/cpuinfo\"\n    - name: Execute the UNAME command\n      register: unameout\n      command: \"uname -a\"\n    - debug:\n        var: unameout.stdout_lines, cpuinfo.stdout_lines\n</code></pre> <p>In order to run playbooks use following commands.</p> <pre><code># Running playbooks\nansible-playbook playbook/test.yaml\nansible-playbook playbook/test.yaml -i inventory.yaml\n\n# Ansible's check mode allows you to execute a playbook without applying any alterations to your systems. You can use check mode to test playbooks before implementing them in a production environment.\nansible-playbook --check playbook/test.yaml\n</code></pre>"},{"location":"tools/ansible/#roles","title":"Roles","text":"<p>Playbooks are stored in the <code>roles</code> folder. Within this folder, a new folder is created for each role.</p> <p>Create a task file at <code>roles/info/tasks/main.yaml</code></p> <pre><code>---\n- name: Get CPU Info\n  register: cpuinfo\n  command: \"cat /proc/cpuinfo\"\n\n- name: Execute the uname command\n  register: unameout\n  command: \"uname -a\"\n\n- name: Execute custom command\n  register: commandout\n  command: \"{{ custom_command }}\"\n\n- debug:\n    var: unameout.stdout_lines, cpuinfo.stdout_lines, commandout.stdout_lines\n</code></pre> <p>Create a playbook file <code>./playbook/test.yaml</code> that uses that role.</p> <pre><code>---\n\n- name: Cluster Info\n  hosts: k3s_cluster\n  gather_facts: true\n  become: true\n  roles:\n    - role: info\n</code></pre>"},{"location":"tools/byor/","title":"Build my Radar","text":"<p>Note: You could previously build your own radar on <code>radar.thoughtworks.com</code> using custom quadrant and ring names. Going forward, on the online version of BYOR, you will need to use the 4 quadrant and ring names from the TW Tech Radar, as mentioned below:</p> <ul> <li>Quadrant names: Techniques, Platforms, Tools, Languages &amp; Frameworks</li> <li>Ring names: Adopt, Trial, Assess, Hold</li> </ul> <p>If you set up a local version of BYOR, you can configure custom quadrant and ring names. You can find more details in the readme file.</p> <pre><code># Run docker container (linux/amd64)\ndocker run \\\n  --rm \\\n  --platform linux/amd64 \\\n  -p 8080:80 \\\n  -e SERVER_NAMES=\"localhost 127.0.0.1\" \\\n  -v $PWD/files/:/opt/build-your-own-radar/files \\\n  wwwthoughtworks/build-your-own-radar:v1.1.5\n\n# Test\nhttp://localhost:8080\n\n# Use following urls\nhttp://localhost:8080/files/radar.csv\nhttp://localhost:8080/files/vol29.csv\nhttp://localhost:8080/files/vol30.csv\n\u00a0\n# Local files can be modified locally, however there is a delay until the server loads the new file.\n````\n\nFile Format\n\n```csv\nText,Adopt,Techniques,TRUE,\"\"\nText,Trial,Techniques,TRUE,\"\"\nText,Assess,Techniques,TRUE,\"\"\nText,Hold,Techniques,TRUE,\"\"\n\nText,Adopt,Platforms,TRUE,\"\"\nText,Trial,Platforms,TRUE,\"\"\nText,Assess,Platforms,TRUE,\"\"\nText,Hold,Platforms,TRUE,\"\"\n\nText,Adopt,Tools,TRUE,\"\"\nText,Trial,Tools,TRUE,\"\"\nText,Assess,Tools,TRUE,\"\"\nText,Hold,Tools,TRUE,\"\"\n\nText,Adopt,Languages &amp; Frameworks,TRUE,\"\"\nText,Trial,Languages &amp; Frameworks,TRUE,\"\"\nText,Assess,Languages &amp; Frameworks,TRUE,\"\"\nText,Hold,Languages &amp; Frameworks,TRUE,\"\"\n</code></pre>"},{"location":"tools/certificates/","title":"Certificates","text":""},{"location":"tools/certificates/#concepts","title":"Concepts","text":""},{"location":"tools/certificates/#ssh","title":"SSH","text":""},{"location":"tools/certificates/#tls","title":"TLS","text":""},{"location":"tools/certificates/#https","title":"HTTPS","text":""},{"location":"tools/certificates/#certificate-authority-ca","title":"Certificate Authority (CA)","text":"<p>Issuer</p>"},{"location":"tools/certificates/#key-types","title":"Key types","text":"<p>These are some of the certificates and key files extensions most commonly used:</p> <ul> <li><code>.key</code> is the <code>private key</code>. This is accessible the key owner and no one else.</li> <li><code>.csr</code> is the <code>certificate request</code>. This is a request for a certificate authority to sign the key. (The key itself is not included.)</li> <li><code>.crt</code> is the certificate produced by the <code>certificate authority</code> that verifies the authenticity of the key. (The key itself is not included.) This is given to other parties, e.g. HTTPS client.</li> <li><code>.pem</code> is a text-based container using base-64 encoding. It could be any of the above files.</li> <li><code>.p12</code> is a <code>PKCS12</code> file, which is a container format usually used to combine the private key and certificate.</li> </ul>"},{"location":"tools/certificates/#certificate-signing-request-csr","title":"Certificate Signing Request (CSR)","text":"<p>A certificate signing request (CSR or certification request) is a message sent from an applicant to a certificate authority (CA) of the public key infrastructure (PKI) in order to apply for a digital identity certificate. The CSR usually contains the public key for which the certificate should be issued, identifying information (such as a domain name) and a proof of authenticity including integrity protection (e.g., a digital signature). The most common format for CSRs is the <code>PKCS #10</code> specification; others include the more capable Certificate Request Message Format (CRMF) and the SPKAC (Signed Public Key and Challenge) format generated by some web browsers.</p> <p>The CRS is used to generate <code>X.509</code> certificates that can be used to configure any webserver (Apache, NGNIX, Wordpress), Database (MySQL, Postgresql DB), IoT gateway/devices or even your own web app server(HTTPS) and web app client. The client SSL/TLS X.509 certificate can be used for any application that requires mutual TLS authentication for Zero Trust Security.</p> <p>Before creating a CSR for an X.509 certificate, the applicant first generates a key pair, keeping the private key of that pair secret. The CSR contains information identifying the applicant (such as a distinguished name), the public key chosen by the applicant, and possibly further information. When using the <code>PKCS #10</code> format, the request must be self-signed using the applicant's private key, which provides proof-of-possession of the private key but limits the use of this format to keys that can be used for signing. The CSR should be accompanied by a proof of origin (i.e., proof of identity of the applicant) that is required by the certificate authority, and the certificate authority may contact the applicant for further information.</p> <p>Typical information required in a CSR are</p> <ul> <li>CN (Common Name):This is fully qualified domain name that you wish to secure (*.wikipedia.org)</li> <li>O (Organization Name): Usually the legal name of a company or entity and should include any suffixes such as Ltd., Inc., or Corp. (Wikimedia Foundation, Inc.)</li> <li>OU (Organizational Unit): Internal organization department/division name (IT)</li> <li>L (Locality): Town, city, village, etc. name (San Francisco)</li> <li>ST (State): Province, region, county or state. This should not be abbreviated (e.g. West Sussex, Normandy, New Jersey). (California)</li> <li>C (Country): The two-letter ISO code for the country where your organization is located (US)</li> <li>EMAIL (Email Address): The organization contact, usually of the certificate administrator or IT department.</li> </ul>"},{"location":"tools/certificates/#create-the-certificate-in-server","title":"Create the certificate in server","text":"<p>The first command we're gonna used is <code>openssl req</code>, which stands for <code>request</code>.</p> <pre><code>#\u00a0Create a Certificate Signing Request. Use ECDSA encryption algorithm instead RSA for production.\nopenssl req -new -newkey rsa:4096 -sha256 -days 365 -out MyCertificate.csr -keyout MyKey.key\n\n# Provide all the information without prompting\nopenssl req -new -newkey rsa:4096 -sha256 -days 365 -out MyCertificate.csr -keyout MyKey.key -subj \"/C=ES/ST=Madrid/L=city/O=JSantosA/OU=IT/CN=*.javiersant.com/emailAddress=jsantosa@gmail.com\"\n\n# Create a Self-signed certificate (-x509) without pass phrase (-nodes)\nopenssl req -new -newkey rsa:4096 -x509 -sha256 -days 365 -nodes -out MySelfSignedCertificate.crt -keyout MyKey.key\n</code></pre> <p>You will be prompted to add identifying information about your website or organization to the certificate. If this certificate will be passed on to a certificate authority (CA) for signing, the information needs to be as accurate as possible. However, for self-signed certificates this information isn't necessary since it is not public.</p> <p>The following is a breakdown of the OpenSSL options used in this command. There are many other options available, but these will create a basic certificate which will be good for a year. For more information, see man openssl in your terminal.</p> <ul> <li><code>-newkey rsa:4096</code>: Create a 4096 bit RSA key for use with the certificate. RSA 2048 is the default on more recent versions of OpenSSL but to be sure of the key size, you should specify it during creation. NOTE: Use <code>ECDSA</code> encryption algorithm instead <code>RSA</code> for production.</li> <li><code>-x509</code>: This option is used to tell <code>openssl</code> to output a self-signed certificate instead of a certificate request. Used for demo or internal purposes.</li> <li><code>-sha256</code>: Generate the certificate request using 265-bit SHA (Secure Hash Algorithm).</li> <li><code>-days</code>: Determines the length of time in days that the certificate is being issued for. For a self-signed certificate, this value can be increased as necessary.</li> <li><code>-nodes</code>: Create a certificate that does not require a passphrase. If this option is excluded, you will be required to enter the passphrase in the console each time the application using it is restarted.</li> </ul> <p>The generated private key (<code>MayKey.key</code>) is encrypted whereas the certificate signing request (<code>MyCertificate.csr</code>) is not, it's base64-encoded only. Note that the information storef in <code>MyCertificate.csr</code> starts with <code>BEGIN CERTIFICATE REQUEST</code>.</p> <pre><code>-----BEGIN CERTIFICATE REQUEST-----\nMIIE2DCCAsACAQAwgZIxCzAJBgNVBAYTAkZSMRYwFAYDVQQIDA1JbGUgZGUgRnJh\nbmNlMQ4wDAYDVQQHDAVQ...pWofr2eOeBQ4Q=\n-----END CERTIFICATE REQUEST-----\n</code></pre> <p>If the certificate is self-signed, we can use the <code>openssl x509</code> command to display all the information encoded in this certificate. Note that the information storef in <code>MySelfSignedCertificate.crt</code> starts with <code>BEGIN CERTIFICATE</code>.</p> <pre><code>-----BEGIN CERTIFICATE-----\nMIIFxjCCA64CCQCNT+eP2vjJxzANBgkqhkiG9w0BAQsFADCBpDELMAkGA1UEBhMC\nRlIxEjAQBgNVBAgMC...udJwE7HnnA7lpA\n-----END CERTIFICATE-----\n</code></pre> <p>This command can also be used to get the information of the certificate (<code>-x509</code>).</p> <pre><code># Get the x509 certificate info\nopenssl x509 -in MySelfSignedCertificate.crt -noout -text\n</code></pre>"},{"location":"tools/certificates/#sign-certificate-request","title":"Sign Certificate Request","text":"<p>To emulate the CA (issuer) for issue the certificate we are going to generate a CA certificate.</p> <pre><code># Generate the private key to become a local CA\nopenssl genrsa -des3 -out MyCA.key 2048\n\n# Generate a root certificate (you can) use crt or pem format\nopenssl req -x509 -new -nodes -key MyCA.key -sha256 -days 1825 -out MyCA.crt -subj \"/C=ES/ST=Madrid/L=city/O=CA/OU=CA/CN=*.ca.org/emailAddress=ca@gmail.com\"\n</code></pre> <p>To sign the certificate, we will use the same <code>openssl x509</code> command that we've used to display certificate before.</p> <pre><code>#\u00a0Sign the Certificate Request using `x509`\nopenssl x509 -req -in MyCertificate.csr -CA MyCA.crt -CAkey MyCA.key -CAcreateserial -out MyCertificate.crt\n</code></pre> <p>In this command, we use the <code>-req</code> option to tell openssl that we're gonna pass in a certificate request. We use the <code>-in</code> option follow by the name of the request file: <code>server-req.pem</code>. Next we use the <code>-CA</code> option to pass in the certificate file of the CA (<code>ca-cert.pem</code>). And the <code>-CAkey</code> option to pass in the private key of the CA ()<code>ca-key.pem</code>).</p> <p>Then important option is <code>-CAcreateserial</code>. Basically the CA must ensure that each certificate it signs goes with a unique serial number. So with this option, a file containing the next serial number will be generated if it doesn't exist.</p> <pre><code># Get the x509 certificate info\nopenssl x509 -in  MyCertificate.crt -noout -text\n</code></pre> <p>To verify a certificate (simulate web browser interaction) you need to compare both CA and server certificates, it can be used using <code>openssl verify</code> command</p> <pre><code># Compare certificates\nopenssl verify -CAfile MyCA.crt MyCertificate.crt\n</code></pre>"},{"location":"tools/certificates/#acme","title":"ACME","text":"<ul> <li>https://www.youtube.com/watch?v=rIwszDULXvc&amp;t=499s</li> </ul>"},{"location":"tools/devpod/","title":"DevPod","text":"<p>DevPod is a client-only tool to create reproducible developer environments based on a devcontainer.json on any backend. Each developer environment runs in a container and is specified through a devcontainer.json. Through DevPod providers, these environments can be created on any backend, such as the local computer, a Kubernetes cluster, any reachable remote machine, or in a VM in the cloud.</p> <p></p> <p>You can think of DevPod as the glue that connects your local IDE to a machine where you want to develop. So depending on the requirements of your project, you can either create a workspace locally on the computer, on a beefy cloud machine with many GPUs, or a spare remote computer. Within DevPod, every workspace is managed the same way, which also makes it easy to switch between workspaces that might be hosted somewhere else.</p> <p></p>"},{"location":"tools/devpod/#why-devpod","title":"Why DevPod?","text":"<p>DevPod reuses the open DevContainer standard (used by GitHub Codespaces and VSCode DevContainers) to create a consistent developer experience no matter what backend you want to use.</p> <p>Compared to hosted services such as Github Codespaces, JetBrains Spaces, or Google Cloud Workstations, DevPod has the following advantages:</p> <ul> <li>Cost savings: DevPod is usually around 5-10 times cheaper than existing services with comparable feature sets because it uses bare virtual machines in any cloud and shuts down unused virtual machines automatically.</li> <li>No vendor lock-in: Choose whatever cloud provider suits you best, be it the cheapest one or the most powerful, DevPod supports all cloud providers. If you are tired of using a provider, change it with a single command.</li> <li>Local development: You get the same developer experience also locally, so you don't need to rely on a cloud provider at all.</li> <li>Cross IDE support: VSCode and the full JetBrains suite is supported, all others can be connected through simple ssh.</li> <li>Client-only: No need to install a server backend, DevPod runs only on your computer.</li> <li>Open-Source: DevPod is 100% open-source and extensible. A provider doesn't exist? Just create your own.</li> <li>Rich feature set: DevPod already supports prebuilds, auto inactivity shutdown, git &amp; docker credentials sync, and many more features to come.</li> <li>Desktop App: DevPod comes with an easy-to-use desktop application that abstracts all the complexity away. If you want to build your own integration, DevPod offers a feature-rich CLI as well.</li> </ul> <p>In order to get started with DevPod, you can choose between the DevPod Desktop application and DevPod CLI.</p>"},{"location":"tools/devpod/#cli","title":"CLI","text":"<p>DevPod CLI can be very useful to control DevPod from a terminal.</p>"},{"location":"tools/devpod/#installation","title":"Installation","text":"<p>Select one of the installation methods available here.</p> <pre><code># MacOS Silicon/ARM\ncurl -L -o devpod \"https://github.com/loft-sh/devpod/releases/latest/download/devpod-darwin-arm64\" &amp;&amp; \\\nsudo install -c -m 0755 devpod /usr/local/bin &amp;&amp; \\\nrm -f devpod\n</code></pre>"},{"location":"tools/devpod/#providers","title":"Providers","text":"<p>The DevPod team maintains providers for popular services such as Docker, kubernetes, SSH, AWS, Google Cloud, Azurem etc..</p> <p>These providers can be installed with the DevPod CLI using following commands.</p> <pre><code># List of available providers\ndevpod provider list-available\n\n# Docker provider\ndevpod provider add docker\n\n# Kubernetes provider\ndevpod provider add kubernetes\n\n#\u00a0Installed providers\ndevpod provider list\n</code></pre>"},{"location":"tools/devpod/#workspace","title":"Workspace","text":"<p>Afterwards you can start by creating workspaces. Microsoft providers a variety of samples projects to test devcontainers vscode-remote-try-*.</p> <p>For kubernetes provider it will create a namespace called <code>devpod</code></p> <pre><code># Start in VS Code browser (Node)\ndevpod up github.com/microsoft/vscode-remote-try-node --ide openvscode\n\n# Start in VS Code (Node)\ndevpod up github.com/microsoft/vscode-remote-try-node --ide vscode\n\n# Start in VS Code (Go)\ndevpod up github.com/microsoft/vscode-remote-try-go --ide vscode\n\n# Start in IntelliJ (Java)\n# NOTE: JetBrains Gateway is required\ndevpod up github.com/microsoft/vscode-remote-try-java --ide intellij\n\n# Start without IDE (Node)\ndevpod up github.com/microsoft/vscode-remote-try-node --ide none\n\n# DevPod IDE commands\ndevpod ide list\n\n# Get Pods in devpod namespace\nkubectl get pods -n devpod\n</code></pre> <p>Useful Commands</p> <pre><code># Shows the status of a workspace\ndevpod status\n\n# Lists existing workspaces\ndevpod list\n\n           NAME          |                         SOURCE                          | MACHINE |  PROVIDER  |   IDE    | LAST USED |  AGE\n-------------------------+---------------------------------------------------------+---------+------------+----------+-----------+---------\n  vscode-remote-try-java | git:https://github.com/microsoft/vscode-remote-try-java |         | kubernetes | intellij | 2m15s     | 2m15s\n  vscode-remote-try-node | git:https://github.com/microsoft/vscode-remote-try-node |         | kubernetes | vscode   | 23m44s    | 26m27s\n\n# Reconnect to the workspace already created\ndevpod up vscode-remote-try-node --ide vscode\ndevpod up vscode-remote-try-java --ide vscode # intellij if JetBrains Gateway available\n\n# Starts a new ssh session-only to a workspace\ndevpod ssh vscode-remote-try-java\n\n# Deletes an existing workspace\ndevpod delete vscode-remote-try-java\n\n# Get Pods\nkubectl get pods -n devpod\n</code></pre>"},{"location":"tools/devpod/#devcontainerjson","title":"devcontainer.json","text":"<p>DevPod uses the open <code>devcontainer.json</code> standard to allow users to customize their development containers. Development containers are Docker containers that provide a user with a fully featured development environment. Within DevPod, this container is created based on the underlying provider either locally, in a remote virtual machine or even in a Kubernetes cluster. DevPod makes sure that no matter where you use this configuration the developer experience stays the same.</p> <pre><code>{\n  \"name\": \"Java\",\n  \"image\": \"mcr.microsoft.com/devcontainers/java:1-21\",\n  \"features\": {\n    \"ghcr.io/devcontainers/features/java:1\": {\n      \"version\": \"none\",\n      \"installMaven\": \"true\",\n      \"mavenVersion\": \"3.8.6\",\n      \"installGradle\": \"false\"\n    }\n  },\n  \"customizations\": {\n    \"vscode\": {\n      \"settings\": {},\n      \"extensions\": [\n        \"streetsidesoftware.code-spell-checker\"\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"tools/dns/","title":"DNS","text":""},{"location":"tools/gitops/","title":"Gitops","text":""},{"location":"tools/gitops/#repository-considerations","title":"Repository considerations","text":""},{"location":"tools/gitops/#monorepo","title":"Monorepo","text":""},{"location":"tools/gitops/#references","title":"References","text":"<ul> <li>Repository Considerations</li> </ul>"},{"location":"tools/iam/","title":"IAM","text":""},{"location":"tools/iam/#identity-provider-idp","title":"Identity Provider (IDP)","text":""},{"location":"tools/iam/#oauth","title":"OAuth","text":""},{"location":"tools/iam/#pkce","title":"PKCE","text":""},{"location":"tools/iam/zidatel-commands/","title":"README","text":"<pre><code># https://zitadel.com/docs/apis/resources/mgmt/management-service-import-human-user\nexport ZITADEL_URL=zitadel.javiersant.com\nexport ZITADEL_PAT=Jf6MOx9h9UaWEfPuWZzCbho13KQSnbwNHNhJaVfpAXz2IxsFVfe7DI9Rqo5lurEI24ryiHM\n\n# Create a User\ncurl -L -k -X POST \"https://$ZITADEL_URL/management/v1/users/human/_import\" \\\n-H 'Content-Type: application/json' \\\n-H 'Accept: application/json' \\\n-H \"Authorization: Bearer $ZITADEL_PAT\" \\\n--data-raw '{\n  \"userName\": \"minnie-mouse\",\n  \"profile\": {\n    \"firstName\": \"Minnie\",\n    \"lastName\": \"Mouse\",\n    \"nickName\": \"Mini\",\n    \"displayName\": \"Minnie Mouse\",\n    \"preferredLanguage\": \"en\",\n    \"gender\": \"GENDER_FEMALE\"\n  },\n  \"email\": {\n    \"email\": \"minnie@mouse.com\",\n    \"isEmailVerified\": true\n  },\n  \"phone\": {\n    \"phone\": \"+41 71 000 00 00\",\n    \"isPhoneVerified\": true\n  },\n  \"password\": \"RootPassword1!\",\n  \"hashedPassword\": {\n    \"value\": \"$2a$12$8p.NrnjLvA/lMC.5kS.LLeja7vc8W.oQt.3d.vwWsAfiJs8juUMiy\"\n  },\n  \"passwordChangeRequired\": false,\n  \"requestPasswordlessRegistration\": false\n}' \\\n| jq .\n\ncurl -k \"https://$ZITADEL_URL/api/public\"\n\ncurl -k \"https://$ZITADEL_URL/api/private-scoped\" \\\n-H \"Authorization: Bearer $ZITADEL_PAT\"\n\n# Create an user an associate it with a Identity provider (IDP)\ncurl -L -k -X POST \"https://$ZITADEL_URL/management/v1/users/human/_import\" \\\n-H 'Content-Type: application/json' \\\n-H 'Accept: application/json' \\\n-H \"Authorization: Bearer $ZITADEL_PAT\" \\\n--data-raw '{\n  \"userName\": \"minnie-mouse\",\n  \"profile\": {\n    \"firstName\": \"Minnie\",\n    \"lastName\": \"Mouse\",\n    \"nickName\": \"Mini\",\n    \"displayName\": \"Minnie Mouse\",\n    \"preferredLanguage\": \"en\",\n    \"gender\": \"GENDER_FEMALE\"\n  },\n  \"email\": {\n    \"email\": \"minnie@mouse.com\",\n    \"isEmailVerified\": true\n  },\n  \"phone\": {\n    \"phone\": \"+41 71 000 00 00\",\n    \"isPhoneVerified\": true\n  },\n  \"password\": \"RootPassword1!\",\n  \"hashedPassword\": {\n    \"value\": \"$2a$12$8p.NrnjLvA/lMC.5kS.LLeja7vc8W.oQt.3d.vwWsAfiJs8juUMiy\"\n  },\n  \"passwordChangeRequired\": false,\n  \"requestPasswordlessRegistration\": true,\n  \"otpCode\": \"string\",\n  \"idps\": [\n    {\n      \"configId\": \"idp-config-id\",\n      \"externalUserId\": \"idp-config-id\",\n      \"displayName\": \"minnie.mouse@gmail.com\"\n    }\n  ]\n}' \\\n| jq .\n</code></pre>"},{"location":"tools/iam/service-user-jwt/","title":"Call a Secured API Using JSON Web Token (JWT) Profile","text":""},{"location":"tools/iam/service-user-jwt/#prerequisites-to-run-the-samples","title":"Prerequisites to Run the Samples","text":"<ul> <li>Clone this repository.</li> <li>Have python3 and pip3 installed in your machine.</li> <li>Install required dependencies by running <code>pip3 install -r requirements.txt</code> on your terminal.</li> <li>Deploy ZITADEL instance into kubernetes cluster. This will create a default Machine with <code>IAM_OWNER</code> role.</li> <li>Make sure that you replace the values in the .env file in each project with the values you obtain from ZITADEL.</li> </ul>"},{"location":"tools/iam/service-user-jwt/#generate-the-token","title":"Generate the Token","text":"<p>Execute the following command to get the <code>client-key-file</code> in json format created automatically by ZIDATEL.</p> <pre><code>kubectl get -n iam secrets zitadel-admin-sa -o=jsonpath='{.data.zitadel-admin-sa\\.json}' | base64 --decode | jq . &gt; docs/iam/service-user-jwt/client-key-file.json\n</code></pre>"},{"location":"tools/iam/service-user-jwt/#1-the-downloaded-key-will-be-of-the-following-format","title":"1. The downloaded key will be of the following format","text":"<pre><code>{\n    \"type\":\"serviceaccount\",\n    \"keyId\":\"&lt;YOUR_KEY_ID&gt;\",\n    \"key\":\"-----BEGIN RSA PRIVATE KEY-----\\n&lt;YOUR_KEY&gt;\\n-----END RSA PRIVATE KEY-----\\n\",\n    \"userId\":\"&lt;YOUR_USER_ID&gt;\"\n}\n</code></pre> <p>Set the environment variables in <code>.env</code> file</p> <pre><code>CLIENT_PRIVATE_KEY_FILE_PATH = \"client-key-file.json\"\nPROJECT_ID=\"zitadel\"\nZITADEL_DOMAIN = \"https://zitadel.javiersant.com\"\nZITADEL_TOKEN_URL = \"https://zitadel.javiersant.com/oauth/v2/token\"\n</code></pre>"},{"location":"tools/iam/service-user-jwt/#2-generate-the-token","title":"2. Generate the token","text":"<p>This process will generate a token using the private key from <code>client-key-file</code>.</p> <ol> <li>cd to this directory: <code>cd service-user-jwt/</code></li> <li>Copy the content in your downloaded key file to <code>client-key-file.json</code>.</li> <li>Replace the values of PROJECT_ID, ZITADEL_DOMAIN and ZITADEL_TOKEN_URL in the <code>.env file</code> with your values you obtained earlier.</li> <li>Run the script to generate a token by running <code>python3 jwt-profile-token-generator.py</code> in the terminal.</li> <li>Copy the printed access token and set the value to a shell variable called <code>TOKEN</code> as shown below.</li> </ol> <pre><code># Execute following command\ncd docs/iam/service-user-jwt/\npython3 jwt-profile-token-generator.py\n\n# Get only the token\npython3 jwt-profile-token-generator.py | grep \"Access token:\" | awk '{print $3}'\n</code></pre> <p>Because conflicting python jwt libraries, you could have the following error <code>jwt.encode</code>.</p> <pre><code>pip3 uninstall jwt\npip3 uninstall PyJWT\npip3 install PyJWT\n</code></pre>"},{"location":"tools/iam/service-user-jwt/#3-invoke-the-api-with-token","title":"3. Invoke the API with Token","text":"<p>Create a random user</p> <pre><code># https://zitadel.com/docs/apis/resources/mgmt/management-service-import-human-user\nexport ZITADEL_URL=zitadel.javiersant.com\nexport ZITADEL_PAT=$(python3 jwt-profile-token-generator.py | grep \"Access token:\" | awk '{print $3}')\n\n# Create a User\ncurl -L -k -X POST \"https://$ZITADEL_URL/management/v1/users/human/_import\" \\\n-H 'Content-Type: application/json' \\\n-H 'Accept: application/json' \\\n-H \"Authorization: Bearer $ZITADEL_PAT\" \\\n--data-raw '{\n  \"userName\": \"admin\",\n  \"profile\": {\n    \"firstName\": \"admin\",\n    \"lastName\": \"admin\",\n    \"nickName\": \"admin\",\n    \"displayName\": \"Admin\",\n    \"preferredLanguage\": \"en\",\n    \"gender\": \"GENDER_MALE\"\n  },\n  \"email\": {\n    \"email\": \"admin@zitadel.com\",\n    \"isEmailVerified\": true\n  },\n  \"phone\": {\n    \"phone\": \"+41 71 000 00 00\",\n    \"isPhoneVerified\": true\n  },\n  \"password\": \"RootPassword1!\",\n  \"hashedPassword\": {\n    \"value\": \"$2a$12$8p.NrnjLvA/lMC.5kS.LLeja7vc8W.oQt.3d.vwWsAfiJs8juUMiy\"\n  },\n  \"passwordChangeRequired\": false,\n  \"requestPasswordlessRegistration\": false\n}' \\\n| jq .\n</code></pre> <p>List Member roles</p> <pre><code>curl -L -k -X POST \"https://$ZITADEL_URL/management/v1/orgs/members/roles/_search\" \\\n-H 'Accept: application/json' \\\n-H \"Authorization: Bearer $ZITADEL_PAT\" \\\n| jq .\n</code></pre> <p>Add Role To User</p> <pre><code>export ZITADEL_USER_ID=257116669862740269\nexport ZITADEL_USER_ROLE=ORG_OWNER\ncurl -L -k -X POST \"https://$ZITADEL_URL/management/v1/orgs/me/members\" \\\n-H 'Content-Type: application/json' \\\n-H 'Accept: application/json' \\\n-H \"Authorization: Bearer $ZITADEL_PAT\" \\\n--data-raw \"{\n  \\\"userId\\\": \\\"$ZITADEL_USER_ID\\\",\n  \\\"roles\\\": [\n    \\\"$ZITADEL_USER_ROLE\\\"\n  ]\n}\" \\\n| jq .\n</code></pre> <p>Get all members of the organization</p> <pre><code>curl -L -k -X POST \"https://$ZITADEL_URL/management/v1/orgs/me/members/_search\" \\\n-H 'Content-Type: application/json' \\\n-H 'Accept: application/json' \\\n-H \"Authorization: Bearer $ZITADEL_PAT\" \\\n--data-raw '{\n  \"query\": {\n    \"offset\": \"0\",\n    \"limit\": 100,\n    \"asc\": true\n  },\n  \"queries\": []\n}' \\\n| jq .\n</code></pre>"},{"location":"tools/iam/tofu/","title":"OpenTofu","text":"<p>OpenTofu is an open-source project that serves as a fork of the legacy MPL-licensed Terraform. It was developed as a response to a change in HashiCorp's licensing of Terraform, from Mozilla Public License (MPL) to a Business Source License (BSL), which imposed limitations on the use of Terraform for commercial purposes. As an alternative, OpenTofu aims to offer a reliable, community-driven solution under the Linux Foundation.</p> <p>The elements of OpenFou/terraform are:</p> <ul> <li>Providers are essentially plugins enabling OpenTofu to interact with various infrastructure resources.</li> <li>Resources in OpenTofu refer to the infrastructure elements that OpenTofu can manage.</li> <li>Data source in OpenTofu is a configuration element designed to <code>fetch</code> data from an external source.</li> </ul>"},{"location":"tools/iam/tofu/#install","title":"Install","text":"<p>there are multiple ways yo install OpenTofu, from MacOs the easiest way is to use <code>Homebrew</code>.</p> <pre><code># Install using following commands.\nbrew update\nbrew install opentofu\n\n# Verify the installation\ntofu --version\n</code></pre> <p>Create an <code>IngressRoute</code> with <code>h2c</code> scheme to allow provider to connect via grpc in a secure way.</p> <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: zitadel-dashboard\n  namespace: iam\n  annotations:\n    kubernetes.io/ingress.class: traefik-external\n    external-dns.alpha.kubernetes.io/target: javiersant.com\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`zitadel.javiersant.com`)\n      kind: Rule\n      services:\n        - kind: Service\n          name: zitadel\n          scheme: h2c\n          passHostHeader: true\n          port: 8080\n  tls:\n    secretName: wildcard-javiersant-com-tls\n</code></pre>"},{"location":"tools/iam/tofu/#structure","title":"Structure","text":"<p>Create <code>versions.tf</code> and <code>main.tf</code> with the providers and versions to enable managing Zitadel resources.</p>"},{"location":"tools/iam/tofu/#execute","title":"Execute","text":"<p>Get the <code>client-key-file</code> in JSON format created automatically by <code>ZIDATEL</code> deployment.</p> <pre><code>kubectl get -n iam secrets zitadel-admin-sa -o=jsonpath='{.data.zitadel-admin-sa\\.json}' | base64 --decode | jq . &gt; ../service-user-jwt/client-key-file.json\n</code></pre> <p>Initialize Tofu project to download dependencies</p> <p>Install self-signed certificates if needed by going to browser, download the <code>.pem</code> and installing certificate into <code>Login</code> category within <code>KeyChain Access</code> tool.</p> <pre><code># Remove previous state\nrm terraform.tfs*\n\n# Initialize and upgrade project (migration from Terraform)\ntofu init -upgrade\n</code></pre> <p>Verify the changes to the plan</p> <pre><code># Get the tofu plan\ntofu plan\n\n# Debug Traces\nTF_LOG=\"DEBUG\" tofu plan\n</code></pre> <p>Apply the changes</p> <pre><code># Get the tofu plan\ntofu apply\n</code></pre> <p>Get data from outputs</p> <pre><code># Get generated clientId (use '-raw' to get variable without quotes)\ntofu output -raw zitadel_application_client_id\n\n# Get generated SecretId\ntofu output -raw zitadel_application_client_secret\n</code></pre>"},{"location":"tools/iam/tofu/#local-providers","title":"Local Providers","text":"<p>Copy the generated plugin version compiled (<code>go build</code>) using following structure</p> <pre><code> ~/.terraform.d\n    \u2514\u2500\u2500 plugins\n        \u2514\u2500\u2500 local.providers\n            \u2514\u2500\u2500 zitadel\n                \u2514\u2500\u2500 zitadel\n                    \u2514\u2500\u2500 1.1.1\n                        \u2514\u2500\u2500 darwin_arm64\n                            \u251c\u2500\u2500 LICENSE\n                            \u251c\u2500\u2500 README.md\n                            \u2514\u2500\u2500 terraform-provider-zitadel_v1.1.1\n</code></pre> <p>Add following provider into <code>versions.tf</code></p> <pre><code># Add local provider\nterraform {\n  required_providers {\n    zitadel = {\n      source  = \"local.providers/zitadel/zitadel\"\n      version = \"1.1.1\"\n    }\n  }\n}\n</code></pre>"},{"location":"tools/iam/tofu/#job","title":"Job","text":"<p>Create a Job to bootstrap Zitadel configuration.</p> <pre><code># Apply manifests\nkubectl apply -k docs/tools/iam/manifests\n\n# Get all manifests\nkubectl get secrets,job,pods -n iam\n\n# Get and execute the pod\n# command: [ \"sleep\" ]\n# args: [ \"infinity\" ]\nkubectl exec -it $(kubectl get pods -n iam -l job-name=oauth2-proxy-default-user  | tail -n 1 | awk {'print $1'}) -n iam  -- sh\n\n#\u00a0Run commands\n\n# Delete manifest\nkubectl delete -k docs/tools/iam/manifests\n</code></pre> <p>Execute following scritp</p> <pre><code># Run alpine image\n#\u00a0docker run -it --rm alpine:3.19.1\n\n# TODO: Build docker container for immutable versions\n\n#\u00a0Install utils\napk add curl git openssl kubectl\n\n# Install opentofu\nOPEN_TOFU_VERSION=1.6.2\nOPEN_TOFU_ARCH=arm64\nwget https://github.com/opentofu/opentofu/releases/download/v${OPEN_TOFU_VERSION}/tofu_${OPEN_TOFU_VERSION}_${OPEN_TOFU_ARCH}.apk -O tofu.apk\napk add --allow-untrusted tofu.apk\n\n# Check installed correctly\ntofu -v\n\n#\u00a0Clone the repository\n# git clone https://github.com/jsa4000/homelab-ops.git\ngit clone https://${GITHUB_PAT}@github.com/jsa4000/homelab-ops.git\ncd homelab-ops/infrastructure/terraform/zitadel/\n\n# Get HTTP Status Code\necho \"Wating to connect to Zitadel\"\nstatus_code=$(curl -sk -o /dev/null -w \"%{http_code}\" https://zitadel.javiersant.com/debug/ready)\nif [[ \"$status_code\" -ne 200 ]] ; then\n  sleep 5m\nfi\n\n#\u00a0Download the certificate\n#\u00a0ls /usr/local/share/ca-certificates/\nopenssl s_client -connect zitadel.javiersant.com:443 -servername zitadel.javiersant.com &lt;/dev/null | openssl x509 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' &gt; javiersant.com.pem\ncp javiersant.com.pem /usr/local/share/ca-certificates/javiersant.com.pem\nupdate-ca-certificates\n\n#\u00a0Init tofu providers\ntofu init -upgrade\ntofu apply -auto-approve -var jwt_profile_file=/etc/config/zitadel-admin-sa.json\n\n# Create the secret after run OpenTofu\nkubectl create secret -n iam generic oauth2-proxy \\\n    --from-literal=client-id=$(tofu output -raw zitadel_application_client_id) \\\n    --from-literal=client-secret=$(tofu output -raw zitadel_application_client_secret) \\\n    --from-literal=cookie-secret=$(LC_ALL=C tr -dc A-Za-z0-9 &lt;/dev/urandom | head -c 32)\n</code></pre>"},{"location":"tools/k3s/","title":"K3s","text":""},{"location":"tools/k3s/#install","title":"Install","text":""},{"location":"tools/k3s/#containerd","title":"Containerd","text":"<pre><code># Systems\nSYSTEM_OS=linux\nSYSTEM_ARCH=arm64\n\n# Versions\nCONTAINERD_VERSION=1.7.15\nRUNC_VERSION=1.1.12\nCNI_VERSION=1.4.1\n\n# Download containerd\nwget https://github.com/containerd/containerd/releases/download/v$CONTAINERD_VERSION/containerd-$CONTAINERD_VERSION-$SYSTEM_OS-$SYSTEM_ARCH.tar.gz\n\n# Unpack\nsudo tar Cxzvf /usr/local containerd-$CONTAINERD_VERSION-$SYSTEM_OS-$SYSTEM_ARCH.tar.gz\n\n#Install runc\nwget https://github.com/opencontainers/runc/releases/download/v$RUNC_VERSION/runc.$SYSTEM_ARCH\nsudo install -m 755 runc.$SYSTEM_ARCH /usr/local/sbin/runc\n\n# Download and install CNI plugins :\nwget https://github.com/containernetworking/plugins/releases/download/v$CNI_VERSION/cni-plugins-linux-$SYSTEM_ARCH-v$CNI_VERSION.tgz\nsudo mkdir -p /opt/cni/bin\nsudo tar Cxzvf /opt/cni/bin cni-plugins-$SYSTEM_OS-$SYSTEM_ARCH-v$CNI_VERSION.tgz\nsudo chown -R root:root /opt/cni/bin\n\n# Configure containerd\nsudo mkdir /etc/containerd\ncontainerd config default | sudo tee /etc/containerd/config.toml\n#\u00a0NOTE: kubelet and containerd config need to agree about what cgroup driver to use, so do not modify containerd config\n# In k3s it would be needed to set '--kubelet-arg cgroup-driver=systemd'\nsudo sed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml\n# Pperform syncfs after pull https://github.com/containerd/containerd/pull/9401\nsudo sed -i 's/image_pull_with_sync_fs \\= false/image_pull_with_sync_fs \\= true/g' /etc/containerd/config.toml\nsudo curl -L https://raw.githubusercontent.com/containerd/containerd/main/containerd.service -o /etc/systemd/system/containerd.service\n\n# Step 5: Start containerd service\nsudo systemctl daemon-reload\nsudo systemctl enable --now containerd\n</code></pre> <p>Verify <code>containerd</code> is running</p> <pre><code># Verify containerd is running\nsudo systemctl status containerd\n</code></pre> <p>Remove containerd</p> <pre><code># Stop containerd\nsudo systemctl disable --now containerd\n\n# Remove containerd\nsudo rm -r /etc/containerd\nsudo rm -r /run/containerd\nsudo rm -r /var/lib/containerd/\nsudo rm /etc/systemd/system/containerd.service\n</code></pre>"},{"location":"tools/k3s/#k3s_1","title":"k3s","text":"<p>Create a <code>k3s</code> with not HA (High Availability) on master nodes and not distributed <code>etcd</code>.</p> <pre><code># Create Master Note (192.168.205.101)\n# Use K3S_KUBECONFIG_MODE=\"644\" for develop, so sudo is not needed\nINSTALL_K3S_VERSION=v1.31.5+k3s1\ncurl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=$INSTALL_K3S_VERSION K3S_KUBECONFIG_MODE=\"644\" sh -\n\n# Get the token from master\necho \"K3S_TOKEN=$(sudo cat /var/lib/rancher/k3s/server/node-token)\"\n\n# Join Workers\n# Copy the previous K3S_TOKEN into workers.\nK3S_TOKEN=&lt;token&gt;\ncurl -sfL https://get.k3s.io | K3S_URL=https://192.168.205.101:6443 K3S_TOKEN=$K3S_TOKEN sh -\n\n# Test example deployment\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/docs/examples/chashsubset/deployment.yaml\nkubectl get pods -w -o wide\n\n# Uninstalling Servers\n/usr/local/bin/k3s-uninstall.sh\n\n# Uninstalling Agents\n/usr/local/bin/k3s-agent-uninstall.sh\n</code></pre> <p>Create <code>k3s</code> cluster with already existing <code>containerd</code> using <code>--container-runtime-endpoint</code> flag.</p> <p>containerd default socket is <code>unix:///run/containerd/containerd.sock</code></p> <pre><code># Create Master Note (192.168.205.101)\n# Use K3S_KUBECONFIG_MODE=\"644\" for develop, so sudo is not needed\nINSTALL_K3S_VERSION=v1.31.5+k3s1\ncurl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=$INSTALL_K3S_VERSION K3S_KUBECONFIG_MODE=\"644\" sh -s - --container-runtime-endpoint=\"unix:///run/containerd/containerd.sock\"\n\n# Get the token from master\necho \"K3S_TOKEN=$(sudo cat /var/lib/rancher/k3s/server/node-token)\"\n\n# Join Workers\n# Copy the previous K3S_TOKEN into workers.\nK3S_TOKEN=&lt;token&gt;\ncurl -sfL https://get.k3s.io | K3S_URL=https://192.168.205.101:6443 K3S_TOKEN=$K3S_TOKEN sh -s - --container-runtime-endpoint=\"unix:///run/containerd/containerd.sock\"\n\n#\u00a0install CNI\nkubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\n\n# Test example deployment\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/docs/examples/chashsubset/deployment.yaml\nkubectl get pods -o wide -w\n\n# Uninstalling Servers\n/usr/local/bin/k3s-uninstall.sh\n\n# Uninstalling Agents\n/usr/local/bin/k3s-agent-uninstall.sh\n</code></pre>"},{"location":"tools/k3s/#faq","title":"FAQ","text":""},{"location":"tools/k3s/#error-exec-format-error","title":"Error <code>exec format error</code>","text":"<p>This container error is mostly due the following scenarios:</p> <ul> <li>The image currently running is not supported by the OS. For example running <code>amd64</code> image into <code>arm64</code> platform will throw this exception.</li> <li>By missing the <code>command</code> or <code>entrypoint</code> of the OCI image, so when the container start it can reach to this error.</li> <li>Lastly, and the actual error, is when the layers are pulled by the container runtime, but for some reason (restart, networking issue, I/O Throttling, etc..) those become corrupted so any time the container starts it throws an error.</li> </ul> <p>In <code>containerd</code> configuration file <code>/etc/containerd/config.toml</code>, set <code>image_pull_with_sync_fs = true</code> to check and sync the image layers from the <code>snapshotter</code>, usually the recommended <code>overlayfs</code>.</p> <ul> <li>exec user process caused: exec format error</li> <li>Add option to perform syncfs after pull</li> </ul> <pre><code>[plugins.\"io.containerd.grpc.v1.cri\"]\n\n    image_pull_with_sync_fs = true\n</code></pre>"},{"location":"tools/k3s/#error-kernel-modules","title":"Error Kernel modules","text":"<p><code>error=\"program cil_from_overlay: replacing clsact qdisc for interface cilium_vxlan: no such file or directory\"</code></p> <p>In order for the BPF feature to be enabled properly, the following kernel configuration options must be enabled. This is typically the case with distribution kernels. When an option can be built as a module or statically linked, either choice is valid.</p> <pre><code># Get Kernel installed\nsudo cat /boot/config-6.1.43-rockchip-rk3588 | grep -E \"CONFIG_BPF|CONFIG_BPF_SYSCALL|CONFIG_NET_CLS_BPF|CONFIG_BPF_JIT|CONFIG_NET_CLS_ACT|CONFIG_NET_SCH_INGRESS|CONFIG_CRYPTO_SHA1|CONFIG_CRYPTO_USER_API_HASH\"\n\n# Add single modules to kernel\nsudo modprobe xfrm_user\n\n# Add multiple modules to kernel\nsudo modprobe -a ipt_REJECT xt_mark xt_multiport xt_TPROXY xt_CT sch_ingress ip_set cls_bpf xfrm_user configs\n\n# Check Kernel modules\ncat /proc/modules | grep ipt_REJECT\n</code></pre>"},{"location":"tools/k3s/#kubernetes-node-not-ready","title":"Kubernetes Node Not Ready","text":"<ul> <li>Kubernetes Node Not Ready Error and How to Fix It</li> <li>Diagnosis and Troubleshooting of a Kubernetes Node in \"Not Ready\" State</li> <li>Worker latency profiles</li> <li>How To Make Kubernetes React Faster When Nodes Fail?</li> </ul> <p>This is what happens when node dies or go into offline mode:</p> <ul> <li>The <code>kubelet</code> posts its status to masters by <code>--node-status-update-fequency=10s</code>.</li> <li><code>kube-controller-manager</code> is monitoring all the nodes by <code>--node-monitor-period=5s</code></li> <li><code>kube-controller-manager</code> will see the node is unresponsive and has the grace period <code>--node-monitor-grace-period=40s</code> until it considers node unhealthy. &gt; This parameter should be in N x <code>node-status-update-fequency</code></li> <li>Once the node marked unhealthy, the <code>kube-controller-manager</code> will remove the pods based on <code>--pod-eviction-timeout=5m</code></li> </ul> <p>Now, if you tweaked the parameter <code>pod-eviction-timeout</code> to say 30 seconds, it will still take total 70 seconds to evict the pod from node The <code>node-status-update-fequency</code> and <code>node-monitor-grace-period</code> time counts in <code>node-monitor-grace-period</code> also. You can tweak these variable as well to further lower down your total node eviction time.</p>"},{"location":"tools/k3s/#medium-worker-latency-profile","title":"Medium worker latency profile","text":"<ul> <li>Use the MediumUpdateAverageReaction profile if the network latency is slightly higher than usual.</li> <li>The MediumUpdateAverageReaction profile reduces the frequency of kubelet updates to 20 seconds and changes the period that the Kubernetes Controller Manager waits for those updates to 2 minutes. The pod eviction period for a pod on that node is reduced to 60 seconds. If the pod has the tolerationSeconds parameter, the eviction waits for the period specified by that parameter.</li> <li>The Kubernetes Controller Manager waits for 2 minutes to consider a node unhealthy. In another minute, the eviction process starts.</li> </ul> Component Parameter Value kubelet <code>node-status-update-frequency</code> <code>1m</code> Kubelet Controller Manager <code>node-monitor-grace-period</code> <code>5m</code> Kubernetes API Server Operator <code>default-not-ready-toleration-seconds</code> <code>60s</code> Kubernetes API Server Operator <code>default-unreachable-toleration-seconds</code> <code>60s</code>"},{"location":"tools/k3s/#low-worker-latency-profile","title":"Low worker latency profile**","text":"<ul> <li>Use the LowUpdateSlowReaction profile if the network latency is extremely high.</li> <li>The LowUpdateSlowReaction profile reduces the frequency of kubelet updates to 1 minute and changes the period that the Kubernetes Controller Manager waits for those updates to 5 minutes. The pod eviction period for a pod on that node is reduced to 60 seconds. If the pod has the tolerationSeconds parameter, the eviction waits for the period specified by that parameter.</li> <li>The Kubernetes Controller Manager waits for 5 minutes to consider a node unhealthy. In another minute, the eviction process starts.</li> </ul> Component Parameter Value kubelet <code>node-status-update-frequency</code> <code>20s</code> Kubelet Controller Manager <code>node-monitor-grace-period</code> <code>2m</code> Kubernetes API Server Operator <code>default-not-ready-toleration-seconds</code> <code>60s</code> Kubernetes API Server Operator <code>default-unreachable-toleration-seconds</code> <code>60s</code>"},{"location":"tools/k3s/#troubleshooting","title":"Troubleshooting","text":"<p>For troubleshooting use following command:</p> <pre><code>#\u00a0Watch the pods and nodes statuses\nwatch -n 0.5 kubectl get pods -A -o wide\nwatch -n 0.5 kubectl get pods -A -o wide\n\n# Check that you have resolved any Additional OS Preparation\nk3s check-config\n\n# Check the events throw to kubernetes to get the state\nkubectl get events -A\n\n#\u00a0Get the status\nsudo systemctl status k3s\n\n# Get the logs\nsudo journalctl -u k3s | grep kubelet\n\n#\u00a0Get logs from agent\nsystemctl status k3s-agent\nsudo journalctl -u k3s-agent -xn | less\n</code></pre> <p>When a pod get stucks into Terminating state, use these commands to remove the pod completely.</p> <pre><code># Force to dekete the pod\nkubectl delete pod &lt;pod&gt; -n &lt;namespace&gt; --grace-period=0 --force\n\n# Modify the finalizers that prevents finish the pod.\nkubectl patch pod &lt;pod&gt;  -n &lt;namespace&gt; -p '{\"metadata\":{\"finalizers\":null}}'\n</code></pre>"},{"location":"tools/manifest/","title":"README","text":""},{"location":"tools/manifest/#usage","title":"Usage","text":"<pre><code># Get all pods\nkubectl get pods,services -A -o wide\n\n# Create godaddy api key at https://developer.godaddy.com/\nexport GODADDY_API_KEY=&lt;MY-APY-KEY&gt;\nexport GODADDY_SECRET_KEY=&lt;MY-SECRET-KEY\n\n# Create Github Crendentials\nexport GITHUB_REPO=&lt;GITHUB_REPO&gt;\nexport GITHUB_USERNAME=&lt;GITHUB_USERNAME&gt;\nexport GITHUB_PASSWORD=&lt;GITHUB_PASSWORD&gt;\n\n# Open Router NAT ports at http://192.168.3.1\nsudo socat TCP-LISTEN:8443,fork TCP:192.168.205.200:443     # LoadBalancer\nsudo socat TCP-LISTEN:8443,fork TCP:192.168.205.101:30443   # NodePort\n\n# Set environment (copy to root path if desired)\nsource \"/Users/jsantosa/Library/CloudStorage/GoogleDrive-jsa4000@gmail.com/My Drive/Ocio/Cluster/keys/.env\"\n\n# Create global secret\nkubectl create namespace security\nkubectl create secret -n security generic cluster-secrets \\\n    --from-literal=GODADDY_API_KEY=$GODADDY_API_KEY \\\n    --from-literal=GODADDY_SECRET_KEY=$GODADDY_SECRET_KEY \\\n    --from-literal=GITHUB_REPO=$GITHUB_REPO \\\n    --from-literal=GITHUB_USERNAME=$GITHUB_USERNAME \\\n    --from-literal=GITHUB_PAT=$GITHUB_PAT\n\n# Add to /etc/hosts\n# 192.168.205.200 traefik.javiersant.com grafana.javiersant.com prometheus.javiersant.com longhorn.javiersant.com argocd.javiersant.com zitadel.javiersant.com oauth.javiersant.com\n\n#########################\n# Cilium\n#########################\n\n# Deploy cilium\nkubectl create namespace networking\nkubectl kustomize clusters/local/addons/networking/cilium --enable-helm | kubectl apply -f -\nkubectl kustomize clusters/remote/addons/networking/cilium --enable-helm | kubectl apply -f -\n\n#\u00a0Wait until cilium operator and agents are ready\nkubectl wait --for=condition=ready pod -n networking -l name=cilium-operator\nkubectl wait --for=condition=ready pod -n networking -l k8s-app=cilium\n\n# Default configuration\nkubectl kustomize kubernetes/addons/networking/cilium --enable-helm | kubectl apply -f -\n\n# Remove cilium\nkubectl kustomize kubernetes/addons/networking/cilium --enable-helm | kubectl delete -f -\n\n#########################\n# External Secrets\n#########################\n\n# Deploy external-secrets\nkubectl create namespace security\nkubectl kustomize kubernetes/addons/security/external-secrets --enable-helm | kubectl apply -f -\n\n#\u00a0Local\nkubectl kustomize clusters/local/addons/security/external-secrets --enable-helm | kubectl apply -f -\n\n# Remove external-secrets\nkubectl kustomize kubernetes/addons/security/external-secrets --enable-helm | kubectl delete -f -\n\n# Get services deployed\nkubectl get pods,services -n security\nkubectl get ClusterSecretStore,SecretStore -n security\n\n#########################\n# Deploy Argo-cd\n#########################\n\n# Deploy Argo-cd\nkubectl create namespace gitops\nkubectl kustomize clusters/local/addons/gitops/argocd --enable-helm | kubectl apply -f -\n\nkubectl kustomize clusters/remote/addons/gitops/argocd --enable-helm | kubectl apply -f -\n\n# https://argo-cd.readthedocs.io/en/stable/operator-manual/argocd-cmd-params-cm-yaml/\n\n# Create Github Credentials\n# https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#repositories\ncat &lt;&lt;EOF | kubectl apply -n gitops -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: github-secret\n  namespace: argo-cd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: $GITHUB_REPO\n  username: $GITHUB_USERNAME\n  password: $GITHUB_PAT\nEOF\n\n# Remove Argo-cd\nkubectl kustomize kubernetes/addons/gitops/argocd --enable-helm | kubectl delete -f -\n\n# Get services deployed\nkubectl get pods,services -n gitops\n\n# Connect to Argocd\nkubectl port-forward svc/argocd-server -n gitops 8080:80\n\n# https://argocd.javiersant.com\n\n# Get the \"admin\" password\nkubectl get secret argocd-initial-admin-secret -n gitops -o jsonpath=\"{.data.password}\" | base64 -d\n\n# Apply addons\nkubectl apply -f kubernetes/bootstrap/addons-appset.yaml\n\n# Apply apps\nkubectl apply -f kubernetes/bootstrap/apps-appset.yaml\n\n# NOTES:\n# 1. It is needed to open port 443 on router and route traffic (port-forwarding) for local environment\n#   - LoadBalancer: sudo socat TCP-LISTEN:8443,fork TCP:192.168.205.200:443\n#   - NodePort:     sudo socat TCP-LISTEN:8443,fork TCP:192.168.205.101:30443\n# 2. Sometimes it's needed to go to ArgoCD UI and \"Terminate\" then force to Sync again to trigger the creation. (Zitadel)\n# 3. Delete de Job from oauth-proxy, since it depends from previous task. `kubectl delete -n iam job oauth2-proxy-zitadel-init`\n# 4. When connection errors from argocd server use 'kubectl -n gitops delete pods --all'\n# 4. Go to https://zitadel.javiersant.com and set the new password (admin/RootPassword1!)\n\n# Wathc all the pods whili initializing\nwatch -n 0.5 kubectl get pods -A\n\n# Specific layer\nkubectl apply -n gitops -f kubernetes/addons/gitops/appset.yaml\nkubectl apply -n gitops -f kubernetes/addons/kube-system/appset.yaml\nkubectl apply -n gitops -f kubernetes/addons/security/appset.yaml\nkubectl apply -n gitops -f kubernetes/addons/networking/appset.yaml\nkubectl apply -n gitops -f kubernetes/addons/storage/appset.yaml\nkubectl apply -n gitops -f kubernetes/addons/database/appset.yaml\nkubectl apply -n gitops -f kubernetes/addons/observability/appset.yaml\nkubectl apply -n gitops -f kubernetes/addons/iam/appset.yaml\n\n# cri-tools\n# https://github.com/kubernetes-sigs/cri-tools\n# CLI and validation tools for Kubelet Container Runtime Interface (CRI) .\n\n# Get the runtime container name and version\nsudo crictl version\n\nVersion:  0.1.0\nRuntimeName:  containerd\nRuntimeVersion:  v1.7.11-k3s2\nRuntimeApiVersion:  v1\n\n# Get containers from K3s\nsudo k3s crictl ps\nsudo crictl ps\n\n# Get images from K3s\nsudo crictl images\nsudo crictl images --verbose\n\n# Remove image that had has a problem with \"exec format error...\"\n# https://github.com/containerd/containerd/issues/5854\n# https://github.com/containerd/containerd/pull/9401\n# https://stackoverflow.com/questions/71572715/decoupling-k3s-and-containerd\n# https://github.com/kinvolk/containerd-cri/blob/master/docs/installation.md\nkubectl get pods -A -o wide | grep CrashLoopBackOff | awk '{print $1\" \"$2}'\nkubectl get pod -n storage csi-resizer-7466f7b45f-bttgv -o jsonpath='{$.spec.nodeName}'\nkubectl get pod -n storage longhorn-csi-plugin-nglx2  -o jsonpath='{$.spec.containers[].image}'\n\nkubectl get deployment -n storage\nkubectl scale deployment -n storage csi-provisioner --replicas=0\n\nsudo crictl images | grep csi-provisioner\nsudo crictl rmi 2c4e3070dfbc0\nsudo crictl rmi --prune\n\nkubectl scale deployment -n storage csi-provisioner --replicas=3\n\nsudo ctr images pull docker.io/csi-node-driver-registrar:v2.9.2\n\nkubectl delete pod -n storage csi-resizer-7466f7b45f-bttgv\n\n# Check if still don0t working\nkubectl get pod -A -o wide| grep engine-image\n\n# Force to pull the image or delete the pod.\nsudo ctr images pull docker.io/longhornio/csi-resizer:v1.9.2\n\n# https://github.com/longhorn/longhorn/discussions/7117\nsudo k3s ctr content ls | grep longhorn-manager | awk '{print $1}' | xargs sudo k3s ctr content del\nsudo crictl rmi docker.io/longhornio/longhorn-manager:v1.6.0\nsudo ctr images pull docker.io/longhornio/longhorn-manager:v1.6.0\n\n# Ensure if snapshot's pagecache has been discarded in a unexpected reboot\n# https://github.com/juliusl/containerd/commit/57f5e1d2f0af89b6e4ae9597422eb501b7151c76\nsudo ctr image usage --snapshotter overlayfs docker.io/longhornio/longhorn-manager:v1.6.0\n\n# Restart pods by Status\nkubectl get pods --all-namespaces | grep Unknown | awk '{print $2 \" --namespace=\" $1}' | xargs kubectl delete --force pod\nkubectl get pods --all-namespaces | grep Terminating | awk '{print $2 \" --namespace=\" $1}' | xargs kubectl delete --force pod\nkubectl get pods --all-namespaces | grep CrashLoopBackOff | awk '{print $2 \" --namespace=\" $1}' | xargs kubectl delete --force pod\nkubectl get pods --all-namespaces | grep instance-manager | awk '{print $2 \" --namespace=\" $1}' | xargs kubectl delete --force pod\n\n# Restart pods by any Status\nkubectl delete -A --field-selector 'status.phase!=Running' pods --force\n\n# Restart the entire deployment and replicas\nkubectl get deployments -n gitops\nkubectl rollout restart deployment -n gitops argocd-server\nkubectl rollout restart deployment -n gitops argocd-repo-server\nkubectl rollout restart deployment -n gitops argocd-redis\nkubectl rollout restart deployment -n gitops argocd-applicationset-controller\n\n# Multiple Kube-System pods not running with Unknown Status\n# https://github.com/k3s-io/k3s/issues/6185\nsudo k3s ctr container rm $(sudo find /var/lib/cni/flannel/ -size 0 | sudo xargs -n1 basename)\n\nsudo find /var/lib/cni/flannel/ -size 0 -delete\nsudo find /var/lib/cni/results/ -size 0 -delete # cached result\n\n#########################\n# Deploy Metallb\n#########################\n\n# Deplot Metallb Operator\nkubectl apply -k manifests/metallb\n\n# Deploy Metallb Pool (Configuration)\nkubectl apply -k manifests/metallb-pool/overlays/local\n\n# Check Metallb Pool overlays (Preview)\nkubectl create namespace networking\nkubectl kustomize clusters/remote/addons/networking/metallb --enable-helm | kubectl apply -f -\n\n# Remove Metallb\nkubectl kustomize clusters/remote/addons/networking/metallb --enable-helm | kubectl delete -f -\n\n# kubectl apply -k manifests/metallb-pool/overlays/home -o yaml\n# kubectl apply -k manifests/metallb-pool/overlays/local -o yaml\n\n# Deploy nginx (it will create a load balancer)\nkubectl apply -f kubernetes/apps/nginx\n\n#\u00a0Check the LoadBalancer has been assigned to the service\nkubectl get pods,services\n\n# http://192.168.205.210\n\n#########################\n# External DNS\n#########################\n\n# https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/godaddy.md\n# https://github.com/anthonycorbacho/external-dns/blob/master/docs/tutorials/traefik-proxy.md\n\n# Deploy external-dns\nkubectl kustomize manifests/external-dns  --enable-helm | kubectl apply -f -\n\n# Overlay\nkubectl kustomize clusters/remote/addons/networking/goddady-external-dns --enable-helm | kubectl apply -f -\n\n#\u00a0NOTE: Already created using external-secrets\nkubectl create secret -n networking generic external-dns-godaddy \\\n    --from-literal=GODADDY_API_KEY=$GODADDY_API_KEY \\\n    --from-literal=GODADDY_SECRET_KEY=$GODADDY_SECRET_KEY\n\n# Remove external-dns\nkubectl kustomize manifests/external-dns --enable-helm | kubectl delete -f -\nkubectl delete secret -n networking external-dns-godaddy\n\n# Get services deployed\nkubectl get pods,services -n networking\n\n# Deploy nginx (it will create a load balancer)\nkubectl apply -f manifests/nginx\nkubectl delete -f manifests/nginx\n\n#\u00a0Get logs for external-dns\nkubectl logs -n networking -l app=external-dns\n\n# Cloudflare\n\nkubectl kustomize clusters/local/addons/networking/cloudflare-external-dns --enable-helm | kubectl apply -f -\nkubectl kustomize clusters/remote/addons/networking/cloudflare-external-dns --enable-helm | kubectl apply -f -\n\n# Test\nkubectl kustomize kubernetes/addons/networking/cloudflare-external-dns --enable-helm &gt; test.yaml\n\n#########################\n# Dynamic DNS (DDNS)\n#########################\n\n# DNS (Domain Name System) Record Types\n# A: A stands for Address (IP Address). A records is used to resolve a hostname which corresponds to an IPv4 address\n# AAAA: Is equivalent to A record but is AAAA records are used to resolve a domain name which corresponds to an IPv6 address.\n#\u00a0CNAME: Stand for Canonical Name. This record routes the traffic to another domain record, it can be to a A, AAAA, CNAME, etc.. buy it cannot by an IP Address. The CNAME record maps a name to another name. It should only be used when there are no other records on that name. This introduces a performance penalty since at least one additional DNS lookup must be performed to resolve the target (lb.example.net).\n# ALIAS: Is similar to CNAME but if differs a bit. The ALIAS record maps a name to another name, but can coexist with other records on that name. The server is the one that perform the lookups so it has better performance that CNAME, but it has it onw drawbacks, for example it loses geo-targeting information.\n# NS: Stands for Name Server records. NS records tell the Internet where to go to find out a domain's IP address. If you have the domain purchased in A provider and the DNS are hosted by B provider, in order to resolve the domain and subdomain, you will need to tell to A provider where to lookup, so you need to configure your NS records into the domain provider to point to B provider, with the DNS (authoritative) server.\n# SOA: This record (Start of Authority Record) indicates who is responsible for that domain. It contains administrative information about the zone, especially regarding zone transfers, etc..\n# TXT: The DNS 'text' (TXT) record lets a domain administrator enter text into the Domain Name System (DNS). The TXT record was originally intended as a place for human-readable notes. However, now it is also possible to put some machine-readable data into TXT records. One domain can have many TXT records. Today, two of the most important uses for DNS TXT records are email spam prevention and domain ownership verification, although TXT records were not designed for these uses originally (SSL Certificates challenges).\n\n# Type      Name  (host     Value (Points To)\n# A         @               188.26.209.56       # https://www.myip.com/ https://dnschecker.org/\n# CNAME     www             javiersant.com       # Optional\n# CNAME     traefik         javiersant.com       #\u00a0Two lookups to resolve the DNS: traefik.javiersant.com -&gt; javiersant.com -&gt; 188.26.209.56\n\n# NOTE: \"@\" is short way to specify the same domain @ == javiersant.com\n\n#\u00a0Home servers or AWS Free Tier EC2 instances generally have dynamic IPv4 address. IP address keep changing when we restart our server or automatically after sometimes. Since, it's not easy to update the DNS record in GoDaddy manually every time IPv4 address changes.\n#\u00a0https://github.com/navilg/godaddy-ddns\n# https://hub.docker.com/r/linuxshots/godaddy-ddns\n\n# Deploy godaddy-ddns\nkubectl apply -k manifests/godaddy-ddns\n\n# Overlay\nkubectl kustomize clusters/remote/addons/networking/godaddy-ddns --enable-helm | kubectl apply -f -\n\n#\u00a0NOTE: Already created using external-secrets\nkubectl create secret -n networking generic godaddy-ddns \\\n    --from-literal=GD_KEY=$GODADDY_API_KEY \\\n    --from-literal=GD_SECRET=$GODADDY_SECRET_KEY\n\n# Remove godaddy-ddns\nkubectl delete -k manifests/godaddy-ddns\nkubectl delete secret -n networking godaddy-ddns\n\n# Get services deployed\nkubectl get pods,services,cm,secret -n networking\n\n# Get the logs from godaddy-ddns\nkubectl logs -n networking -l app=godaddy-ddns -f\n\n# Cloudfare ddns\n# https://github.com/favonia/cloudflare-ddns\n\nkubectl create namespace networking\nkubectl kustomize clusters/local/addons/networking/cloudflare-ddns --enable-helm | kubectl apply -f -\nkubectl kustomize clusters/remote/addons/networking/cloudflare-ddns --enable-helm | kubectl apply -f -\n\nkubectl kustomize kubernetes/addons/networking/cloudflare-ddns --enable-helm | kubectl apply -f -\n\n#########################\n# Deploy Traefik\n#########################\n\n# Deploy traeifk\nkubectl kustomize kubernetes/addons/networking/traefik-external --enable-helm | kubectl apply -f -\n\n# Overlay\nkubectl kustomize clusters/remote/addons/networking/traefik-external --enable-helm | kubectl apply -f -\nkubectl kustomize clusters/local/addons/networking/traefik-external --enable-helm | kubectl apply -f -\n\n# Remove traeifk\nkubectl kustomize kubernetes/addons/networking/traefik-external --enable-helm | kubectl delete -f -\n\n# Get services deployed\nkubectl get pods,services -n networking\n\n# Debug\nhelm template traefik-external /Users/jsantosa/Projects/Github/Mini-Cluster-Setup/manifests/traefik-external/charts/traefik --namespace networking -f ./manifests/traefik-external/values.yaml  --include-crds\n\n# Host file sudo 'code /etc/hosts'\n# 192.168.205.200 traefik.local.example.com grafana.local.example.com prometheus.local.example.com\n#\u00a0192.168.205.200 traefik.javiersant.com grafana.javiersant.com prometheus.javiersant.com\n\n#\u00a0https://traefik.javiersant.com/\n\n# Traefik Base\nkubectl kustomize kubernetes/addons/networking/traefik-external --enable-helm | grep loadBalancerIP\n# Traefik Overlay\nkubectl kustomize clusters/local/addons/networking/traefik-external --enable-helm | grep loadBalancerIP\n\n###########################################################################\n# Deploy Prometheus Stack ( Prometheus Operator + Prometheus + Grafana )\n###########################################################################\n\n# How to Fix \"Too long: must have at most 262144 bytes\":\n# This is because whenever you use kubectl `apply` to create/update resources a `metadata.annotation` is added called `kubectl.kubernetes.io/last-applied-configuration` which is a JSON document that records the last-applied-configuration. Typically this is no issue but in rare circumstances (like large CRDs) it can cause a problem with the apparent size constraint: https://medium.com/pareture/kubectl-install-crd-failed-annotations-too-long-2ebc91b40c7d\n# - A workaround for this issue is to use the more imperative kubectl create to create CRDs and kubectl replace to update the CRDs\u2014 they do not add this field. Of course you can use apply to create the CRDs and create / update to target the one CRD which is too long.\n# - If you are using Kubernetes v1.22 then you can use Server-Side Apply. This is a declarative method which enables different managers to apply different parts of a workload configuration with partial configurations and doesn't use the annotation which is used by Client-Side Apply. https://foxutech.medium.com/how-to-fix-too-long-must-have-at-most-262144-bytes-in-argocd-2a00cddbbe99\n\n# Create Chart\nkubectl kustomize manifests/prometheus --enable-helm | kubectl create -f -                # Use create instead apply to avoid error,\nkubectl kustomize manifests/prometheus --enable-helm | kubectl apply --server-side -f -   # Another workaround (server-side apply)\n\n# Overlays\nkubectl create namespace observability\nkubectl kustomize clusters/remote/addons/observability/prometheus --enable-helm | kubectl create -f -\nkubectl kustomize clusters/remote/addons/observability/prometheus --enable-helm | kubectl apply --server-side -f -\n\nkubectl kustomize clusters/remote/addons/observability/prometheus --enable-helm | kubectl delete -f -\n\n# Remove Chart\nkubectl kustomize manifests/prometheus --enable-helm | kubectl delete -f -\n\n# Get all pods\nkubectl get -n monitoring pods -w\nkubectl get -n monitoring pods,services\nkubectl edit -n monitoring service prometheus-kube-prometheus-prometheus  #\u00a0Use loadbalancer or traefik instead (Push from Ceph)\n\n# Deploy MicroCeph monitoring\nkubectl apply -k manifests/microceph\n\n# Remove MicroCeph monitoring\nkubectl delete -k manifests/microceph\n\n# Get user and password\necho \"Username: $(kubectl get -n monitoring secrets kube-prometheus-stack-grafana -o=jsonpath='{.data.admin-user}' | base64 --decode)\"\necho \"Password: $(kubectl get -n monitoring secrets kube-prometheus-stack-grafana -o=jsonpath='{.data.admin-password}' | base64 --decode)\"\n\n# http://localhost:8080 (admin, prom-operator) (2842)\nkubectl port-forward -n monitoring svc/prometheus-stack-grafana 8080:80\n\n# http://localhost:9090\nkubectl port-forward -n monitoring svc/prometheus-stack-kube-prom-prometheus 9090:9090\n\n#\u00a0https://prometheus.javiersant.com\n#\u00a0https://grafana.javiersant.com\n\n#########################\n# Deploy Reflector\n#########################\n\n# Deploy reflector\nkubectl kustomize manifests/reflector --enable-helm | kubectl apply -f -\n\n# Overlay\nkubectl kustomize clusters/remote/addons/kube-system/reflector --enable-helm | kubectl apply -f -\n\n# Remove reflector\nkubectl kustomize manifests/reflector --enable-helm | kubectl delete -f -\n\n# Get services deployed\nkubectl get pods,services -n kube-system\n\n#########################\n# Deploy Cert Manager\n#########################\n\n#\u00a0ACME (Automated Certificate Management Environment) is a standard protocol for automated domain validation and installation of X.509 certificates. Http01 and dns01 ACME challenge methods allows to create certificates automatically, dns support to create wildcard certificates. The DNS-01 challenge is more difficult to automate than HTTP-01, requiring that your DNS provider supply an API for managing your DNS records, since it need to create a TXT record to validate the certificate in the challenge.\n# https://www.ssl.com/faqs/which-acme-challenge-type-should-i-use-http-01-or-dns-01/\n# There are several circumstances where you might choose DNS-01 over HTTP-01:\n#  - If your domain has more that one web server, you will not have to manage challenge files on multiple servers.\n#  - DNS-01 can be used even if port 80 is blocked on your web server.\n\n# Beside the default providers supported by cert-manager, there is a list of Webhook providers supported by the community:\n# All DNS01 providers will contain their own specific configuration however all require a 'groupName' and 'solverName' field.\n# https://github.com/topics/cert-manager-webhook\n\n# Deploy cert-manager\nkubectl kustomize kubernetes/addons/kube-system/cert-manager --enable-helm | kubectl apply -f -\nkubectl kustomize kubernetes/addons/kube-system/cert-manager/webhooks --enable-helm | kubectl apply -f -\nkubectl kustomize kubernetes/addons/kube-system/cert-manager/certs/staging --enable-helm | kubectl apply -f -\n\n# Overlay\nkubectl kustomize clusters/remote/addons/kube-system/cert-manager --enable-helm | kubectl apply -f -\n\n#\u00a0NOTE: Already created using external-secrets\nkubectl create secret -n cert-manager generic godaddy-api-key \\\n    --from-literal=token=$GODADDY_API_KEY:$GODADDY_SECRET_KEY\n\n# Remove cert-manager\nkubectl kustomize kubernetes/addons/kube-system/cert-manager --enable-helm | kubectl delete -f -\nkubectl kustomize kubernetes/addons/kube-system/cert-manager/webhooks --enable-helm | kubectl delete -f -\nkubectl kustomize kubernetes/addons/kube-system/cert-manager/certs/staging --enable-helm | kubectl delete -f -\nkubectl delete secret -n kube-system godaddy-api-key\n\n# Get services deployed\nkubectl get pods,services,secrets,configmap -n kube-system\n\n# Get challenges\nkubectl get challenges,Certificates -n kube-system\n\n# Get the logs from cert-manager\nkubectl logs -n kube-system -l app=cert-manager -f\n\n\n# Cloud flare\nkubectl kustomize clusters/local/addons/kube-system/cloudflare-cert-manager --enable-helm | kubectl apply -f -\n\n#########################\n# Deploy Longhorn\n#########################\n\n##\u00a0Check if system is compatible with longhorn\n## curl -sSfL https://raw.githubusercontent.com/longhorn/longhorn/master/scripts/environment_check.sh | bash\n\n# Deploy longhorn\nkubectl create namespace storage\nkubectl kustomize clusters/remote/addons/storage/longhorn --enable-helm | kubectl apply -f -\n\n# Remove longhorn\nkubectl kustomize manifests/longhorn --enable-helm | kubectl delete -f -\n\n# Get services deployed\nkubectl get pods,services,storageclass -n storage\n\n# Go to https://longhorn.javiersant.com\n\n# Use Port-forwarding http://localhost:8080\nkubectl port-forward svc/longhorn-frontend -n storage 8080:80\n\n# Longhorn folder /var/lib/longhorn\n\n#########################\n# Deploy Cloud Native PG\n#########################\n\n# https://awslabs.github.io/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres\n# https://github.com/awslabs/data-on-eks/tree/main/distributed-databases/cloudnative-postgres/monitoring\n\n# Deploy Cloud Native PG\nkubectl kustomize manifests/cloudnative-pg --enable-helm | kubectl apply -f -\nkubectl apply -k manifests/postgres\n\n#\u00a0Create remote\nkubectl create namespace database\nkubectl kustomize clusters/remote/addons/database/cloudnative-pg --enable-helm | kubectl apply -f -\nkubectl kustomize clusters/remote/addons/database/postgres --enable-helm | kubectl apply -f -\n\n# Delete Postgres\nkubectl kustomize clusters/remote/addons/database/postgres --enable-helm | kubectl delete -f -\n\n# Remove Cloud Native PG\nkubectl kustomize manifests/cloudnative-pg --enable-helm | kubectl delete -f -\nkubectl delete -k manifests/postgres\n\n# Get clusters\nkubectl get clusters -n database\n\n# NAME               AGE     INSTANCES   READY   STATUS                     PRIMARY\n#\u00a0postgres-cluster   5m11s   1           1       Cluster in healthy state   postgres-cluster-1\n\n# Get services deployed\nkubectl get pods,services -n database\n\n#\u00a0Services to connect to database depending on the purpose: read, read-only, read-write, etc..\n# Usually you would need to connect to read-write service, ie 'service/postgres-cluster-rw'\n# NAME                           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\n# service/postgres-cluster-r     ClusterIP   10.43.135.93   &lt;none&gt;        5432/TCP   6s\n# service/postgres-cluster-ro    ClusterIP   10.43.197.60   &lt;none&gt;        5432/TCP   6s\n# service/postgres-cluster-rw    ClusterIP   10.43.96.202   &lt;none&gt;        5432/TCP   6s\n\n# Port forward to test the connection (postgres/password)\nkubectl port-forward service/postgres-cluster-rw -n database 5432:5432\n\n# Get certificates created by cert-manager\nkubectl get certificates -n database\n\n# Extract certificates created by cert-manager and set into postgres\nkubectl get secret postgres-zitadel-client-cert -n iam -o jsonpath='{.data.tls\\.key}' | base64 -d &gt; ~/Projects/Github/temp/tls.key\nkubectl get secret postgres-zitadel-client-cert -n iam -o jsonpath='{.data.tls\\.crt}' | base64 -d &gt; ~/Projects/Github/temp/tls.crt\nkubectl get secret postgres-zitadel-client-cert -n iam -o jsonpath='{.data.ca\\.crt}' | base64 -d &gt; ~/Projects/Github/temp/ca.crt\n\nkubectl get secret postgres-cluster-superuser-cert -n database -o jsonpath='{.data.tls\\.key}' | base64 -d &gt; ~/Projects/Github/temp/tls.key\nkubectl get secret postgres-cluster-superuser-cert -n database -o jsonpath='{.data.tls\\.crt}' | base64 -d &gt; ~/Projects/Github/temp/tls.crt\nkubectl get secret postgres-cluster-superuser-cert -n database -o jsonpath='{.data.ca\\.crt}' | base64 -d &gt; ~/Projects/Github/temp/ca.crt\n\n# Get all CRD (Custom Resource Definition) created\nkubectl get crds | grep cnpg.io\n\n#########################\n# Deploy Zitadel\n#########################\n\n# https://github.com/zitadel/zitadel/blob/main/cmd/defaults.yaml\n\n# Deploy Zitadel\nkubectl kustomize manifests/zitadel --enable-helm | kubectl apply -f -\n\n# Create Remote\nkubectl create namespace iam\nkubectl kustomize clusters/remote/addons/iam/zitadel --enable-helm | kubectl apply -f -\n\n# Delete Remote\nkubectl kustomize clusters/remote/addons/iam/zitadel --enable-helm | kubectl delete -f -\n\n# Remove Zitadel\nkubectl kustomize manifests/zitadel --enable-helm | kubectl delete -f -\n\n# Get services deployed\nkubectl get pods,services -n iam\nkubectl get secret,configmap,pod -n iam\n\n# It will create a database called Zitadel within postgres cluster\n\n# Get Secret (zitadel-admin-sa.json)\nkubectl get -n iam secrets zitadel-admin-sa -o yaml\nkubectl get -n iam secrets zitadel-admin-sa -o=jsonpath='{.data.zitadel-admin-sa\\.json}' | base64 --decode | jq .\n\n# OpenTofu\n#\u00a0Go to https://zitadel.javiersant.com and download certificate if not Let's Encrypt production.\n# openssl s_client -showcerts -connect zitadel.javiersant.com:443 -servername zitadel.javiersant.com  &lt;/dev/null | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' &gt; javiersant.com.pem\n# sudo security add-trusted-cert -d -r trustAsRoot -k /Library/Keychains/System.keychain javiersant.com.pem\n# rm javiersant.com.pem\n\ncd ~/Projects/Github/Mini-Cluster-Setup/docs/iam/tofu\nkubectl get -n iam secrets zitadel-admin-sa -o=jsonpath='{.data.zitadel-admin-sa\\.json}' | base64 --decode | jq . &gt; ../service-user-jwt/client-key-file.json\nrm terraform.tfs*\ntofu init -upgrade\ntofu apply -auto-approve\n\n# Create the secret after run OpenTofu\nkubectl create secret -n iam generic oauth2-proxy \\\n    --from-literal=client-id=$(tofu output -raw zitadel_application_client_id) \\\n    --from-literal=client-secret=$(tofu output -raw zitadel_application_client_secret) \\\n    --from-literal=cookie-secret=$(LC_ALL=C tr -dc A-Za-z0-9 &lt;/dev/urandom | head -c 32)\n\ncd ~/Projects/Github/Mini-Cluster-Setup\n\n# admin/RootPassword1!\n#\u00a0https://zitadel.javiersant.com\n# https://zitadel.javiersant.com/.well-known/openid-configuration\n\n#########################\n# Deploy OAuth Proxy\n#########################\n\n# https://www.leejohnmartin.co.uk/infrastructure/kubernetes/2022/05/31/traefik-oauth-proxy.html\n# https://joeeey.com/blog/selfhosting-sso-with-traefik-oauth2-proxy-part-2/#why-oauth2-proxy\n# https://zitadel.com/docs/examples/identity-proxy/oauth2-proxy\n\n# Deploy OAuth Proxy\nkubectl kustomize manifests/oauth2-proxy --enable-helm | kubectl apply -f -\n\n# Create Remote\nkubectl kustomize clusters/remote/addons/iam/oauth2-proxy --enable-helm | kubectl apply -f -\n\nkubectl kustomize clusters/remote/addons/iam/oauth2-proxy --enable-helm | kubectl delete -f -\n\n# Remove OAuth Proxy\nkubectl kustomize manifests/oauth2-proxy --enable-helm | kubectl delete -f -\n\n# Get services deployed\nkubectl get pods,services -n iam\n\n# Get logs from Pod\nkubectl logs -n iam -l app=oauth2-proxy\n\n#\u00a0https://oauth.javiersant.com\n\n#########################\n# Deploy Homepage\n#########################\n\n# https://github.com/gethomepage/homepage/blob/main/kubernetes.md\n# https://gethomepage.dev/main/configs/kubernetes/\n\n# Deploy Homepage\nkubectl create namespace home\nkubectl kustomize kubernetes/apps/home/homepage --enable-helm | kubectl apply -f -\n\n# Delete deployment\nkubectl kustomize kubernetes/apps/home/homepage --enable-helm | kubectl delete -f -\n\n# Port forward\nkubectl port-forward svc/homepage -n home 3000:3000\n\n# Exec into a container using deployment or service (randomily pickup the pod)\nkubectl exec -n home -it deployment/homepage -- sh\n\n# Get the configmao\nkubectl get cm -n home homepage-config -o yaml\n\n# Verify the parameters build helm + kustomize\nkubectl kustomize kubernetes/apps/home/homepage --enable-helm &gt; test.yaml\nkubectl kustomize clusters/staging/apps/home/homepage --enable-helm &gt; test.yaml\n\n#########################\n# Deploy Loki\n#########################\n\n# Deploy Loki\nkubectl kustomize manifests/loki --enable-helm | kubectl apply -f -\n\n# Remove Loki\nkubectl kustomize manifests/loki --enable-helm | kubectl delete -f -\n\n# Get services deployed\nkubectl get pods,services -n logging\n\n#########################\n# Rook\n#########################\n\n# Deploy Rook Operator (use apply to ignore if namespace has been already created, be care for the previous error in kubernetes because annotation too long)\nkubectl kustomize manifests/rook-ceph --enable-helm | kubectl apply -f -\n\n# Get limits and quotas configured for the operator\nkubectl -n rook-ceph get configmap -w\nkubectl describe -n rook-ceph cm rook-ceph-operator-config\n\n# Wait until the operator is ready\nkubectl -n rook-ceph get pod -w\nkubectl -n rook-ceph get services\nkubectl -n rook-ceph get cm,secrets\nkubectl top pods -n rook-ceph\n\n# Create the Rook Ceph cluster (external). Secrets from external cluster must be created previously.\nkubectl kustomize manifests/external-ceph-cluster --enable-helm | kubectl apply -f -\n\n# Go to dashboard: https://192.168.205.101:8443\n\n# Create Rook Ceph cluster (managed)\n\n# Tear Down Rook from previous installation\n# https://rook.io/docs/rook/latest-release/Getting-Started/ceph-teardown/\nsudo rm -rf /var/lib/rook\nDISK=\"/dev/mapper/ubuntu--vg-lv1\"\nsudo dd if=/dev/zero of=\"$DISK\" bs=1M count=100 oflag=direct,dsync\nsudo sgdisk --zap-all $DISK\n\n# NOTE: It requires 8GiB and 8CPUs to initialize properly, to check the reservations request and limits per node)\n# k describe node server-2\nkubectl kustomize manifests/rook-ceph-cluster --enable-helm | kubectl apply -f -\n\n# Connect to dashboard (admin/..)\nkubectl port-forward -n rook-ceph svc/rook-ceph-mgr-dashboard 7000:7000\nkubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=\"{['data']['password']}\" | base64 --decode &amp;&amp; echo\n\n# If not using Helm Storage Classes and other resources must be created for Block, NFS and Object Storage support\n# Create a Storage Pools, Storage Classes and COSI (bject Storage Management) for Ceph using Rook.\nkubectl apply -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/csi/rbd/storageclass.yaml\nkubectl apply -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/filesystem.yaml\nkubectl apply -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/csi/cephfs/storageclass.yaml\n\n# Check available StorageClasses created\nkubectl -n rook-ceph get storageclass\n\n# Check if the cluster is Connected\nkubectl get -n rook-ceph CephCluster,CephFilesystem,CephObjectStore\n\n# You can set the default one if set multiple StorageClasses\nkubectl patch storageclass rook-ceph-block -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n\n# Get running pods\nkubectl -n rook-ceph get pod -w\nkubectl -n rook-ceph get secrets,cm\n\n# Get resources from nodes\nkubectl top node\n\n# Get secrets info\nkubectl get -n rook-ceph secret rook-ceph-mon -o yaml\n\n#########################\n# Persistent Volumes\n#########################\n\n#\u00a0Kubernetes Volume Types:\n# - Block Devices (RBD, LOOP, LVM, etc..) -&gt; ReadWriteOnce\n# - FileSystem Data (NFS) -&gt; ReadWriteMany\n# - COSI (Container Object Storage Interface) (S3, GCS,  Minio, etc..)\n\n# kubectl create -k github.com/kubernetes-sigs/container-object-storage-interface-api\n\n# Create Stateful Set\nkubectl apply -f kubernetes/apps/stateful-set\n\n# Check whether the volumes has been created and bind\nkubectl get pod -w\nkubectl get pod,pvc,pv\n\n# Ceph Tools from kubernetes (sudo access)\nCEPH_TOOL_POD=$(kubectl get pod -n rook-ceph -l app=rook-ceph-tools | awk '{print $1}' | tail -n +2)\nkubectl exec -it -n rook-ceph $CEPH_TOOL_POD -- bash\n\n# ceph health mute  POOL_NO_REDUNDANCY\n\n# Copy file to volumes types (restart the pod)\nkubectl cp README.md k8s-summit-demo-0:/data\nkubectl cp README.md k8s-summit-demo-0:/usr/share/nginx/html\n\n# Kill the pod and check the file is there\nkubectl delete pod k8s-summit-demo-0\nkubectl exec -it k8s-summit-demo-0 -- ls /usr/share/nginx/html\n\n# Check the FileSystem to be replicated over other pods\nkubectl exec -it k8s-summit-demo-0 -- ls /data\nkubectl exec -it k8s-summit-demo-1 -- ls /data\nkubectl exec -it k8s-summit-demo-2 -- ls /data\n\n# Get secrets anc configmaps created by the objectbucket resource (cosi)\nkubectl get cm,secrets\n\n# config-map, secret, OBC will part of default if no specific name space mentioned\nexport AWS_HOST=$(kubectl -n default get cm ceph-delete-bucket -o jsonpath='{.data.BUCKET_HOST}')\nexport PORT=$(kubectl -n default get cm ceph-delete-bucket -o jsonpath='{.data.BUCKET_PORT}')\nexport BUCKET_NAME=$(kubectl -n default get cm ceph-delete-bucket -o jsonpath='{.data.BUCKET_NAME}')\nexport AWS_ACCESS_KEY_ID=$(kubectl -n default get secret ceph-delete-bucket -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 --decode)\nexport AWS_SECRET_ACCESS_KEY=$(kubectl -n default get secret ceph-delete-bucket -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 --decode)\n\n#\u00a0Install s5cmd\nbrew install peak/tap/s5cmd\n\n# Upload a file to the newly created bucket\necho \"Hello Rook\" &gt; /tmp/rookObj\ns5cmd --endpoint-url http://$AWS_HOST:$PORT cp /tmp/rookObj s3://$BUCKET_NAME\n\n# Download and verify the file from the bucket\ns5cmd --endpoint-url http://$AWS_HOST:$PORT cp s3://$BUCKET_NAME/rookObj /tmp/rookObj-download\ncat /tmp/rookObj-download\n\n#########################\n# Other\n#########################\n\n# Get all pods\nkubectl get pods -A\nkubectl get pods,services -A -o wide\nkubectl get pods,services -A --show-labels\n\n# Get all namespaces\nkubectl get namespaces\n\n# Crean Failed Pods\nkubectl delete pods --field-selector status.phase=Failed -A\n\n# Test the IP Address metallb has assigned to nginx service (EXTERNAL-IP)\nhttp://192.168.3.200\nhttp://192.168.205.200\n\n# Get Cluster Memory\nkubectl top nodes\nkubectl describe node server-1\nkubectl describe node server-2\nkubectl describe node server-3\n\nkubectl top pods -A\n\n# This project can be installed with Krew:\n# https://krew.sigs.k8s.io/docs/user-guide/setup/install/\nkubectl krew install resource-capacity\n\nkubectl resource-capacity --util\nkubectl resource-capacity --pods --util\n\n#\u00a0Get process running in node (Hold Shift + H to group by application)\nhtop\n</code></pre> <pre><code># Execute DDNS using Goddady API\n\n# For linux running on amd64/arm\ndocker run \\\n    --env GD_NAME=@ \\\n    --env GD_DOMAIN=javiersant.com \\\n    --env GD_TTL=600 \\\n    --env GD_KEY=$GODADDY_API_KEY \\\n    --env GD_SECRET=$GODADDY_SECRET_KEY \\\n    linuxshots/godaddy-ddns:1.1.1\n</code></pre>"},{"location":"tools/manifest/#port-forward-mac","title":"Port Forward Mac","text":"<p>There is a list of required information that we need, to port forward to Mac. These information details include:</p> <ol> <li> <p>The first thing you need to do is find out what your router's IP address is. To do this, go to your router's configuration page and look for the IP address.</p> </li> <li> <p>Open your web browser using the router IP address or router gateway. (i.e <code>http://192.168.3.1</code>)</p> </li> <li>Provide your credentials, username, and password.</li> <li>Go to the port forwarding section from the settings.</li> <li>Enter the IP address, TCP, and UDP in their relevant fields.</li> <li> <p>Now restart the router to make changes effective.</p> </li> <li> <p>The IP address of the device to connect.</p> </li> <li>Port forward:</li> <li>ssh: <code>ssh -L 8443:127.0.0.1:443 ubuntu@192.168.205.200</code></li> <li>pf:</li> </ol> <pre><code># https://medium.com/@ginurx/how-to-set-port-forwarding-for-internet-sharing-on-mac-os-x-using-pf-a7a338f09953\n\n# Check whether is enabled or disabled\n#\u00a0In System Preferences / Security &amp; Privacy / Firewall Options..., check \"Enable stealth mode\" and turn on Firewall.\nsudo pfctl -s info | egrep -i --color=auto 'enabled|disabled'\n\n# Append to /etc/pf.conf\nsudo code /etc/pf.conf\ncat /etc/pf.conf\n\n# Added Port-forward (empty line and the end is required)\nanchor \"org.javstudio/*\"\nload anchor \"org.javstudio\" from \"/etc/pf.anchors/org.javstudio\"\n\n# Create /etc/pf.anchors/org.javstudio\nsudo touch /etc/pf.anchors/org.javstudio\nsudo code /etc/pf.anchors/org.javstudio\ncat /etc/pf.anchors/org.javstudio\n\n# Port forward from localhost to VM\n# # ifconfig -a | awk '/^bridge/ {print; while(getline) { if ($0 ~ /^[ \\t]/) print; else break; }}' | grep -E \"^bridge|inet |status\"\n\n#rdr pass on en0 inet proto tcp from any to any port 8443 -&gt; 192.168.205.200 port 443\nrdr on en0 inet proto tcp to any port 8443 -&gt; 192.168.205.200 port 443\n# rdr pass on en0 inet proto tcp from 192.168.205.200 to any port 443 -&gt; 127.0.0.1 port 8443\n# rdr on bridge100 inet proto tcp from 192.168.205.200 to any port 443 -&gt; 127.0.0.1 port 8443\n\n# Second redirect now incoming traffic to localhost 8080 for all traffic that matches our host and port filter\nrdr on en0 proto tcp from en0 to any port { 80, 443, 8080, 8443 } -&gt; 127.0.0.1 port 8443\n# First route all outgoing traffic from en0 to lo0 that matches our host and port filter and user\npass out on en0 route-to lo0 proto tcp from en0 to &lt;IP to redirect to proxy&gt; port { 80, 443 } keep state user { &lt;user id you are running your browser under&gt; }\n\n# load the new rules\nsudo pfctl -f /etc/pf.conf\n\n# Check the rules\nsudo pfctl -a org.javstudio -sn\n\n#\u00a0Disable org.javstudio anchor\nsudo pfctl -a org.javstudio -F all\n\n# Ensure that IP forwarding is enabled:\nsudo sysctl -w net.inet.ip.forwarding=1\n\n# To verify that IP forwarding is enabled, you can use:\nsudo sysctl -a | grep net.inet.ip.forwarding\n\n# Chek rules loaded\nsudo pfctl -s rules\nsudo pfctl -s References\n</code></pre> <pre><code>NGINX_VERSION=1.12.2\nmkdir -p ./tmp\ncd ./tmp\ncurl -OL http://nginx.org/download/nginx-$NGINX_VERSION.tar.gz\ntar -xvzf nginx-$NGINX_VERSION.tar.gz &amp;&amp; rm nginx-$NGINX_VERSION.tar.gz\nmv nginx-$NGINX_VERSION nginx\n\n# Set Redirect rues\n..\n</code></pre> <pre><code>#\u00a0Install socat\nbrew install socat\n\n# Redirect incoming 8443 traffic to 192.168.205.200:443\nsudo socat TCP-LISTEN:8443,fork TCP:192.168.205.200:443\n\n# 1. Ensure firewall allow for incoming traffic\n# 2. Enable port in Router/firewall (Port Trigger) and use Port Forwarding for the Machine (NAT)\n# 3. Ensure port forwarding is enabled\n#   - Option1: sudo cat /proc/sys/net/ipv4/ip_forward\n#   - Option2: sudo sysctl -a | grep net.inet.ip.forwarding\n# 4. Ensure the Network provider allow you to enable port (https://ping.eu/port-chk/)\n</code></pre>"},{"location":"tools/microceph/","title":"README","text":""},{"location":"tools/microceph/#introduction","title":"Introduction","text":"<p>Hyper-converged infrastructure (HCI) is a software-defined IT infrastructure that <code>virtualizes</code> all of the elements of conventional \"hardware-defined\" systems. HCI includes, at a minimum, virtualized computing (a hypervisor), software-defined storage, and virtualized networking (software-defined networking). HCI typically runs on commercial off-the-shelf (COTS) servers. The traditional silos of compute and storage resources can be wrapped up into a single hyper-converged appliance. Separate storage networks (SANs) and connections via network attached storage (NAS) disappear.</p> <p>There has been discussion around having suitable storage solution for Kubernetes and this discussion has always centred on the need that storage should be run as its own service, to be consumed by deployments, rather than built as part of them using native components. Although we have a few different storage services available, but may really not be suitable for what we want to achieve:</p> <ul> <li>Cloud Storage - perfect for object storage, but not particularly performant and will usually require application refactoring.</li> <li>Cloud Filestore - expensive but reliable and performant managed NFS solution.</li> <li>Roll-your-own in Compute - the last resort of a desperate system admin. One can get lost for days trying to string a <code>GlusterFS</code> cluster together and then comes the headache when there's need to change it.</li> <li>Vendor lock-in - some organizations that prefer to run their own self-manage Kubernetes cluster on-premises may not want to be locked into any cloud vendors and still wants a high-performing, highly scalable distributed storage system with no single point of failure.</li> </ul>"},{"location":"tools/microceph/#rook-operator","title":"Rook Operator","text":"<p>This is an operator and orchestrator for Kubernetes that automates the provisioning, configuration, scaling, migration and disaster recovery of storage. Rook supports several backend providers (such <code>ceph</code>, <code>cassandra</code>, etc.) and uses a consistent common framework across all of them. The Ceph provider for Rook is stable and production ready.</p> <p>Rook consists of multiple components:</p> <ul> <li>Rook Operator is the core of Rook. The Rook operator is a simple container that automatically bootstraps the storage clusters and monitors the storage daemons to ensure the storage clusters are healthy.</li> <li>Rook Agents run on each storage node and configure a FlexVolume plugin that integrates with Kubernetes' volume controller framework. Agents handle all storage operations such as attaching network storage devices, mounting volumes on the host, and formatting the filesystem.</li> <li>Rook Discovers detect storage devices attached to the storage node.</li> </ul> <p>Rook also deploys <code>MON</code>, <code>OSD</code> and <code>MGR</code> daemons for the Ceph clusters as Kubernetes pods.</p> <p>The Rook Operator enables you to create and manage your storage clusters through CRDs. Each type of resource has its own CRD defined.</p> <ul> <li>A Rook Cluster provides the settings of the storage cluster to serve block, object stores, and shared file systems. Each cluster has multiple pools.</li> <li>A Pool manages the backing store for a block store. Pools are also used internally by object and file stores.</li> <li>An Object Store exposes storage with an S3-compatible interface.</li> <li>A File System provides shared storage for multiple Kubernetes pods.</li> <li>Rook Ceph provide <code>RWO</code> (CephRBD) and <code>RWX</code> (CephFS) volumes types. Also is support Object Storage via RADOS engine.</li> </ul>"},{"location":"tools/microceph/#ceph","title":"Ceph","text":"<p>Ceph is an open-source project that provides massively scalable, software-defined storage systems on commodity hardware. It can provide object, block or file system storage, and automatically distributes and replicates data across multiple storage nodes to guarantee no single point of failure.</p> <p>At its heart, Ceph is a distributed storage system engineered to carry out massive data management tasks, which it accomplishes through the use of the RADOS (Reliable Autonomic Distributed Object Store) system. Its architecture is designed to distribute data across various machines in a scalable fashion.</p> <p></p> <p>Ceph consists of multiple components:</p> <ul> <li>Ceph Monitors (MON) are responsible for forming cluster quorums. All the cluster nodes report to monitor nodes and share information about every change in their state.</li> <li>Ceph Object Store Devices (OSD) are responsible for storing objects on local file systems and providing access to them over the network. Usually, one OSD daemon is tied to one physical disk in your cluster. Ceph clients interact with OSDs directly.</li> <li>Ceph Manager (MGR) provides additional monitoring and interfaces to external monitoring and management systems.</li> <li>Reliable Autonomic Distributed Object Stores (RADOS) are at the core of Ceph storage clusters. This layer makes sure that stored data always remains consistent and performs data replication, failure detection, and recovery among others.</li> </ul> <p>To read/write data from/to a Ceph cluster, a client will first contact Ceph MONs to obtain the most recent copy of their cluster map. The cluster map contains the cluster topology as well as the data storage locations. Ceph clients use the cluster map to figure out which OSD to interact with and initiate a connection with the associated OSD.</p> <p>Files stored in the disk (<code>images</code>) are converted into several objects. These objects are then distributed into <code>placement groups</code> (<code>pg</code>) which are put into <code>pools</code>. A <code>pool</code> has some properties configured as how many <code>replicas</code> of a <code>pg</code> will be stored in the cluster (3 by default). Those <code>pg</code> will finally be physically stored into an <code>Object Storage Daemon</code> (OSD). An <code>OSD</code> stores <code>pg</code> (and so the objects within it) and provides access to them over the network.</p> <p></p> <p>There are two techniques in order to maintain the data spread for all the cluster, in order to ensure <code>high availability</code> and <code>disaster recovery</code>.</p> <ul> <li>replication: this allows the OSD (disks) to be replicated over some other Ceph instances within the same cluster. This can be replication within the same Availability Zone but with separation of rack, netwrking, power suppliers, etc..</li> <li>mirroring: this allows the OSD (disks) to be replicated over some other Ceph instances within different clusters. This can be different region for backups cluster for disaster Recove, so depending on metrics RTO or RPO decice the strategy to follow: BACKUP/Restore, Pilot Light, Warmup StandBy or Active/Passive, Active-Active.</li> <li>Snapshot: Since an OSD is created by mounting a device (loop, partition, lvm, disk, etc..) you can take an snapshot or backup directly. Using Kubernetes and Rook, there is a native resource from Kubernetes (<code>snapshot.storage.k8s.io/v1</code>) that allows to create backups of the persistent volumes in order to perform maintenance task of other two methods fail.</li> </ul>"},{"location":"tools/microceph/#rwo-vs-rwx","title":"RWO vs RWX","text":"<p>Both volume types are supported by Kubernetes and Container Storage Interface (CSI), that is implemented by Ceph Rook.</p> <p>Basically <code>RWO</code> is <code>ReadWriteOnce</code>, that means only one Pod can Read or Write to that particular volume. However <code>RWX</code> is <code>ReadWriteMany</code>, that means a volume can be shared among multiple Pods.</p> <p>RWO vs RWX: An important difference is that the filesystem for <code>RWO</code> is <code>ext4</code> and for <code>RWX</code> is <code>nfs4</code>. As the <code>RWX</code> is using a network file storage (NFS), there will be a lot of overhead, which is only necessary to allow for such a remote storage.</p> <p>Following are some conclusions for standard benchmarks:</p> <ul> <li>RWX local vs remote, does not make a difference. Likely because the same calls are being made regardless.</li> <li>RWO local vs remote, for large and medium sized blocks there is a difference up to 30%, but no difference when the files get small.</li> <li>RWO vs RWX: Significant Difference of 50% or more in the duration! IOPS and Bandwidth are even up to 250% faster (might be too artificial, so not soo relevant). Difference of remote and local is as expected when using the data of the left plots.</li> </ul>"},{"location":"tools/microceph/#microceph","title":"MicroCeph","text":"<p>MicroCeph is the easiest way to get up and running with Ceph.</p> <p>MicroCeph is a lightweight way of deploying and managing a Ceph cluster. Ceph is a highly scalable, open-source distributed storage system designed to provide excellent performance, reliability, and flexibility for object, block, and file-level storage.</p> <p>Ceph cluster management is streamlined by simplifying key distribution, service placement, and disk administration for quick, effortless deployment and operations. This applies to clusters that span private clouds, edge clouds, as well as home labs and single workstations.</p> <p>MicroCeph is focused on providing a modern deployment and management experience to Ceph administrators and storage software developers.</p> <p></p> <p>The microceph snap packages all the required ceph-binaries, dqlite and a small management daemon (microcephd) which ties all of this together. Using the light-weight distributed dqlite layer, MicroCeph enables orchestration of a ceph cluster in a centralised and easy to use manner.</p>"},{"location":"tools/microceph/#installation","title":"Installation","text":"<pre><code># Connect to the Servers\nssh ubuntu@localhost -p 50031\n\n # Install MicroCeph in servers\nsudo snap install microceph --channel latest/stable\nsudo snap connect microceph:hardware-observe\nsudo snap connect microceph:dm-crypt\nsudo snap refresh --hold microceph\n\n# Check status (same outputs)\nsudo microceph.ceph status\nsudo ceph status\n\n# Configure MicroCeph and add nodes\n\n#sudo microceph init # It Works because the IP can be changed to use different network interface\n#sudo microceph cluster bootstrap # https://github.com/canonical/microceph/issues/197\n#sudo microceph cluster bootstrap --mon-ip 192.168.205.101 --public-network 192.168.205.101/24 --cluster-network 192.168.205.101/24\n\n# Master Node\nSERVER_IP=192.168.205.101\nSERVER_MASTER=yes\nSERVER_NAME=server-1\nADD_SERVER=no\nADD_DISK=no\nprintf \"$SERVER_IP\\n$SERVER_MASTER\\n$SERVER_NAME\\n$ADD_SERVER\\n$ADD_DISK\\n\" | sudo microceph init\n\n# Check the cluster status\nsudo ceph status\nsudo microceph cluster list\n\n# Add Nodes\nsudo microceph cluster add server-2\nsudo microceph cluster add server-3\n\n# Join Nodes\n\n# Node 1\nSERVER_IP=192.168.205.102\nSERVER_MASTER=no\nSERVER_TOKEN=eyJuYW1lIjoic2VydmVyLTIiLCJzZWNyZXQiOiJjYjQ5ZDcwZmQ1MzRiZmNiOTg2Y2U3NjBlMWE4NmQwNGNmN2JiMGQ5MDhiZTdjMzEwNWY0ZmYzYTg3MmVkMjg5IiwiZmluZ2VycHJpbnQiOiI0YTcwMGRjYmFhYWZkZTJhN2YyMzFiM2MxMzU1MWRhZTg5MGM4ZTZkN2YzYjBmNzhkOGUyZjM1NzlmNWYwNmE4Iiwiam9pbl9hZGRyZXNzZXMiOlsiMTkyLjE2OC4yMDUuMTAxOjc0NDMiXX0=\nADD_DISK=no\nprintf \"$SERVER_IP\\n$SERVER_MASTER\\n$SERVER_TOKEN\\n$ADD_DISK\\n\" | sudo microceph init\n\n# Node 2\nSERVER_IP=192.168.205.103\nSERVER_MASTER=no\nSERVER_TOKEN=eyJuYW1lIjoic2VydmVyLTMiLCJzZWNyZXQiOiIxOTBlMWM3YjhiNzA5YzA5ZjhmZGQzYWQxMDE2NzZiZmMyMzNiYTc3Yzc2Y2E5YjQzNjM2MzExZDYzMGZmNzRkIiwiZmluZ2VycHJpbnQiOiI0YTcwMGRjYmFhYWZkZTJhN2YyMzFiM2MxMzU1MWRhZTg5MGM4ZTZkN2YzYjBmNzhkOGUyZjM1NzlmNWYwNmE4Iiwiam9pbl9hZGRyZXNzZXMiOlsiMTkyLjE2OC4yMDUuMTAxOjc0NDMiXX0=\nADD_DISK=no\nprintf \"$SERVER_IP\\n$SERVER_MASTER\\n$SERVER_TOKEN\\n$ADD_DISK\\n\" | sudo microceph init\n\n# Check the status\nsudo ceph status\nsudo microceph cluster list\n# The status of the cluster must be HEALTH_OK\n\n# Enable RADOS Gateway (Object Storage)\nsudo microceph enable rgw\n# Create Client Key and Client Secret\nsudo radosgw-admin user create --uid=myuser --display-name=myuser\n\n# Enable Dashboard https://github.com/UtkarshBhatthere/microceph/blob/2cb0efaf57b5dd7d8328b2ff2891460b0da16125/docs/tutorial/enable_dashboard.rst\nsudo ceph mgr module enable dashboard\nsudo ceph dashboard create-self-signed-cert\nsudo ceph mgr services\nsudo microceph.ceph mgr module ls\n\necho \"ubuntu1234\" &gt; ceph-pass.txt\nsudo cp ~/ceph-pass.txt /root/  #\u00a0Workaround for permissions\nsudo ceph dashboard ac-user-create ubuntu -i /root/ceph-pass.txt administrator\n\n# (ubuntu/ubuntu1234)\n# http#s://192.168.205.101:8443/#/login?returnUrl=%2Fdashboard\n\n# Enable Prometheus in Ceph\nsudo ceph mgr module enable prometheus\nsudo ceph config set mgr mgr/prometheus/server_addr 192.168.205.101\nsudo ceph config set mgr mgr/prometheus/server_port 30436\nsudo ceph mgr services\n\n#{\n#    \"dashboard\": \"https://192.168.205.101:8443/\",\n#    \"prometheus\": \"http://192.168.205.101:9283/\"\n#}\n\n# Alerts\nsudo ceph dashboard set-alertmanager-api-host 'http://localhost:9093'\nsudo ceph dashboard set-prometheus-api-host 'http://192.168.205.101:30436' # Through ingress, load balancer or NodePort (not recommended)\nsudo ceph dashboard set-prometheus-api-ssl-verify False\nsudo ceph dashboard set-alertmanager-api-ssl-verify False\n\n# Upgrade MicroCeph (https://github.com/canonical/microceph/blob/main/docs/how-to/reef-upgrade.rst)\nsudo snap refresh microceph --channel reef/stable\n\n# Uninstall MicroCeph\nsudo snap remove microceph\n</code></pre> <p>Main cluster Configuration and commands</p> <pre><code>sudo microceph cluster config list\nsudo microceph cluster config set cluster_network 192.168.205.0/24\nsudo microceph cluster config reset cluster_network\n\nsudo microceph cluster list\nsudo microceph cluster remove server-1\nsudo microceph cluster add server-2\nsudo microceph cluster add server-3\n\necho eyJuYW1lIjoic2VydmVyLTMiLCJzZWNyZXQiOiIzMTM1Y2JmNzE0MTQ1NzYzNDdkNmY2ODllZjJiNWY1MGU5NWI0MjA0MzhhZDQxNTg4MzlmN2RlODhmZTViMTZlIiwiZmluZ2VycHJpbnQiOiI5NWM4MjhjN2E2NGNjZDYzYWM3OWNjZGFkNWVkNmFmMjQzZDFkMmUzODZjMGE2NzYyMDk3OTA2ZjM0NmY5YmU5Iiwiam9pbl9hZGRyZXNzZXMiOlsiMTkyLjE2OC4yMDUuMTAxOjc0NDMiLCIxMC4wLjIuMTU6NzQ0MyJdfQ== | base64 -d | jq .\n\nsudo microceph cluster join  eyJuYW1lIjoic2VydmVyLTMiLCJzZWNyZXQiOiIzMTM1Y2JmNzE0MTQ1NzYzNDdkNmY2ODllZjJiNWY1MGU5NWI0MjA0MzhhZDQxNTg4MzlmN2RlODhmZTViMTZlIiwiZmluZ2VycHJpbnQiOiI5NWM4MjhjN2E2NGNjZDYzYWM3OWNjZGFkNWVkNmFmMjQzZDFkMmUzODZjMGE2NzYyMDk3OTA2ZjM0NmY5YmU5Iiwiam9pbl9hZGRyZXNzZXMiOlsiMTkyLjE2OC4yMDUuMTAxOjc0NDMiLCIxMC4wLjIuMTU6NzQ0MyJdfQ==\n</code></pre> <p>Basic Ceph configuration commands.</p> <pre><code>sudo microceph.ceph status\nsudo ceph status\n\nsudo microceph.ceph config dump\nsudo microceph.ceph mon dump\nsudo microceph.ceph osd dump\n\nsudo ceph osd lspools\nsudo ceph osd tree\n\ncat /var/snap/microceph/current/conf/ceph.conf\ncat /var/snap/microceph/current/conf/keyring.conf\nsudo ss -tlnp | grep mon\n</code></pre> <p>Create a complex Ceph configuration, use an BD pool backed by specific OSDs (e.g. tenant separation, use only SSD disks, ...)</p> <pre><code># Create Crush Rule\nsudo ceph osd crush rule create-replicated my_replicated_ssd_rule default host ssd\n\n#\u00a0Create a new pool\nsudo rbd pool create my-ssd-pool\n\n# Set The Crash Rule to the pool\nsudo ceph osd pool set my-ssd-pool crush_rule my_replicated_ssd_rule\n\n# Finally, configure rook-ceph CRDs to use the new created `my-ssd-pool`with custom rules\n</code></pre> <p>Create RDB (Rados Data Block) Pool</p> <pre><code># https://docs.ceph.com/en/latest/rados/operations/pools/\n# Pool names beginning with . are reserved for use by Ceph's internal operations. Do not create or manipulate pools with these names.\n\nsudo ceph osd pool create default.pool replicated\nsudo ceph osd pool set default.pool size 3\nsudo ceph osd pool set default.pool min_size 2\nsudo ceph osd pool set default.pool pg_autoscale_mode on\nsudo sudo rbd pool init default.pool\n\n\n# Get list pools\nsudo ceph osd pool ls\nsudo ceph osd pool ls detail --format json-pretty\n\n#\u00a0Pool types\n#  The replicated pools require more raw storage but can implement all Ceph operations.\n#  The erasure pools require less raw storage but can perform only some Ceph tasks and may provide decreased performance.\n\n# Replicated Block\nsudo ceph osd pool create {pool-name} [{pg-num} [{pgp-num}]] [replicated] [crush-rule-name] [expected-num-objects]\n\n# Erasure Block (Better performance for anti-corruption)\nsudo ceph osd pool create {pool-name} [{pg-num} [{pgp-num}]] [erasure] \\\n         [erasure-code-profile] [crush-rule-name] [expected_num_objects] [--autoscale-mode=&lt;on,off,warn&gt;]\n</code></pre> <p>CephFS for RWX support</p> <pre><code># https://docs.ceph.com/en/quincy/cephfs/createfs/\nsudo ceph osd pool create default.cephfs.data\nsudo ceph osd pool create default.cephfs.metadata\nsudo ceph fs new default.cephfs default.cephfs.metadata default.cephfs.data\nsudo ceph fs set default.cephfs allow_standby_replay 1\n\n# Get application from CephFs\nsudo ceph osd pool application get cephfs_data cephfs\nsudo ceph osd pool application get cephfs_metadata cephfs\n\n# erasure Pool https://docs.ceph.com/en/latest/rados/operations/erasure-code/\nsudo ceph osd pool create ecpool erasure\nsudo ceph osd pool set cephfs_data allow_ec_overwrites true\n\n#\u00a0Create FS volume\nsudo ceph fs volume create mmy-cephfso\n\n#\u00a0Get the status from CephFS\nsudo ceph fs status\n\n# Configure Rook-Ceph\n</code></pre> <p>Add loop devices to Cepg</p> <pre><code># Three OSDs will be required to form a minimal Ceph cluster. In a production system, typically we would assign a physical block device to an OSD.\n#  This means that, on each of the three machines, one entire disk must be allocated for storage or in single installation three OSD in the same machine\n# This will create a file backed OSDs.\nsudo microceph disk add loop,4G,1\n\n#\u00a0Repeat the same in other cluster for redundancy and replication\nsudo microceph disk list\n</code></pre> <p>The following loop creates three files under /mnt that will back respective loop devices. Each Virtual disk is then added as an OSD to Ceph, but the need to be added at /etc/fstab in order the be recognized at startup.</p> <pre><code># Create loop device at /mnt folder\nsudo mktemp -p /mnt XXXX.img # /mnt/IICQ.img\nsudo truncate -s 1G \"IICQ.img\" #\u00a0Expand to 1Gb\nsudo losetup --show -f IICQ.img # /dev/loop3\n# the block-devices plug doesn't allow accessing '/dev/loopX' devices so we make those same devices available under alternate names '/dev/sdiY'\nsudo mknod -m 0660 \"/dev/sdia\" b 7 3 # number 3 is X from /dev/loopX\n\n# Add device to Ceph\n# For each Disk Ceph will create an OSD\n# In total it will create 9 OSD and 3 disk per server\nsudo microceph disk add --wipe \"/dev/sdi${l}\"\n\n# List Block Devices\nlsblk\nlosetup -a  #\u00a0 list loop devices\nsudo microceph disk list\nsudo ceph mgr services\n\n# Detach from loop devices and remove from /dev\nsudo microceph disk remove /dev/sdia\nsudo microceph disk remove /dev/sdib\nsudo losetup -d /dev/loop3\nsudo losetup -d /dev/loop4\nsudo rm -rf /dev/sdia\nsudo rm -rf /dev/sdib\n</code></pre> <p>Following script creates three disks on each server of Ceph cluster</p> <pre><code>############\n# Script\n############\n# Script to create 1Gb loop devices: /dev/sdia,/dev/sdib,/dev/sdic\nfor l in a b c; do\n  loop_file=\"$(sudo mktemp -p /mnt XXXX.img)\"\n  sudo truncate -s 1G \"${loop_file}\"\n  loop_dev=\"$(sudo losetup --show -f \"${loop_file}\")\"\n  # the block-devices plug doesn't allow accessing /dev/loopX\n  # devices so we make those same devices available under alternate\n  # names (/dev/sdiY)\n  minor=\"${loop_dev##/dev/loop}\"\n  sudo mknod -m 0660 \"/dev/sdi${l}\" b 7 \"${minor}\"\n  sudo microceph disk add --wipe \"/dev/sdi${l}\" # --encrypt\n  # to be permanent add the device into /etc/fstab\n  # /path/to/file       /path/to/mount       ext4       loop       0 0\ndone\n\n# Get disks\nsudo microceph disk list\nsudo ceph mgr services\n\ndf\n</code></pre> <p>Test the replication</p> <pre><code># Mount of the loop devices\nsudo mount /dev/sdia /mnt/ &amp;&amp; sudo sync\n\n#\u00a0Unmount device\nsudo umount /mnt/  &amp;&amp; sudo sync\n</code></pre>"},{"location":"tools/microceph/#notes","title":"Notes","text":"<p>[https://discuss.linuxcontainers.org/t/microceph-vs-cephadm-microceph-partition-fix/18976]</p> <p>I've been experimenting with microceph &amp; more briefly with cephadm (with services running in podman) on a 5 x node \"low end\" cluster - these details are probably more interesting for people with nodes having single disks who still want to run ceph. TLDR - I'm going to be using microceph for various reasons:</p> <ul> <li>For your cluster you want at least 2 x NIC's ideally with a 10gb or greater internal network for ceph</li> <li>5 x node clusters will have better performance 1 than 3 x node clusters.</li> <li>microceph uses around 1.5gb of RAM on monitor nodes &amp; 500mb on the other nodes</li> <li>microceph clusters reboot much faster (less than 10 seconds on nvme) - than cephadm nodes (the services in podman took much longer to stop - 30 to 60 * seconds or so)</li> <li>cephadm uses a lot more disk (as it's running a complete Centos 8 stream system in each container) - RAM usage was similar to microceph</li> <li>cephadm will not install directly onto partitions - it requires lvm lv's</li> <li>install podman then apt install cephadm --no-install-recommends (to avoid pulling in docker) - ubuntu 23.10 gives you podman v4 from the official repos * now. You probably want to use docker with cephadm - my cephadm cluster never became healthy (mgr kept crashing - possibly due to using podman ?)</li> <li>Rather than going by the microceph docs - use microceph init to initialise every node so you can choose the specific ip address to run ceph on (otherwise * by default it will use your public interfaces)</li> </ul>"},{"location":"tools/microceph/#rook","title":"Rook","text":"<pre><code># https://www.mrajanna.com/setup-external-ceph-with-rook/\n# https://medium.com/techlogs/configuring-rook-with-external-ceph-6b4b49626112\n# https://blog.mymware.com/2022/11/02/rook-ceph-survguide.html\n# https://www.youtube.com/watch?v=-7hpXFs7au8\n\n# Connect to the Servers\nssh ubuntu@localhost -p 50031\n\n##################################\n# Automatic\n##################################\n\nsudo microceph.ceph mon dump\nsudo ceph auth get-key client.admin\nsudo ceph auth get-or-create client.kubernetes2\nsudo ceph auth get-or-create client.csi-rbd-provisioner2\n\n# Check the following links to import external Ceph cluster\n# https://rook.io/docs/rook/latest-release/CRDs/Cluster/external-cluster/?h=external#import-the-source-data\n# https://rook.io/docs/rook/latest-release/CRDs/Cluster/external-cluster/#1-create-all-users-and-keys\n# https://github.com/canonical/microk8s-core-addons/blob/main/addons/rook-ceph/plugin/connect-external-ceph\n# https://github.com/rook/rook/issues/11157\n\nsudo apt install jq vim htop -y\nsudo apt-get install python3-rbd python3-rados -y\nwget https://raw.githubusercontent.com/rook/rook/master/deploy/examples/create-external-cluster-resources.py\n\nsudo python3 create-external-cluster-resources.py \\\n  --namespace rook-ceph \\\n  --ceph-conf=/var/snap/microceph/current/conf/ceph.conf \\\n  --keyring=/var/snap/microceph/current/conf/ceph.keyring \\\n  --rbd-data-pool-name default.pool \\\n  --cephfs-filesystem-name default.cephfs \\\n  --cephfs-metadata-pool-name default.cephfs.meta \\\n  --cephfs-data-pool-name default.cephfs.data \\\n  --rgw-endpoint 192.168.205.101:8080 \\\n  --rgw-pool-prefix default \\\n  --format bash\n\n# Replace following environment\nexport ROOK_EXTERNAL_ADMIN_SECRET=$(sudo ceph auth get-or-create-key client.admin mon 'allow *' osd 'allow *' mgr 'allow *' mds 'allow *')\nexport ROOK_EXTERNAL_MONITOR_SECRET=$(sudo ceph auth get-or-create-key mon. mon 'allow *')\n\nwget https://raw.githubusercontent.com/rook/rook/master/deploy/examples/import-external-cluster.sh\nchmod +x import-external-cluster.sh\n./import-external-cluster.sh\n\n# Delete previous secrets and configmap created\nkubectl -n rook-ceph delete secret rook-ceph-mon\nkubectl -n rook-ceph delete cm rook-ceph-mon-endpoints rook-csi-rbd-node csi-rbd-provisioner\nkubectl delete storageclass ceph-rbd\n\n##################################\n# Manually\n##################################\n\nROOK_ADMIN_KEY=$(sudo ceph auth get-or-create-key client.admin mon 'allow *' osd 'allow *' mgr 'allow *' mds 'allow *')\nROOK_MON_KEY=$(sudo ceph auth get-or-create-key mon. mon 'allow *')\n\nexport NAMESPACE=rook-ceph\nexport ROOK_EXTERNAL_CEPH_MON_DATA=server-1=192.168.205.101:6789,server-2=192.168.205.102:6789,server-3=192.168.205.103:6789\nexport ROOK_EXTERNAL_FSID=$(sudo ceph fsid)\nexport ROOK_EXTERNAL_CLUSTER_NAME=$NAMESPACE\nexport ROOK_EXTERNAL_MAX_MON_ID=0\n\nkubectl create namespace $NAMESPACE\n\nkubectl -n \"$NAMESPACE\"  create secret generic rook-ceph-mon \\\n --from-literal=cluster-name=\"$ROOK_EXTERNAL_CLUSTER_NAME\" \\\n --from-literal=fsid=\"$ROOK_EXTERNAL_FSID\" \\\n --from-literal=admin-secret=\"$ROOK_ADMIN_KEY\" \\\n --from-literal=mon-secret=\"$ROOK_MON_KEY\"\n\nkubectl -n \"$NAMESPACE\" create configmap rook-ceph-mon-endpoints \\\n --from-literal=data=\"$ROOK_EXTERNAL_CEPH_MON_DATA\" \\\n --from-literal=mapping=\"$ROOK_EXTERNAL_MAPPING\" \\\n --from-literal=maxMonId=\"$ROOK_EXTERNAL_MAX_MON_ID\"\n</code></pre>"},{"location":"tools/microceph/#lvm","title":"LVM","text":"<p>Logical Volume Manager (LVM) plays an important role in the Linux operating system by improving the availability, disk I/O, performance and capability of disk management. LVM is a widely used technique that is extremely flexible for disk management.</p> <p>This adds an extra layer between the physical disks and the file system, allowing you to create a logical volume instead of a physical disk. LVM allows you to easily <code>resize</code>, <code>extend</code> and <code>decrease</code> the logical volume when you need it.</p> <p></p> <p>The steps to create logical volumens are the following.</p> <ul> <li>Create a Physical Volumes(PV) on the disk. (<code>pvcreate [Physical Volume Name]</code>)</li> <li>Create the Volume Group(VG) on the Physical Volumes (<code>vgcreate [Volume Group Name] [Physical Volume Name] [Physical Volume Name]</code>)</li> <li>Create Logical Volumes(LV) on the Volume Group (<code>lvcreate \u2013L [Logical Volume Size] \u2013n [Logical Volume Name] [Name of the Volume Group where the LV to be created]</code>)</li> <li>Create a filesystem for the logical volumes**</li> </ul> <pre><code># Run all List Blocks devices (look for the TYPE lvm) to identify the correct disk which are to be used in the LVM\n# using the fdisk command or any other disk management command [lsblk].\nsudo lsblk # sudo fdisk -l\n\n# NAME                      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\n# loop0                       7:0    0 69.2M  1 loop /snap/core22/1125\n# loop2                       7:2    0 35.2M  1 loop /snap/snapd/20674\n# vda                       252:0    0   60G  0 disk\n# \u251c\u2500vda1                    252:1    0    1G  0 part /boot/efi\n# \u251c\u2500vda2                    252:2    0    2G  0 part /boot\n# \u2514\u2500vda3                    252:3    0 56.9G  0 part\n#   \u2514\u2500ubuntu--vg-ubuntu--lv 253:0    0 28.5G  0 lvm  /\n\n# The pvdisplay command provides a verbose multi-line output for each physical volume. It displays physical properties (size, extents, volume group, etc.) in a fixed format.\nsudo pvdisplay\n\n# The pvscan command scans all supported LVM block devices in the system for physical volumes.\nsudo pvscan\n\n#  PV /dev/vda3   VG ubuntu-vg       lvm2 [&lt;56.95 GiB / 28.47 GiB free]\n#  Total: 1 [&lt;56.95 GiB] / in use: 1 [&lt;56.95 GiB] / in no VG: 0 [0   ]\n\n#  --- Physical volume ---\n#  PV Name               /dev/vda3\n#  VG Name               ubuntu-vg\n#  PV Size               &lt;56.95 GiB / not usable 3.00 MiB\n#  Allocatable           yes\n#  PE Size               4.00 MiB\n#  Total PE              14578\n#  Free PE               7289\n#  Allocated PE          7289\n#  PV UUID               9ZIc1w-wRGn-7LbN-Xnmk-czBV-QcX9-5XGnR2\n\n#\u00a0Get the volumes groups already creted\nsudo vgs\n\n#  VG        #PV #LV #SN Attr   VSize   VFree\n#  ubuntu-vg   1   1   0 wz--n- &lt;56.95g 28.47g\n\n# Create Logical Volume\nsudo lvcreate -L 4GB -n lv1 ubuntu-vg\n\n# List Logical Volumes\nsudo lvs\n\n# Format the logical volumen with ext4 format (sudo lsblk -f)\nsudo mkfs -t ext4 /dev/ubuntu-vg/lv1\n\n#\u00a0Create filesystem for the logical volumes and mount into the mapper\n# The device mapper is a framework provided by the Linux kernel for mapping physical block devices onto higher-level virtual block devices. It forms the foundation of the logical volume manager (LVM), software RAIDs and dm-crypt disk encryption, and offers additional features such as file system snapshots.\nsudo mkdir -p /data/01\nsudo mount /dev/mapper/ubuntu--vg-lv1 /data/01\n\n# To mount it permanently use that UUID number and paste it inside etc/fstab path\n#   https://xan.manning.io/2017/05/29/best-practice-for-mounting-an-lvm-logical-volume-with-etc-fstab.html\n#  sudo apt install vim -y\nsudo blkid | grep ubuntu--vg-lv1\n\n# /dev/mapper/ubuntu--vg-ubuntu--lv: UUID=\"b3087c64-de0a-41f0-b77f-85b67b8d9b26\" BLOCK_SIZE=\"4096\" TYPE=\"ext4\"\n# /dev/mapper/ubuntu--vg-mylv: UUID=\"ca9f7725-5a84-4757-b112-158598bc7a5c\" BLOCK_SIZE=\"4096\" TYPE=\"ext4\"\nsudo vi /etc/fstab\n\n# Add following line\n/dev/mapper/ubuntu--vg-lv1 /data/01  ext4 defaults 0 2\n\n# Finally get all the mounted locations\n# https://github.com/rook/rook/blob/master/design/ceph/ceph-volume-provisioning.md\nsudo df -h\nsudo lsblk\n\nsudo reboot\n</code></pre>"},{"location":"tools/microceph/#lvm-rook-ceph","title":"LVM Rook-Ceph","text":"<p>Using Rook Ceph, the logical volume must be created and formatted. So, as if the physical disks is not empty, it would be neecesary to formtat the device.</p> <pre><code># Create Logical Volume\nsudo lvcreate -L 4GB -n lv1 ubuntu-vg\n\nDISK=\"/dev/mapper/ubuntu--vg-lv1\"\nsudo dd if=/dev/zero of=\"$DISK\" bs=1M count=100 oflag=direct,dsync\nsudo sgdisk --zap-all $DISK\n\n\n# Remove Logical Volume\nsudo lvremove /dev/ubuntu-vg/lv1\n\n# Check current device\nsudo lsblk /dev/dm-1 --bytes --nodeps --pairs --paths --output SIZE,ROTA,RO,TYPE,PKNAME,NAME,KNAME,MOUNTPOINT,FSTYPE\nsudo lsblk /dev/mapper/ubuntu--vg-lv1 --bytes --nodeps --pairs --paths --output SIZE,ROTA,RO,TYPE,PKNAME,NAME,KNAME,MOUNTPOINT,FSTYPE\n\n#SIZE=\"4294967296\" ROTA=\"1\" RO=\"0\" TYPE=\"lvm\" PKNAME=\"\" NAME=\"/dev/mapper/ubuntu--vg-lv1\" KNAME=\"/dev/dm-1\" MOUNTPOINT=\"\" FSTYPE=\"\"\n</code></pre>"},{"location":"tools/microceph/#references","title":"References","text":"<ul> <li>[https://www.youtube.com/watch?v=Uvbp3mtOltw]</li> <li>[https://www.youtube.com/watch?v=98QujsS7jFI&amp;t=1630s]</li> <li>[https://www.youtube.com/watch?v=cR-s26Zzx4Y]</li> </ul>"},{"location":"tools/qemu/","title":"QEMU","text":""},{"location":"tools/qemu/#installation","title":"Installation","text":"<p>Create Ubuntu VM using QEMU</p> <ul> <li> <p>Download Ubuntu Server for ARM</p> </li> <li> <p>Install qemu</p> </li> </ul> <pre><code># Install Main packages\nbrew install rpm2cpio qemu\n</code></pre> <ul> <li>Install and launch <code>socket_vmnet</code></li> </ul> <p>socket_vmnet <code>vmnet.framework</code> support for rootless and VDE-less <code>QEMU</code>.</p> <pre><code># Install socket_vmnet.\nbrew install socket_vmnet\n\n########################################\n## Start service using launchd and brew\n########################################\n\n# Install the launchd service\nbrew tap homebrew/services\n# sudo is necessary for the next line (default gateway: 192.168.105.1)\nsudo brew services start socket_vmnet\nsudo brew services start socket_vmnet --file=\"docs/tools/qemu/templates/homebrew.mxcl.socket_vmnet.plist\"\n\n# Uninstall services\nsudo brew services stop socket_vmnet\n\n##############################\n## Start manually the service\n##############################\n\n# Start socket_vmnet with specific gateway ip-address\nmkdir -p \"$HOMEBREW_PREFIX/var/run\"\nsudo \"$HOMEBREW_PREFIX/opt/socket_vmnet/bin/socket_vmnet\" --vmnet-gateway=192.168.205.1 \"$HOMEBREW_PREFIX/var/run/socket_vmnet\"\n</code></pre>"},{"location":"tools/qemu/#create-base-image","title":"Create Base Image","text":"<p>Create Base image from Ubuntu</p> <pre><code># Create global environment variables\nexport QEMU_FOLDER=/opt/homebrew/share/qemu\nexport QEMU_IMAGE=../images/ubuntu-22.04.3-live-server-arm64.iso\nexport QEMU_BIOS=edk2-aarch64-code.fd\nexport QEMU_UEFI=edk2-arm-vars.fd\n\n#\u00a0Create Specific environment variables for current VM to be created\nexport QEMU_DISK=base-image.qcow2\nexport QEMU_MAC=3A:AA:06:A4:FE:E0\nexport QEMU_NET=net0\n\n# Copy ARM BIOS for aarch64\ncp $QEMU_FOLDER/$QEMU_UEFI $QEMU_UEFI\ncp $QEMU_FOLDER/$QEMU_BIOS $QEMU_BIOS\n\n# Create initial Disk to store the Base image to create SnapShots\nqemu-img create -f qcow2 $QEMU_DISK 20G\n\n#\u00a0Create a new tab\n#open -a iTerm .\n\n# Create Base VM (qemu-system-{arch})\nqemu-system-aarch64 \\\n    -smp cpus=2,sockets=1,cores=2,threads=1 \\\n    -m 2048 \\\n    -machine virt,accel=hvf,highmem=off \\\n    -cpu host \\\n    -device virtio-gpu-pci -device qemu-xhci -device usb-kbd -device usb-tablet \\\n    -drive if=virtio,format=qcow2,file=$QEMU_DISK,cache=writethrough \\\n    -drive if=pflash,format=raw,file=$QEMU_BIOS,unit=0,readonly=on \\\n    -drive if=pflash,format=raw,file=$QEMU_UEFI,unit=1 \\\n    -device virtio-net-pci,mac=$QEMU_MAC,netdev=$QEMU_NET \\\n    -netdev user,id=$QEMU_NET \\\n    -monitor stdio \\\n    -cdrom $QEMU_IMAGE\n\n# Set the main configuration hostname, user and password during the installation\n\nexport QEMU_DISK1='snapshot-01.qcow2'\n\n# Create snapshots from the previous image (This is created in vagrant-qemu automatically)\nqemu-img create -f qcow2 -F qcow2 -b $QEMU_DISK $QEMU_DISK1\n\n# Start VM using snapshot(qemu-system-{arch})\nqemu-system-aarch64 \\\n    -smp cpus=2,sockets=1,cores=2,threads=1 \\\n    -m 2048 \\\n    -machine virt,accel=hvf,highmem=off \\\n    -cpu host \\\n    -device virtio-gpu-pci -device qemu-xhci -device usb-kbd -device usb-tablet \\\n    -drive if=virtio,format=qcow2,file=$QEMU_DISK1,cache=writethrough \\\n    -drive if=pflash,format=raw,file=$QEMU_BIOS,unit=0,readonly=on \\\n    -drive if=pflash,format=raw,file=$QEMU_UEFI,unit=1 \\\n    -device virtio-net-pci,mac=$QEMU_MAC,netdev=$QEMU_NET \\\n    -netdev user,id=$QEMU_NET,hostfwd=tcp::50022-:22 \\\n    -monitor stdio\n\n    # With output\n    -monitor stdio\n\n    # No output\n    -pidfile qemu.pid \\\n    -daemonize -parallel null -monitor none -display none -vga none\n\n\n# SSH into the machie using the forwarded port\nssh jsantosa@localhost -p 50022\n\n# SSH and run commando (-t for ask sudo permissions)\nssh -t jsantosa@localhost -p 50022 'sudo apt-get update &amp;&amp; sudo apt-get upgrade -y &amp;&amp; sudo apt-get install net-tools iputils-ping python3 -y'\n</code></pre>"},{"location":"tools/qemu/#create-virtual-machine","title":"Create Virtual Machine","text":"<ul> <li>Create Global Environment Variables for Configuration</li> </ul> <pre><code># Create global environment variables\nexport QEMU_FOLDER=/opt/homebrew/share/qemu\nexport QEMU_IMAGE=../images/ubuntu-22.04.1-live-server-arm64.iso\nexport QEMU_BIOS=edk2-aarch64-code.fd\nexport QEMU_UEFI=edk2-arm-vars.fd\n</code></pre> <ul> <li>Create following content</li> </ul> <pre><code># Create an empty file for persisting UEFI variables or using existing one (`edk2-arm-vars.fd`)\ndd if=/dev/zero conv=sync bs=1m count=64 of=$QEMU_UEFI\n\n# Or Copy existing one\n#cp $QEMU_FOLDER/$QEMU_UEFI $QEMU_UEFI\n\n# Copy ARM BIOS for aarch64\ncp $QEMU_FOLDER/$QEMU_BIOS $QEMU_BIOS\n\n# Create directory for temporal files\nmkdir tmp/\n</code></pre> <ul> <li>Run QEMU to create the Base Image</li> </ul> <pre><code>#\u00a0Create Specific environment variables for current VM to be created\nexport QEMU_DISK=base-image.qcow2\nexport QEMU_MAC=3A:AA:06:A4:FE:E0\nexport QEMU_NET=net0\n\n# Create initial Disk to store the Base image to create SnapShots\nqemu-img create -f qcow2 $QEMU_DISK 10G\n\n# Create Base VM (qemu-system-{arch})\nqemu-system-aarch64 \\\n    -smp cpus=2,sockets=1,cores=2,threads=1 \\\n    -m 2048 \\\n    -machine virt,accel=hvf,highmem=off \\\n    -cpu host \\\n    -device virtio-gpu-pci -device qemu-xhci -device usb-kbd -device usb-tablet \\\n    -drive if=virtio,format=qcow2,file=$QEMU_DISK,cache=writethrough \\\n    -drive if=pflash,format=raw,file=$QEMU_BIOS,unit=0,readonly=on \\\n    -drive if=pflash,format=raw,file=$QEMU_UEFI,unit=1 \\\n    -device virtio-net-pci,mac=$QEMU_MAC,netdev=$QEMU_NET \\\n    -netdev user,id=$QEMU_NET \\\n    -monitor stdio \\\n    -cdrom $QEMU_IMAGE\n\n#\u00a0Create a new tab\nopen -a iTerm .\n\n# Start the VM (removing the cdrom)\nsudo qemu-system-aarch64 \\\n    -smp cpus=2,sockets=1,cores=2,threads=1 \\\n    -m 2048 \\\n    -machine virt,accel=hvf,highmem=off \\\n    -cpu host \\\n    -device virtio-gpu-pci -device qemu-xhci -device usb-kbd -device usb-tablet \\\n    -drive if=virtio,format=qcow2,file=$QEMU_DISK,cache=writethrough \\\n    -drive if=pflash,format=raw,file=$QEMU_BIOS,unit=0,readonly=on \\\n    -drive if=pflash,format=raw,file=$QEMU_UEFI,unit=1 \\\n    -device virtio-net-pci,mac=$QEMU_MAC,netdev=$QEMU_NET \\\n    -netdev vmnet-shared,id=$QEMU_NET \\\n    -nographic\n\n# Create Base VM (qemu-system-{arch}) With Multiple interfaces\nexport QEMU_MAC1='3A:AA:06:A4:FE:E0'\nexport QEMU_MAC2='3A:AA:06:A4:FE:E1'\nexport QEMU_NET1='net0'\nexport QEMU_NET2='net1'\nsudo qemu-system-aarch64 \\\n    -smp cpus=2,sockets=1,cores=2,threads=1 \\\n    -m 2048 \\\n    -machine virt,accel=hvf,highmem=off \\\n    -cpu host \\\n    -device virtio-gpu-pci -device qemu-xhci -device usb-kbd -device usb-tablet \\\n    -drive if=virtio,format=qcow2,file=$QEMU_DISK,cache=writethrough \\\n    -drive if=pflash,format=raw,file=$QEMU_BIOS,unit=0,readonly=on \\\n    -drive if=pflash,format=raw,file=$QEMU_UEFI,unit=1 \\\n    -device virtio-net-pci,mac=$QEMU_MAC2,netdev=$QEMU_NET2 \\\n    -netdev vmnet-shared,id=$QEMU_NET2 \\\n    -device virtio-net-pci,mac=$QEMU_MAC1,netdev=$QEMU_NET1 \\\n    -netdev user,id=$QEMU_NET1,hostfwd=tcp::2222-:22 \\\n    -monitor stdio \\\n    -cdrom $QEMU_IMAGE\n\n# It would open on a new Window with QEMU Monitor to start installing VM\n# Use standard hostname, user and password for the main image\n# Then install Linux, set the keyboard layout, region, updates, etc..\n\n# Finally update the image with latest patches\nsudo apt-get update &amp;&amp; sudo apt-get upgrade -y\nsudp apt-get install net-tools iputils-ping -y\n</code></pre> <ul> <li>Create an snapshot to avoid installing the image every time.</li> </ul> <pre><code>export QEMU_DISK1='snapshot-01.qcow2'\nexport QEMU_DISK2='snapshot-02.qcow2'\nexport QEMU_DISK3='snapshot-03.qcow2'\n\n# Create snapshots from the previous image\nqemu-img create -f qcow2 -F qcow2 -b $QEMU_DISK $QEMU_DISK1\nqemu-img create -f qcow2 -F qcow2 -b $QEMU_DISK $QEMU_DISK2\nqemu-img create -f qcow2 -F qcow2 -b $QEMU_DISK $QEMU_DISK3\n</code></pre> <ul> <li>Run different VMs using different snapshots</li> </ul> <pre><code># VM1\nexport QEMU_DISK='snapshot-01.qcow2'\nexport QEMU_MAC='3A:AA:06:A4:FE:E0'\nexport QEMU_NET='net0'\n\n# VM2\nexport QEMU_DISK='snapshot-02.qcow2'\nexport QEMU_MAC='3A:AA:06:A4:FE:E1'\nexport QEMU_NET='net1'\n\n# Run Virtual Machine. No Network (VM1 &amp; VM2)\nqemu-system-aarch64 \\\n    -smp cpus=2,sockets=1,cores=2,threads=1 \\\n    -m 2048 \\\n    -machine virt,accel=hvf,highmem=off \\\n    -cpu host \\\n    -device virtio-gpu-pci -device qemu-xhci -device usb-kbd -device usb-tablet \\\n    -drive if=virtio,format=qcow2,file=$QEMU_DISK,cache=writethrough \\\n    -drive if=pflash,format=raw,file=$QEMU_BIOS,unit=0,readonly=on \\\n    -drive if=pflash,format=raw,file=$QEMU_UEFI,unit=1 \\\n    -device virtio-net-pci,mac=$QEMU_MAC,netdev=$QEMU_NET \\\n    -netdev user,id=$QEMU_NET,hostfwd=tcp::2222-:22\n\n#\u00a0Without QEMU Window\nqemu-system-aarch64 \\\n    -smp cpus=2,sockets=1,cores=2,threads=1 \\\n    -m 2048 \\\n    -machine virt,accel=hvf,highmem=off \\\n    -cpu host \\\n    -device virtio-gpu-pci -device qemu-xhci -device usb-kbd -device usb-tablet \\\n    -drive if=virtio,format=qcow2,file=$QEMU_DISK,cache=writethrough \\\n    -drive if=pflash,format=raw,file=$QEMU_BIOS,unit=0,readonly=on \\\n    -drive if=pflash,format=raw,file=$QEMU_UEFI,unit=1 \\\n    -device virtio-net-pci,mac=$QEMU_MAC,netdev=$QEMU_NET \\\n    -netdev user,id=$QEMU_NET,hostfwd=tcp::2222-:22 \\\n    -parallel null -monitor none -display none -vga none -pidfile qemu.pid &amp;\n\n# Connect to SSH via Port-Forwarding\nssh 127.0.0.1 -p 2222\n\n# Run using Shared Network (Similar to NAT in other Hypervisors)  (VM1 &amp; VM2)\nsudo qemu-system-aarch64 \\\n    -smp cpus=2,sockets=1,cores=2,threads=1 \\\n    -m 2048 \\\n    -machine virt,accel=hvf,highmem=off \\\n    -cpu host \\\n    -device virtio-gpu-pci -device qemu-xhci -device usb-kbd -device usb-tablet \\\n    -drive if=virtio,format=qcow2,file=$QEMU_DISK,cache=writethrough \\\n    -drive if=pflash,format=raw,file=$QEMU_BIOS,unit=0,readonly=on \\\n    -drive if=pflash,format=raw,file=$QEMU_UEFI,unit=1 \\\n    -device virtio-net-pci,mac=$QEMU_MAC,netdev=$QEMU_NET \\\n    -netdev vmnet-shared,id=$QEMU_NET \\\n    -parallel null -monitor none -display none -vga none -pidfile qemu.pid &amp;\n\n    # With graphical option\n    -nographic\n\n# Since there is no Port-forward, open the VM, login-in and check the current ip\nifconfig\n\n# USe the ping to know if VM is reachable from Host\nping 192.168.205.5\n\n# Connect via SSH (Use the default Port 22) (If error use: rm /Users/jsantosa/.ssh/known_hosts)\nssh jsantosa@192.168.205.5\n\n# If using &amp; at the end of the command, the procces will be launched as child process (background)\n\n#\u00a0Get tue pid process\nsudo cat qemu.pid\n\n# Kill the process (using pidfile)\nsudo kill -9 $(sudo cat qemu.pid)\n</code></pre> <ul> <li>Use the proper Network to be used: host-only, bridged, shared (~NAT), etc..</li> </ul> <pre><code># Host-only (with port-forwarding)\n    -device virtio-net-pci,mac=$QEMU_MAC,netdev=$QEMU_NET \\\n    -netdev user,id=$QEMU_NET,hostfwd=tcp::2222-:22 \\\n\n# Shared Network (NAT, internal network with internet access)\n    -device virtio-net-pci,mac=$QEMU_MAC,netdev=$QEMU_NET \\\n    -netdev vmnet-shared,id=$QEMU_NET\n\n# Bridged Network (LAN external network with outside)\n    -device virtio-net-pci,mac=$QEMU_MAC,netdev=$QEMU_NET \\\n    -netdev vmnet-bridged,id=$QEMU_NET,ifname=en0 \\\n</code></pre> <ul> <li>You should be able to install Ubuntu as normal</li> <li>If you want a desktop environment, you can install it using <code>sudo apt-get install ubuntu-desktop</code></li> </ul>"},{"location":"tools/qemu/#run-qemu-rootless","title":"Run QEMU rootless","text":"<p>Use <code>socket_vmnet</code> to use rootless. <code>socket_vmnet</code> must be installed and started.</p> <pre><code># Copy ARM BIOS for aarch64\ncp /opt/homebrew/share/qemu/edk2-aarch64-code.fd ./edk2-aarch64-code.fd\ncp /opt/homebrew/share/qemu/edk2-arm-vars.fd ./edk2-arm-vars.fd\n\n# Create initial Disk to store the Base image to create SnapShots\nqemu-img create -f qcow2 ./base-image.qcow2 256G\n\n# Create VM\n\"$HOMEBREW_PREFIX/opt/socket_vmnet/bin/socket_vmnet_client\" \"$HOMEBREW_PREFIX/var/run/socket_vmnet\" qemu-system-aarch64 \\\n    -smp cpus=4,sockets=1,cores=4,threads=1 \\\n    -m 8192 \\\n    -machine virt,accel=hvf,highmem=on \\\n    -cpu host \\\n    -device virtio-gpu-pci -device qemu-xhci -device usb-kbd -device usb-tablet \\\n    -drive if=virtio,format=qcow2,file=./base-image.qcow2,cache=writethrough \\\n    -drive if=pflash,format=raw,file=./edk2-aarch64-code.fd,unit=0,readonly=on \\\n    -drive if=pflash,format=raw,file=./edk2-arm-vars.fd,unit=1 \\\n    -device virtio-net-pci,mac=ee:db:ea:07:fe:d9,netdev=net0 \\\n    -netdev user,id=net0,hostfwd=tcp::50030-:22 \\\n    -device virtio-net-pci,mac=1e:e0:2b:b1:9b:97,netdev=net1 \\\n    -netdev socket,id=net1,fd=3 \\\n    -cdrom /Users/jsantosa/VMs/images/ubuntu-22.04.3-live-server-arm64.iso \\\n    -monitor stdio\n\n##\u00a0SSH into the VM\nssh ubuntu@localhost -p 50030\n\n# 4. Perform initial configuration and bootsrapping for main packages\nssh-copy-id -i /Users/jsantosa/.ssh/server_key.pub -p 50030 ubuntu@localhost\nssh -i /Users/jsantosa/.ssh/server_key -t ubuntu@localhost -p 50030 \"echo ubuntu | sudo -S apt-get remove needrestart -y &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get upgrade -y &amp;&amp; sudo apt-get install net-tools iputils-ping python3-pip -y\"\n\n# 5. Now you can enter using the public key\nssh -i /Users/jsantosa/.ssh/server_key ubuntu@localhost -p 50030\n\n# 6. Power off the VM\nssh -i /Users/jsantosa/.ssh/server_key -t ubuntu@localhost -p 50030 \"echo ubuntu | sudo -S poweroff\"\n\n# 7. Optionalyy you can add folling configuration to you ssh config file at ~/.ssh/config, so the '-i' flag it's not neccesary to connect via ssh.\n\nHost localhost\n    HostName 127.0.0.1\n    User     ubuntu\n    IdentityFile /Users/jsantosa/.ssh/server_key\n    AddKeysToAgent yes\n    UseKeychain yes\n    IgnoreUnknown UseKeychain\n    StrictHostKeyChecking no\n</code></pre>"},{"location":"tools/qemu/#example","title":"EXAMPLE","text":"<pre><code># Create global environment variables\nexport QEMU_FOLDER='/System/Volumes/Data/opt/homebrew/Cellar/qemu/7.1.0/share/qemu'\nexport QEMU_IMAGE='../images/ubuntu-22.04.1-live-server-arm64.iso'\nexport QEMU_BIOS='edk2-aarch64-code.fd'\nexport QEMU_UEFI='edk2-arm-vars.fd'\n\nexport QEMU_DISK='snapshot-01.qcow2'\nexport QEMU_MAC1='3A:AA:06:A4:FE:E0'\nexport QEMU_MAC2='3A:AA:06:A4:FE:E1'\nexport QEMU_NET1='net0'\nexport QEMU_NET2='net1'\nexport QEMU_SSH='2222'\n\nexport QEMU_DISK='snapshot-02.qcow2'\nexport QEMU_MAC1='3A:AA:06:A4:FE:E2'\nexport QEMU_MAC2='3A:AA:06:A4:FE:E3'\nexport QEMU_NET1='net0'\nexport QEMU_NET2='net1'\nexport QEMU_SSH='2223'\n\n# Create VM with multiple Network Interfaces\nsudo qemu-system-aarch64 \\\n    -smp cpus=2,sockets=1,cores=2,threads=1 \\\n    -m 2048 \\\n    -machine virt,accel=hvf,highmem=off \\\n    -cpu host \\\n    -device virtio-gpu-pci -device qemu-xhci -device usb-kbd -device usb-tablet \\\n    -drive if=virtio,format=qcow2,file=$QEMU_DISK,cache=writethrough \\\n    -drive if=pflash,format=raw,file=$QEMU_BIOS,unit=0,readonly=on \\\n    -drive if=pflash,format=raw,file=$QEMU_UEFI,unit=1 \\\n    -device virtio-net-pci,mac=$QEMU_MAC1,netdev=$QEMU_NET1 \\\n    -netdev user,id=$QEMU_NET1,hostfwd=tcp::$QEMU_SSH-:22 \\\n    -device virtio-net-pci,mac=$QEMU_MAC2,netdev=$QEMU_NET2 \\\n    -netdev vmnet-shared,id=$QEMU_NET2 \\\n    -parallel null -monitor none -display none -vga none -pidfile qemu.pid &amp;\n\n# Bridged Interface\n    -netdev vmnet-bridged,id=$QEMU_NET2,ifname=en0\n\n# Connect to SSH via Port-Forwarding\nssh 127.0.0.1 -p 2222\n</code></pre>"},{"location":"tools/qemu/#configuration","title":"Configuration","text":""},{"location":"tools/qemu/#static-ip","title":"Static IP","text":"<p>Crate a bash file into <code>/etc/netplan</code> to assign static IPs</p> <pre><code># Edit the file or create a new one\nsudo vi /etc/netplan/00-installer-config.yaml\n</code></pre> <p>Put the following content.</p> <pre><code>network:\n  ethernets:\n    enp0s3:\n      dhcp4: false\n      addresses: [192.168.205.100/24]\n      routes:\n      - to: default\n        via: 192.168.205.1\n      nameservers:\n        addresses: [8.8.8.8,8.8.8.4]\n  version: 2\n</code></pre> <p>Apply and test the changes.</p> <pre><code># Apply and test the changes.\nsudo netplan try\n</code></pre>"},{"location":"tools/qemu/#hostname","title":"Hostname","text":"<p>Change Server Hostname</p> <pre><code># Change hostname\nsudo hostnamectl set-hostname server-01\n</code></pre>"},{"location":"tools/qemu/#resize-disk","title":"Resize Disk","text":"<p>Command to expand the disk of an already created image</p> <pre><code># Resize image\nqemu-img resize vm-image.qcow2 +30G\n</code></pre>"},{"location":"tools/qemu/#virtual-block","title":"Virtual Block","text":"<p>Linux users can have a virtual block device called a <code>loop device</code> that maps a normal file to a virtual block, making it ideal for tasks related to isolating processes.</p> <pre><code># Get Loop devices\nlosetup -l\n\n# Search for a particular Virtual block\nlosetup /dev/loop0\n\n# Create a loop device (1GB)\n\n#\u00a01. Create a block file called \"blockfile\" (within current directory)\ndd if=/dev/zero of=blockfile bs=1M count=1024\n\n# 2. Create the loop device (ref to the previous file created)\nsudo losetup /dev/loop0 blockfile\n\n#\u00a0Uae lsblk or losetup to verify the loop device has been created\n\n# 3. You can partition and mount the volumes as disks\n\n# 4. Detach the loop device\nsudo losetup -d /dev/loop0\n</code></pre>"},{"location":"tools/qemu/#references","title":"References","text":"<ul> <li>[https://gist.github.com/max-i-mil/f44e8e6f2416d88055fc2d0f36c6173b]</li> <li>[https://patchew.org/QEMU/20211207101828.22033-1-yaroshchuk2000@gmail.com/]</li> <li>[https://linuxconfig.org/how-to-create-loop-devices-on-linux]</li> <li>[https://github.com/rook/rook/issues/7206]</li> <li>[https://medium.com/techlogs/configuring-rook-with-external-ceph-6b4b49626112]</li> </ul>"},{"location":"tools/ssh/","title":"SSH","text":"<p>The Secure Shell protocol (SSH) is used to create secure connections between your device and private servers. The connection is authenticated using public SSH keys, which are derived from a private SSH key (also known as a private/public key pair). The secure (encrypted) connection is used to securely establish connection to transfer or run commands on remote devices.</p> <p>some of the best practices when creating SSH keys are:</p> <ul> <li>Keys should be issued to individuals, not groups</li> <li>Rotating your keys</li> <li>Don't use the default comment</li> <li>Always use a passphrase</li> </ul>"},{"location":"tools/ssh/#generate-sh-keys","title":"Generate SH Keys","text":"<p>In order to generate SSH keys you need to run following commands and following best practices.</p> <pre><code># [Optional] If ssh folder does not exist\nmkdir -p ~/.ssh\nchmod 700 ~/.ssh\n\n# Do not fill anything in next command just enter\nssh-keygen -t rsa -b 4096 -C \"jsa4000@gmail.com\" -f ~/.ssh/server_key\n\n# [Optional] Copy keys to each node or use AUTO_SETUP_SSH_PUBKEY in 'dietpi.txt' config instead\nssh-copy-id -i ~/.ssh/server_key.pub root@192.168.3.100\n</code></pre>"},{"location":"tools/ssh/#connect","title":"Connect","text":"<p>In order to connect to the remote servers you would need to use <code>ssh</code> client.</p> <pre><code># [Optional] Remove previous known host for previous connections\nrm ~/.ssh/known_hosts\n\n# SSH into the machine (user/password)\nssh root@192.168.3.100\n\n# SSH into the machine (private key)\nssh -i ~/.ssh/server_key root@192.168.3.100\nssh -i ~/.ssh/server_key dietpi@192.168.3.100\n\n# You can run commands using ssh\nssh -i ~/.ssh/server_key dietpi@sbc-server-1 'ls -la'\n\n# You can transfer files between local and server respectively 'scp [source] [destination]'\nscp ~/file.txt dietpi@192.168.3.100:~/\n</code></pre> <p>Using the SSH config file so it's not necessary to enter the private key anymore.</p> <p><code>~/.ssh/config</code></p> <pre><code>Host sbc-server-1\n    HostName 192.168.3.100\n    User     dietpi\n    IdentityFile ~/.ssh/server_key\n\nHost sbc-server-2\n    HostName 192.168.3.101\n    User     dietpi\n    IdentityFile ~/.ssh/server_key\n\nHost sbc-server-3\n    HostName 192.168.3.102\n    User     dietpi\n    IdentityFile ~/.ssh/server_key\n</code></pre> <p>Connect using <code>ssh</code> client and without providing the key.</p> <pre><code># [Optional] Remove previous known host for previous connections\nrm ~/.ssh/known_hosts\n\n# SSH using the user@host specified in the SSH config\nssh dietpi@sbc-server-1\nssh dietpi@sbc-server-2\nssh dietpi@sbc-server-3\n\n# You can run commands remotely\nssh dietpi@sbc-server-1 'sudo poweroff'\nssh dietpi@sbc-server-2 'sudo poweroff'\nssh dietpi@sbc-server-3 'sudo poweroff'\n</code></pre>"},{"location":"tools/ssh/#ssh-agent","title":"SSH Agent","text":"<p>The <code>ssh-agent</code> is caching your keys' in memory once these are unlocked and you will not be asked to provide the passphrase to unlock these keys every time they are used.</p> <pre><code># Check if ssh-agent is already running in the machine\nps ax | grep ssh-agent\n\n# Start the agent of not running\neval \"$(ssh-agent -s)\"\n</code></pre> <p>Add your SSH private key to the <code>ssh-agent</code> and store your passphrase in the keychain.</p> <pre><code># Add ssh to the agent\nssh-add --apple-use-keychain ~/.ssh/server_key\n</code></pre> <p>The result must be similar to this <code>~/.ssh/config</code>, so it can be added manually.</p> <pre><code> Host sbc-server-1\n    HostName 192.168.3.100\n    User     dietpi\n    IdentityFile /Users/jsantosa/.ssh/server_key\n    AddKeysToAgent yes\n    UseKeychain yes\n    IgnoreUnknown UseKeychain\n</code></pre>"}]}