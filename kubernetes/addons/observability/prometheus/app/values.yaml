#fullnameOverride: "prometheus"
crds:
  enabled: true
cleanPrometheusOperatorObjectNames: true

alertmanager:
  enabled: true
  config:
    global:
      resolve_timeout: 5m
    inhibit_rules:
      - source_match:
          severity: "critical"
        target_match:
          severity: "warning"
        equal: ["alertname", "namespace"]
    route:
      group_by: ["alertname", "job"]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 6h
      receiver: 'gotify'
      routes:
        - receiver: "null"
          matchers:
            - alertname =~ "InfoInhibitor|Watchdog"
        - receiver: "gotify"
          matchers:
            - severity = "critical"
            - severity = "warning"
            - severity = "info"
          continue: true
    receivers:
      - name: "null"
      # Alertmanager to Gotify webhook bridge
      # https://github.com/DRuggeri/alertmanager_gotify_bridge
      - name: gotify
        webhook_configs:
          - url: http://gotify-webhook.observability.svc.cluster.local:80/gotify_webhook
            send_resolved: false
    templates:
      - '/etc/alertmanager/config/*.tmpl'
  ingress:
    enabled: false

grafana:
  enabled: true
  defaultDashboardsEnabled: true
  defaultDashboardsTimezone: Europe/Madrid
  admin:
    existingSecret: prometheus-stack-grafana-admin
    userKey: admin-user
    passwordKey: admin-password
  ingress:
    enabled: false
  sidecar:
    dashboards:
      enabled: true
      searchNamespace: ALL
      label: grafana_dashboard
      labelValue: "true"
      folderAnnotation: grafana_folder
      provider:
        disableDelete: true
        foldersFromFilesStructure: true
    datasources:
      enabled: true
      searchNamespace: ALL
      label: grafana_datasource
      labelValue: "true"
  plugins:
    - grafana-clock-panel
    - grafana-piechart-panel
    - grafana-worldmap-panel
    - natel-discrete-panel
    - pr0ps-trackmap-panel
    - vonage-status-panel
  serviceMonitor:
    enabled: true
  persistence:
    enabled: true
    # storageClassName: ceph-rbd
    storageClassName: longhorn
    accessModes: ["ReadWriteOnce"]
    size: 2Gi
    finalizers:
      - kubernetes.io/pvc-protection
  testFramework:
    enabled: false

kubeControllerManager:
  enabled: true
  endpoints: &ce
    - 192.168.205.101
    - 192.168.205.101
    - 192.168.205.101

kubeEtcd:
  enabled: true
  endpoints: *ce

kubeScheduler:
  enabled: true
  endpoints: *ce

prometheusOperator:
  enabled: true
  resources:
    requests:
      cpu: 35m
      memory: 128M
    limits:
      memory: 512M
  prometheusConfigReloader:
    resources:
      requests:
        cpu: 200m
        memory: 50Mi
  # Set ttlSecondsAfterFinished relevant for argocd
  # https://github.com/prometheus-community/helm-charts/pull/4510
  # https://github.com/argoproj/argo-cd/issues/6880
  admissionWebhooks:
    patch:
      enabled: true
      ttlSecondsAfterFinished: 30

prometheus:
  enabled: true
  prometheusSpec:
    replicas: 1
    replicaExternalLabelName: "replica"
    # These values must be set to false to be scrapped by this instance (release label)
    # i.e. metrics.serviceMonitor.additionalLabels.release="prometheus-stack"
    ruleSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    scrapeConfigSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false
    # Adjust the retention period and max size for the metrics to be stored.
    retention: 3d
    retentionSize: 4GB
    enableAdminAPI: true
    walCompression: true
    resources:
      requests:
        cpu: 300m
        memory: 2560M
      limits:
        memory: 5120M
    storageSpec:
      volumeClaimTemplate:
        spec:
          # storageClassName: ceph-rbd
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi
